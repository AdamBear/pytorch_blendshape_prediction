{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "fae648b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: soundfile in /home/hydroxide/.local/lib/python3.7/site-packages (0.10.3.post1)\n",
      "Requirement already satisfied: cffi>=1.0 in /home/hydroxide/.local/lib/python3.7/site-packages (from soundfile) (1.14.6)\n",
      "Requirement already satisfied: pycparser in /home/hydroxide/.local/lib/python3.7/site-packages (from cffi>=1.0->soundfile) (2.20)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in /home/hydroxide/.local/lib/python3.7/site-packages (1.3.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/hydroxide/.local/lib/python3.7/site-packages (from pandas) (2021.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/hydroxide/.local/lib/python3.7/site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /home/hydroxide/.local/lib/python3.7/site-packages (from pandas) (1.19.5)\n",
      "Requirement already satisfied: six>=1.5 in /home/hydroxide/.local/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: scipy in /home/hydroxide/.local/lib/python3.7/site-packages (1.7.1)\n",
      "Requirement already satisfied: numpy<1.23.0,>=1.16.5 in /home/hydroxide/.local/lib/python3.7/site-packages (from scipy) (1.19.5)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: librosa in /home/hydroxide/.local/lib/python3.7/site-packages (0.8.1)\n",
      "Requirement already satisfied: soundfile>=0.10.2 in /home/hydroxide/.local/lib/python3.7/site-packages (from librosa) (0.10.3.post1)\n",
      "Requirement already satisfied: resampy>=0.2.2 in /home/hydroxide/.local/lib/python3.7/site-packages (from librosa) (0.2.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/hydroxide/.local/lib/python3.7/site-packages (from librosa) (1.19.5)\n",
      "Requirement already satisfied: numba>=0.43.0 in /home/hydroxide/.local/lib/python3.7/site-packages (from librosa) (0.48.0)\n",
      "Requirement already satisfied: joblib>=0.14 in /home/hydroxide/.local/lib/python3.7/site-packages (from librosa) (1.0.1)\n",
      "Requirement already satisfied: decorator>=3.0.0 in /home/hydroxide/.local/lib/python3.7/site-packages (from librosa) (5.0.9)\n",
      "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /home/hydroxide/.local/lib/python3.7/site-packages (from librosa) (0.24.2)\n",
      "Requirement already satisfied: scipy>=1.0.0 in /home/hydroxide/.local/lib/python3.7/site-packages (from librosa) (1.7.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/hydroxide/.local/lib/python3.7/site-packages (from librosa) (21.0)\n",
      "Requirement already satisfied: audioread>=2.0.0 in /home/hydroxide/.local/lib/python3.7/site-packages (from librosa) (2.1.9)\n",
      "Requirement already satisfied: pooch>=1.0 in /home/hydroxide/.local/lib/python3.7/site-packages (from librosa) (1.4.0)\n",
      "Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /home/hydroxide/.local/lib/python3.7/site-packages (from numba>=0.43.0->librosa) (0.31.0)\n",
      "Requirement already satisfied: setuptools in /home/hydroxide/.local/lib/python3.7/site-packages (from numba>=0.43.0->librosa) (58.0.4)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/hydroxide/.local/lib/python3.7/site-packages (from packaging>=20.0->librosa) (2.4.7)\n",
      "Requirement already satisfied: requests in /home/hydroxide/.local/lib/python3.7/site-packages (from pooch>=1.0->librosa) (2.25.1)\n",
      "Requirement already satisfied: appdirs in /home/hydroxide/.local/lib/python3.7/site-packages (from pooch>=1.0->librosa) (1.4.4)\n",
      "Requirement already satisfied: six>=1.3 in /home/hydroxide/.local/lib/python3.7/site-packages (from resampy>=0.2.2->librosa) (1.15.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/hydroxide/.local/lib/python3.7/site-packages (from scikit-learn!=0.19.0,>=0.14.0->librosa) (2.2.0)\n",
      "Requirement already satisfied: cffi>=1.0 in /home/hydroxide/.local/lib/python3.7/site-packages (from soundfile>=0.10.2->librosa) (1.14.6)\n",
      "Requirement already satisfied: pycparser in /home/hydroxide/.local/lib/python3.7/site-packages (from cffi>=1.0->soundfile>=0.10.2->librosa) (2.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/hydroxide/.local/lib/python3.7/site-packages (from requests->pooch>=1.0->librosa) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/hydroxide/.local/lib/python3.7/site-packages (from requests->pooch>=1.0->librosa) (2021.5.30)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/hydroxide/.local/lib/python3.7/site-packages (from requests->pooch>=1.0->librosa) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/hydroxide/.local/lib/python3.7/site-packages (from requests->pooch>=1.0->librosa) (1.26.6)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1.9.0+cu102'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install soundfile\n",
    "!pip install pandas\n",
    "!pip install scipy\n",
    "!pip install librosa\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import scipy\n",
    "from scipy import fft\n",
    "import functools\n",
    "from torch import nn\n",
    "import librosa\n",
    "import math\n",
    "import torchaudio \n",
    "import math\n",
    "import yaml\n",
    "torch.__version__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "03235647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dir should be structured as follows:\n",
    "# - speaker_id_1/\n",
    "# - speaker_id_1/sample_id1.wav\n",
    "# - speaker_id_1/sample_id1.csv\n",
    "# - speaker_id_1/sample_id2.wav\n",
    "# - speaker_id_1/sample_id2.wav\n",
    "# - speaker_id_2/sample_id1.wav\n",
    "# ..\n",
    "class VisemeDataset(Dataset):\n",
    "    def __init__(self, data_dir, audio_transform, viseme_transform):\n",
    "        self.viseme_transform= viseme_transform\n",
    "        self.audio_transform = audio_transform\n",
    "        self.audio_files = []\n",
    "        self.visemes = []\n",
    "        self.processed = {}\n",
    "        for file in list(Path(data_dir).rglob(\"*.wav\")):\n",
    "            self.audio_files.append(file)\n",
    "            self.visemes.append(str(file).replace(\"wav\", \"csv\"))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx not in self.processed:\n",
    "            fft, num_samples, mask = self.audio_transform(self.audio_files[idx])\n",
    "            fft = torch.tensor(fft, dtype=torch.float)\n",
    "            viseme_filename = self.visemes[idx]\n",
    "            visemes = torch.tensor(self.viseme_transform(viseme_filename).values.astype(np.float32))       \n",
    "            #assert(visemes.shape[0] == fft.shape[0])\n",
    "            self.processed[idx] =fft, torch.tensor(mask), visemes, viseme_filename\n",
    "        return self.processed[idx]\n",
    "\n",
    "class SeparableConv1d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, bias=False):\n",
    "        super(SeparableConv1d, self).__init__()\n",
    "        self.depthwise = nn.Conv1d(in_channels, in_channels, kernel_size=kernel_size, \n",
    "                               groups=in_channels, bias=bias, padding=1)\n",
    "        self.pointwise = nn.Conv1d(in_channels, out_channels, \n",
    "                               kernel_size=1, bias=bias)\n",
    "    def forward(self, x):\n",
    "        out = self.depthwise(x)\n",
    "        out = self.pointwise(out)\n",
    "        return out\n",
    "    \n",
    "class Conv1dModel(nn.Module):\n",
    "    def __init__(self, seq_length=633, n_ffts=2457, num_viseme=4, ks=256):\n",
    "        super(Conv1dModel, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            SeparableConv1d(seq_length,seq_length,ks),\n",
    "            nn.ReLU(),\n",
    "            SeparableConv1d(seq_length,seq_length,ks),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.attention = nn.MultiheadAttention(1951, 1, batch_first=True)\n",
    "        #self.linear_relu_stack2 = nn.Sequential(\n",
    "        #    SeparableConv1d(seq_length,seq_length,ks),\n",
    "        #    #nn.Linear(seq_length, seq_length),\n",
    "        #    nn.ReLU(),7\n",
    "        #)\n",
    "        self.linear_out = nn.Linear(1951, 11)\n",
    "\n",
    "    def forward(self, x):\n",
    "        o1 = self.conv(x)\n",
    "        attn_output, attn_output_weights = self.attention(o1, o1, o1)\n",
    "        attn_output = attn_output.tile((4,1,1,1)).transpose(0,1)\n",
    "        o1 = self.linear_out(attn_output)        \n",
    "        return o1\n",
    "\n",
    "class BiLSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim=None, hidden_size=512, num_viseme=4):\n",
    "        super(BiLSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = torch.nn.LSTM(input_dim, hidden_size, 1, bidirectional=True)\n",
    "        self.attention = nn.MultiheadAttention(hidden_size*2, 1, batch_first=True)       \n",
    "        self.l1 = torch.nn.Linear(hidden_size*2, 1)\n",
    "        self.l2 = torch.nn.Linear(hidden_size*2, 1)\n",
    "        self.l3 = torch.nn.Linear(hidden_size*2, 1)\n",
    "        self.l4 = torch.nn.Linear(hidden_size*2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out_f, _ = self.lstm(x)\n",
    "        #out_f = out_f[:,:,:self.embed_dim] + out_f[:,:,self.embed_dim:]\n",
    "        attn_output, attn_output_weights = self.attention(out_f, out_f, out_f)\n",
    "        return torch.stack([\n",
    "            self.l1(attn_output),\n",
    "            self.l2(attn_output),\n",
    "            self.l3(attn_output),\n",
    "            self.l4(attn_output)\n",
    "        ],dim=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "5c3e88b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_viseme(csv, pad_len_in_secs=None, target_framerate=None, blendshapes=None):\n",
    "    csv = pd.read_csv(csv)\n",
    "    \n",
    "    # first, drop every nth row to reduce effective framerate    \n",
    "    csv = csv.iloc[::int(59.97 / target_framerate)]\n",
    "    \n",
    "    pad_len = int(pad_len_in_secs * target_framerate)\n",
    "        \n",
    "    if(csv.shape[0] < pad_len):\n",
    "        pad = pd.DataFrame(0, index=[i for i in range(pad_len - csv.shape[0])], columns=csv.columns)\n",
    "        pad.pad(inplace=True)\n",
    "        csv = pd.concat([csv, pad])\n",
    "    else:\n",
    "        csv = csv.iloc[:pad_len]\n",
    "        #print(\"Visemes exceeded max length, truncate?\")\n",
    "    columns = list(csv.columns)\n",
    "    columns.remove(\"Timecode\")\n",
    "    \n",
    "    return csv[blendshapes] if blendshapes is not None else csv\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "83dcec77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_audio(audio, sample_rate, pad_len_in_secs):\n",
    "    # left-pad the audio so we have the left context when starting at the initial viseme\n",
    "    pad_len_in_samples = pad_len_in_secs * sample_rate \n",
    "    if len(audio.shape) > 1:\n",
    "        audio = audio[0]\n",
    "    if audio.shape[0] < pad_len_in_samples:\n",
    "        audio = np.pad(audio, (0, (pad_len_in_secs * sample_rate) - audio.shape[0]), constant_values=0.001)\n",
    "    elif audio.shape[0] > pad_len_in_samples:\n",
    "        audio = audio[:pad_len_in_samples]\n",
    "    #audio = np.hstack([np.zeros((win_length*2,)), audio])\n",
    "    return audio\n",
    "\n",
    "def load_and_pad_audio(filepath, resample_to, pad_len_in_secs):\n",
    "    audio, rate = sf.read(filepath)\n",
    "    audio = librosa.resample(audio, rate, resample_to)\n",
    "    audio = pad_audio(audio, resample_to, pad_len_in_secs)\n",
    "    return audio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79c5e19",
   "metadata": {},
   "source": [
    "\\---S1---/\\---S2---/\\---S3---/\n",
    "WWWWWWWWWWWWWWWWWWWWWWWWWWWWWW\n",
    "____________\\---V1---/\n",
    "_____________AAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\n",
    "\n",
    "             \\---S1---/\\---S2---/\\---S3---/\n",
    "             WWWWWWWWWWWWWWWWWWWWWWWWWWWWWW\n",
    "______________________\\---V2---/\n",
    "_____________AAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\n",
    "\n",
    "\n",
    "A = audio sample\n",
    "V1, V2, etc = viseme 1, viseme 2, etc (aka \"frame\", but not in the MFCC sense)\n",
    "W = a window of audio samples that will be used as input (aka \"frame\" in the MFCC sense. to avoid confusion, we refer to this as the \"window\" and the viseme as the \"frame\")\n",
    "S1, S2, S3 = STFT of a window \n",
    "num_frames == num_windows\n",
    "\n",
    "audio_bins_per_window = the number of S per W\n",
    "samples_per_bin = the length of each S (i.e. number of As)\n",
    "bin_hop_length = the number of A between S1 and S2 (in the picture above, hop_length == len(S1) == len(S2) == samples_per_bin\n",
    "\n",
    "in practice, we calculate as such:\n",
    "          /-----------V2--------------\\\n",
    "/--------------V1------------\\\n",
    "\\---S1---/\\---S2---/\\---S3---/\\---S4---/\\---S5---/\\---S6---/\n",
    "_______________AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\n",
    "\n",
    "then at V1, we take {S1,S2,S3}, at V2 we take {S4,S5,S6}, etc\n",
    "\n",
    "# STFT hop length should equal to length of a single viseme frame, in samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "29094c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stft(filepath, \n",
    "         frame_len, \n",
    "         window_len, \n",
    "         stft_frames_per_window,\n",
    "         resample_to, \n",
    "         pad_len_in_secs, \n",
    "         n_mels):\n",
    "    \n",
    "    audio = load_and_pad_audio(filepath, pad_len_in_secs, resample_to)\n",
    "    #wdw = np.hanning(audio_window_in_samples)\n",
    "    \n",
    "    num_frames = audio.shape[0] // frame_len\n",
    "    # TODO - mask for padding \n",
    "    # [1] * actual_seq_length + [0] * (padded_seq_length - actual_seq_length)\n",
    "    \n",
    "    # take the STFT of the entire audio file \n",
    "    # with a window size equivalent to the audio_window_in_samples / audio_bins_per_window\n",
    "    n_fft = int(window_len / stft_frames_per_window)\n",
    "\n",
    "    transformed = librosa.stft(audio, \n",
    "                               n_fft=n_fft,\n",
    "                               win_length=n_fft,\n",
    "                               hop_length=n_fft)\n",
    "    \n",
    "    melfb = librosa.filters.mel(resample_to, n_fft, n_mels=n_mels)    \n",
    "    mels = np.dot(melfb, np.abs(transformed))\n",
    "    \n",
    "    log_mels = np.log(mels)\n",
    "    \n",
    "    output = mels_to_mfccs(log_mels, padded_seq_length, stft_frames_per_window, n_mels)\n",
    "\n",
    "    return output, audio.shape[0], \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "c0de78e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let num_stft_frames be the number of STFT frames per audio sample \n",
    "# num_stft_frames == (audio_len / n_fft)\n",
    "# (assuming FFT window length == hop length == n_ffts)\n",
    "# coeffs will be B x num_stft_frames x num_mels\n",
    "# every \n",
    "# if we've calculated the FFT size correctly, the hop length will be half \n",
    "# so stft_frames_per_window should be odd\n",
    "\n",
    "def coeffs_to_windows(coeffs, num_frames, stft_frames_per_window, n_mels):\n",
    "    \n",
    "    output = np.zeros((num_frames, stft_frames_per_window*n_mels))\n",
    "    \n",
    "    # the number of frames to hop is just half the number of STFT frames per window\n",
    "    hop_len = int(stft_frames_per_window / 2)\n",
    "    \n",
    "    for i in range(num_frames):\n",
    "        c = coeffs[:, (i*hop_len):(i*hop_len)+stft_frames_per_window]\n",
    "        # padding\n",
    "        if(c.shape[1] < stft_frames_per_window):\n",
    "            c = np.pad(c, [(0,0), (0, stft_frames_per_window - c.shape[1])], constant_values=0)\n",
    "        \n",
    "        output[i, :] = np.reshape(c, stft_frames_per_window*n_mels)\n",
    "    return output\n",
    "\n",
    "\n",
    "# featurize audio in exactly the same way as TensorflowTTS\n",
    "# this enables mels from the synthesis step to be reused for viseme prediction\n",
    "\n",
    "def tftts_mels(filepath, \n",
    "               config=None):\n",
    "    \n",
    "    audio = load_and_pad_audio(filepath, config[\"sampleRate\"], config[\"paddedAudioLength\"])\n",
    "     \n",
    "    # this is (mostly) copied verbatim from https://github.com/TensorSpeech/TensorFlowTTS/blob/master/tensorflow_tts/bin/preprocess.py @ 4a7d584 \n",
    "    # except the hop length must be the size of the window (in samples) divided by the number of STFT frames per window\n",
    "    assert(config[\"hopSize\"] == int(config[\"windowLength\"] // config[\"stftFramesPerWindow\"]))\n",
    "    \n",
    "    D = librosa.stft(\n",
    "        audio,\n",
    "        n_fft=config[\"fftSize\"],\n",
    "        hop_length=config[\"hopSize\"],\n",
    "        win_length=config[\"fftSize\"],\n",
    "        #config[\"windowLength\"] if \"windowLength\" in config else config[\"fftSize\"],\n",
    "        window=config[\"window\"],\n",
    "        pad_mode=\"reflect\",\n",
    "    )\n",
    "    \n",
    "    S, _ = librosa.magphase(D)  # (#bins, #frames)\n",
    "    fmin = 0 if config[\"fmin\"] is None else config[\"fmin\"]\n",
    "    fmax = sampling_rate // 2 if config[\"fmax\"] is None else config[\"fmax\"]\n",
    "    mel_basis = librosa.filters.mel(\n",
    "        sr=config[\"sampleRate\"],\n",
    "        n_fft=config[\"fftSize\"],\n",
    "        n_mels=config[\"numMels\"],\n",
    "        fmin=fmin,\n",
    "        fmax=fmax,\n",
    "    )\n",
    "    log_mels = np.log10(np.maximum(np.dot(mel_basis, S), 1e-10)).T  # (#frames, #bins)\n",
    "    \n",
    "    # coeffs = fft.dct(log_mels.T) <-- where did this come from?\n",
    "    \n",
    "    coeffs = log_mels.T\n",
    "    \n",
    "    num_frames = (config[\"sampleRate\"] * config[\"paddedAudioLength\"]) // config[\"frameLength\"]\n",
    "\n",
    "    output = coeffs_to_windows(coeffs, num_frames,  config[\"stftFramesPerWindow\"], config[\"numMels\"])\n",
    "\n",
    "    return output, audio.shape[0], [1] * log_mels.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "07ffb6e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MouthClose', 'MouthFunnel', 'MouthPucker', 'JawOpen']\n",
      "Dumping\n"
     ]
    }
   ],
   "source": [
    "from tensorflow_tts.inference import AutoConfig\n",
    "import json\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "mappings = OrderedDict()\n",
    "mappings[\"MouthClose\"] = \"Mouth_Lips_Open\"\n",
    "mappings[\"MouthFunnel\"] = \"A29_Mouth_Funnel\"\n",
    "mappings[\"MouthPucker\"] = \"A30_Mouth_Pucker\"\n",
    "mappings[\"JawOpen\"] = \"Mouth_Open\"\n",
    "print(list(mappings.keys()))\n",
    "# config for the input blendshapes\n",
    "source_config = {\n",
    "    # source framerate for raw viseme label input\n",
    "    \"framerate\":59.97,\n",
    "    # we need to map the blendshapes from the incoming CSV from LiveLinkFace to CC3 blendshapes\n",
    "    # unfortunately not 1-to-1, but seems to work well enough\n",
    "    \"mappings\":mappings\n",
    "}\n",
    " \n",
    "# config for the viseme model\n",
    "model_config = {}\n",
    "# the filename for the trained model that we will export below\n",
    "model_config[\"modelPath\"] = \"bilstm.tflite\"\n",
    "# the blendshapes that will be predicted\n",
    "# we also need to export this so the gltf animator can match the model outputs indices to morph target indices\n",
    "model_config[\"targetNames\"] = [\"Mouth_Lips_Open\", \"A29_Mouth_Funnel\", \"A30_Mouth_Pucker\", \"Mouth_Open\"]\n",
    "\n",
    "# actual framerate to use for viseme labels. \n",
    "# raw labels will be resampled/transformed (either averaged or simply dropped).\n",
    "model_config[\"frameRate\"] = source_config[\"framerate\"] / 2\n",
    "# the sample rate that audio will be resampled to\n",
    "model_config[\"sampleRate\"] = 22050\n",
    "\n",
    "# the duration of a viseme frame is (1 / target_framerate) seconds\n",
    "model_config[\"frameLength\"] = math.ceil(model_config[\"sampleRate\"] * (1 / model_config[\"frameRate\"]))\n",
    "# all audio will be padded to the following size\n",
    "model_config[\"paddedAudioLength\"] = 10\n",
    "# the raw input for each viseme frame will be an audio window of size X \n",
    "# the middle sample of the viseme frame is aligned with the middle sample of the audio window\n",
    "# this means, at the nominal \"anchor sample\" of the viseme frame, there will be \n",
    "# X/2 samples to the left and X/2 samples to the right\n",
    "# let's use 0.5 seconds\n",
    "model_config[\"windowLength\"] = int(0.5 * model_config[\"sampleRate\"]) \n",
    "\n",
    "# this raw audio input will then be transformed into a number of STFT frames/coefficients\n",
    "# each viseme frame will have this number of STFT frames, which will be the actual input at each timestep\n",
    "# Since audio windows overlap, we won't want to waste cycles repeatedly computing the STFT across the whole audio sequence\n",
    "# So in practice, we pre-calculate the STFT for the whole sequence, then just sub-sample the coefficients at each timestep\n",
    "# when assigning STFT frames, the hop length will then just be half this value\n",
    "model_config[\"stftFramesPerWindow\"] = 33\n",
    "\n",
    "model_config[\"hopSize\"] = int(model_config[\"windowLength\"] // model_config[\"stftFramesPerWindow\"])\n",
    "\n",
    "# load the TTS config so we can match the same parameters \n",
    "# this may override some of the parameters set above\n",
    "USE_TFTTS_CONFIG=True\n",
    "if USE_TFTTS_CONFIG:\n",
    "    with open(\"/mnt/hdd_2tb/home/hydroxide/projects/TensorFlowTTS/preprocess/baker_preprocess.yaml\", \"r\") as f:\n",
    "        tftts_config = yaml.safe_load(f)\n",
    "        model_config[\"fftSize\"] = tftts_config[\"fft_size\"]\n",
    "        model_config[\"numMels\"] = tftts_config[\"num_mels\"]       \n",
    "        if \"window_length\" in tftts_config:\n",
    "            assert(tftts_config[\"window_length\"] == model_config[\"windowLength\"])\n",
    "        \n",
    "        model_config[\"window\"] = tftts_config[\"window\"]\n",
    "        model_config[\"fmin\"] = tftts_config[\"fmin\"]\n",
    "        model_config[\"fmax\"] = tftts_config[\"fmax\"]\n",
    "        assert(tftts_config[\"sampling_rate\"] == model_config[\"sampleRate\"])\n",
    "        \n",
    "        #config={\"num_mels\":80, \"sampling_rate\":22050,\"fmin\":80,\"fmax\":6000, \"window\":\"hann\", \"fft_size\":512, \"hop_size\":hop_size})\n",
    "\n",
    "#seq_length = math.ceil((pad_len_in_secs * resample_to) / frame_len)\n",
    "\n",
    "# save the config so we can pass\n",
    "with open(\"viseme_model.json\", \"w\",encoding=\"utf-8\") as outfile:\n",
    "    print(\"Dumping\")\n",
    "    json.dump(model_config, outfile)\n",
    "    \n",
    "process_audio = None\n",
    "\n",
    "if USE_TFTTS_CONFIG:\n",
    "    process_audio = functools.partial(tftts_mels, \n",
    "                                          config=model_config)\n",
    "else:\n",
    "    num_mels=39\n",
    "    process_audio = functools.partial(stft, model_config);\n",
    "        #viseme_frame_len_in_samples=viseme_frame_len_in_samples, # this refers to the size of the viseme/audio window,\n",
    "        #audio_window_in_samples=audio_window_in_samples, # TODO - update these\n",
    "        #stft_frames_per_window=stft_frames_per_window,\n",
    "        #resample_to=resample_to, \n",
    "        #pad_len_in_secs=pad_len_in_secs,\n",
    "        #n_mels=num_mels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "d5316a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'modelPath': 'bilstm.tflite',\n",
       " 'targetNames': ['Mouth_Lips_Open',\n",
       "  'A29_Mouth_Funnel',\n",
       "  'A30_Mouth_Pucker',\n",
       "  'Mouth_Open'],\n",
       " 'frameRate': 29.985,\n",
       " 'sampleRate': 22050,\n",
       " 'frameLength': 736,\n",
       " 'paddedAudioLength': 10,\n",
       " 'windowLength': 11025,\n",
       " 'stftFramesPerWindow': 33,\n",
       " 'hopSize': 334,\n",
       " 'fftSize': 512,\n",
       " 'numMels': 80,\n",
       " 'window': 'hann',\n",
       " 'fmin': 80,\n",
       " 'fmax': 7600}"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 20\n",
    "    \n",
    "process_viseme = functools.partial(preprocess_viseme, \n",
    "                                   pad_len_in_secs=model_config[\"paddedAudioLength\"], \n",
    "                                   blendshapes=source_config[\"mappings\"], \n",
    "                                   target_framerate=model_config[\"frameRate\"])\n",
    "\n",
    "training_data = VisemeDataset(\"./data/training/speaker_1/\", \n",
    "                              process_audio, \\\n",
    "                              process_viseme)\n",
    "test_data = VisemeDataset(\"./data/test/speaker_1/\", \n",
    "                              process_audio, \\\n",
    "                              process_viseme)\n",
    "def collate_samples(feat_tuples):\n",
    "    return feat_tuples\n",
    "    padded = torch.nn.utils.rnn.pad_sequence([f[0] for f in feat_tuples], batch_first=True, padding_value=0.0)\n",
    "    #mask = torch.stack([feat_tuples[i][1] for i in range(len(feat_tuples))])\n",
    "    labels = torch.nn.utils.rnn.pad_sequence([f[2] for f in feat_tuples], batch_first=True, padding_value=0.0)\n",
    "    viseme_filenames = [feat_tuples[i][3] for i in range(len(feat_tuples))]\n",
    "    \n",
    "    return padded, labels,viseme_filenames\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Using {} device'.format(device))\n",
    "\n",
    "model_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c102a5f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2640\n",
      "torch.Size([20, 299, 4])\n",
      "Step 0 Avg loss: 0.000343509241938591\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hydroxide/.local/lib/python3.7/site-packages/ipykernel_launcher.py:39: UserWarning: Using a target size (torch.Size([1, 299, 4])) that is different to the input size (torch.Size([299, 4])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "Step 50 Avg loss: 3.736698604337871\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([1, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([1, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "Step 100 Avg loss: 0.007803051685914397\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([1, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "Step 150 Avg loss: 0.010470346729271114\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([1, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([1, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "Step 200 Avg loss: 0.008922593574970961\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([1, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "Step 250 Avg loss: 0.009413444502279163\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([1, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([1, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "Step 300 Avg loss: 0.009076503666583448\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([1, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 299, 4])\n",
      "Step 350 Avg loss: 0.008761207442730665\n",
      "torch.Size([1, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([1, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "Step 400 Avg loss: 0.007683979766443372\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([1, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([1, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "Step 450 Avg loss: 0.00794620679691434\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([1, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "Step 500 Avg loss: 0.008131830915808679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hydroxide/.local/lib/python3.7/site-packages/ipykernel_launcher.py:63: UserWarning: Using a target size (torch.Size([20, 299, 4])) that is different to the input size (torch.Size([20, 1, 299, 4])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "/home/hydroxide/.local/lib/python3.7/site-packages/ipykernel_launcher.py:63: UserWarning: Using a target size (torch.Size([2, 299, 4])) that is different to the input size (torch.Size([2, 1, 299, 4])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss 0.028751344420015812\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([1, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([1, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "Step 550 Avg loss: 0.007516261702403426\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([1, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "Step 600 Avg loss: 0.007595401974394917\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([1, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([1, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "Step 650 Avg loss: 2.5327121409680693\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([1, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "Step 700 Avg loss: 0.6444571202900261\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([1, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([1, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "Step 750 Avg loss: 0.011078346306458115\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([1, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([1, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "Step 800 Avg loss: 0.009142196010798216\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([1, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "Step 850 Avg loss: 0.00834678984247148\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([1, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([1, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "Step 900 Avg loss: 0.009431205596774817\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([1, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "Step 950 Avg loss: 0.008922347966581583\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([1, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([1, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "Step 1000 Avg loss: 0.008777843001298606\n",
      "Test loss 0.029481034725904465\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([1, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "Step 1050 Avg loss: 0.008392773229861632\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([1, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([1, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "Step 1100 Avg loss: 0.007722444757819176\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([1, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "Step 1150 Avg loss: 0.007972445520572365\n",
      "torch.Size([1, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([1, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "Step 1200 Avg loss: 0.007975234137848019\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([1, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([1, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "Step 1250 Avg loss: 0.007568665463477373\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([1, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "Step 1300 Avg loss: 0.00772693632170558\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([1, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([1, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "Step 1350 Avg loss: 0.007648192793130875\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([1, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "Step 1400 Avg loss: 0.00739929785951972\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([1, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([1, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "Step 1450 Avg loss: 0.007618127716705203\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([1, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "Step 1500 Avg loss: 0.007463487347122282\n",
      "Test loss 0.02560475282371044\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([1, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([1, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "Step 1550 Avg loss: 0.0074356370605528355\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([1, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([1, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "Step 1600 Avg loss: 0.007384797129780054\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([1, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "Step 1650 Avg loss: 0.007204031171277166\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n",
      "torch.Size([20, 299, 4])\n"
     ]
    }
   ],
   "source": [
    "input_dim = int(model_config[\"windowLength\"] / model_config[\"hopSize\"] * model_config[\"numMels\"])\n",
    "#print(stft_frames_per_window)\n",
    "#print(tftts_config[\"hop_size\"])\n",
    "print(input_dim)\n",
    "model = BiLSTMModel(hidden_size=512, input_dim=input_dim).to(device)\n",
    "\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "num_steps = 100000\n",
    "print_loss_every = 50\n",
    "eval_every = 500\n",
    "\n",
    "batch = iter(train_dataloader)\n",
    "accum_loss = 0\n",
    "\n",
    "for t in range(num_steps):\n",
    "    optimizer.zero_grad()\n",
    "    train_features, train_mask, train_labels, train_files = next(batch, (None,None,None,None))\n",
    "    #print(train_features.shape)\n",
    "    #print(train_labels.shape)\n",
    "\n",
    "    if train_features is None:\n",
    "        batch = iter(train_dataloader)\n",
    "        train_features, train_mask , train_labels, _ = next(batch)\n",
    "    \n",
    "    #train_mask = torch.unsqueeze(train_mask, 2)\n",
    "    \n",
    "    x = train_features.to(device)     \n",
    "    \n",
    "    y = train_labels.to(device) #* train_mask.to(device)\n",
    "    \n",
    "    preds = model(x) #* train_mask.to(device)\n",
    "    \n",
    "    #print(preds.shape)\n",
    "    \n",
    "    preds = torch.transpose(preds, 1,3).squeeze()\n",
    "    #print(f\"x {x.shape} preds {preds.shape} y {y.shape}\")    \n",
    "    #loss = torch.nn.functional.cross_entropy(preds, y)\n",
    "    loss = torch.nn.functional.huber_loss(preds, y)\n",
    "    if len(preds.shape) < 3:\n",
    "        preds = torch.unsqueeze(preds, 0)\n",
    "    #print(preds.shape)\n",
    "    for i in range(preds.shape[1] - 1):\n",
    "        loss += torch.nn.functional.cosine_embedding_loss(preds[:,i,:], preds[:,i+1,:], (torch.ones(preds.shape[0])).to(device))\n",
    "    accum_loss += loss.item()\n",
    "    if t % print_loss_every == 0:\n",
    "        print(f\"Step {t} Avg loss: {accum_loss / print_loss_every}\")\n",
    "        accum_loss = 0\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    \n",
    "    \n",
    "    if t > 0 and t % eval_every == 0:\n",
    "        accum_loss = 0\n",
    "        for test_features, test_mask, test_labels, _ in iter(test_dataloader):\n",
    "            x = test_features.to(device)\n",
    "            #x = torch.transpose(x, 1, 2)\n",
    "            #x = torch.unsqueeze(x, dim=3)\n",
    "            y = test_labels.to(device)\n",
    "            preds = model(x)\n",
    "            preds = torch.transpose(preds, 1,3)\n",
    "            accum_loss += torch.nn.functional.mse_loss(preds, y).item()\n",
    "            #accum_loss = torch.nn.functional.cross_entropy(preds, y)\n",
    "\n",
    "        print(f\"Test loss {accum_loss}\")\n",
    "        accum_loss = 0\n",
    "    \n",
    "#pred_probab = nn.Softmax(dim=1)(logits)\n",
    "#y_pred = pred_probab.argmax(1)\n",
    "#print(f\"Predicted class: {y_pred}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41510411",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_features, test_mask, test_labels, test_files = next(iter(test_dataloader))\n",
    "#test_features, test_mask, test_labels, test_files = next(iter(train_dataloader))\n",
    "##test_features = torch.transpose(test_features, 1, 2)\n",
    "test_features = train_features\n",
    "export_y_batch = model(test_features.to(device)) \n",
    "export_y = export_y_batch[0,:,:]\n",
    "print(export_y.shape)\n",
    "header = \"Timecode,BlendShapeCount,eyeBlinkRight,eyeLookDownRight,eyeLookInRight,eyeLookOutRight,eyeLookUpRight,eyeSquintRight,eyeWideRight,eyeBlinkLeft,eyeLookDownLeft,eyeLookInLeft,eyeLookOutLeft,eyeLookUpLeft,eyeSquintLeft,eyeWideLeft,jawForward,jawRight,jawLeft,jawOpen,mouthClose,mouthFunnel,mouthPucker,mouthRight,mouthLeft,mouthSmileRight,mouthSmileLeft,mouthFrownRight,mouthFrownLeft,mouthDimpleRight,mouthDimpleLeft,mouthStretchRight,mouthStretchLeft,mouthRollLower,mouthRollUpper,mouthShrugLower,mouthShrugUpper,mouthPressRight,mouthPressLeft,mouthLowerDownRight,mouthLowerDownLeft,mouthUpperUpRight,mouthUpperUpLeft,browDownRight,browDownLeft,browInnerUp,browOuterUpRight,browOuterUpLeft,cheekPuff,cheekSquintRight,cheekSquintLeft,noseSneerRight,noseSneerLeft,tongueOut,HeadYaw,HeadPitch,HeadRoll,LeftEyeYaw,LeftEyePitch,LeftEyeRoll,RightEyeYaw,RightEyePitch,RightEyeRoll\".split(',')\n",
    "selected_output_indices = [header.index(x[0].lower() + x[1:]) for x in blendshapes]\n",
    "num_visemes = len(blendshapes)\n",
    "with open(\"output.csv\", \"w\") as outfile:\n",
    "    outfile.write(\",\".join(header) + \"\\n\")\n",
    "    timer_ms = 0\n",
    "    print(export_y)\n",
    "    for t in range(export_y.shape[1]):\n",
    "        output = [str(0)] * len(header)\n",
    "        second = str(int(timer_ms // 1000)).zfill(2)\n",
    "        frame = (timer_ms % 1000) * target_framerate / 1000\n",
    "        output[0] = f\"00:00:{second}:{frame}\"\n",
    "        for viseme in range(num_visemes): \n",
    "            output[selected_output_indices[viseme]] = str(export_y[viseme,t,:].item())\n",
    "        timer_ms += (1 / target_framerate) * 1000\n",
    "        outfile.write(\",\".join(output) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923dc158",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16652a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_header = \"Timecode,BlendShapeCount,EyeBlinkLeft,EyeLookDownLeft,EyeLookInLeft,EyeLookOutLeft,EyeLookUpLeft,EyeSquintLeft,EyeWideLeft,EyeBlinkRight,EyeLookDownRight,EyeLookInRight,EyeLookOutRight,EyeLookUpRight,EyeSquintRight,EyeWideRight,JawForward,JawRight,JawLeft,JawOpen,MouthClose,MouthFunnel,MouthPucker,MouthRight,MouthLeft,MouthSmileLeft,MouthSmileRight,MouthFrownLeft,MouthFrownRight,MouthDimpleLeft,MouthDimpleRight,MouthStretchLeft,MouthStretchRight,MouthRollLower,MouthRollUpper,MouthShrugLower,MouthShrugUpper,MouthPressLeft,MouthPressRight,MouthLowerDownLeft,MouthLowerDownRight,MouthUpperUpLeft,MouthUpperUpRight,BrowDownLeft,BrowDownRight,BrowInnerUp,BrowOuterUpLeft,BrowOuterUpRight,CheekPuff,CheekSquintLeft,CheekSquintRight,NoseSneerLeft,NoseSneerRight,TongueOut,HeadYaw,HeadPitch,HeadRoll,LeftEyeYaw,LeftEyePitch,LeftEyeRoll,RightEyeYaw,RightEyePitch,RightEyeRoll\"\n",
    "remap = {h:(h[0].lower() + h[1:]) if h not in [\"Timecode\",\"BlendShapeCount\",\"HeadYaw\",\"HeadPitch\",\"HeadRoll\",\"LeftEyeYaw\",\"LeftEyePitch\",\"LeftEyeRoll\",\"RightEyeYaw\",\"RightEyePitch\",\"RightEyeRoll\"]  else h for h in new_header.split(\",\") }\n",
    "for oh in remap.values():\n",
    "    if oh not in header:\n",
    "        print(oh)\n",
    "\n",
    "def new_to_old(csv_df):\n",
    "    return csv_df.rename(columns=remap)\n",
    "df = preprocess_viseme(\"data/training/speaker_1/20210824_1/61.csv\", pad_len_in_secs=pad_len_in_secs, \n",
    "                                   resample_to=target_framerate)\n",
    "#print(df)\n",
    "df = new_to_old(df)\n",
    "cols = [c for c in df.columns if c not in [\"jawOpen\", \"mouthClose\", \"Timecode\"]]\n",
    "df[cols] = 0\n",
    "df = df[df[\"Timecode\"] != 0]\n",
    "df.to_csv(\"original.csv\", index=False)\n",
    "#new_to_old(df)[header].to_csv(\"original.csv\", index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05231fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00801822",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"bilstm.torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c58f7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in [\"MouthClose\", \"MouthFunnel\", \"MouthPucker\", \"JawOpen\"]:\n",
    "    print(new_header.split(\",\").index(item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e9cfa7f",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Can't get attribute 'BiLSTMModel' on <module '__main__'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_109087/1031538405.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bilstm.torch\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    605\u001b[0m                     \u001b[0mopened_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_position\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 607\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    608\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_legacy_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[1;32m    880\u001b[0m     \u001b[0munpickler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUnpicklerWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m     \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersistent_load\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpersistent_load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 882\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_loaded_sparse_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mfind_class\u001b[0;34m(self, mod_name, name)\u001b[0m\n\u001b[1;32m    873\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mfind_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmod_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    874\u001b[0m             \u001b[0mmod_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_module_mapping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmod_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmod_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 875\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmod_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    876\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m     \u001b[0;31m# Load the data (which may in turn use `persistent_load` to load tensors)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: Can't get attribute 'BiLSTMModel' on <module '__main__'>"
     ]
    }
   ],
   "source": [
    "torch.load(\"bilstm.torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70887b32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hydroxide/.local/lib/python3.7/site-packages/torch/onnx/utils.py:1189: UserWarning: Provided key modelInput for dynamic axes is not a valid input/output name\n",
      "  warnings.warn(\"Provided key {} for dynamic axes is not a valid input/output name\".format(key))\n",
      "/home/hydroxide/.local/lib/python3.7/site-packages/torch/onnx/utils.py:1189: UserWarning: Provided key modelOutput for dynamic axes is not a valid input/output name\n",
      "  warnings.warn(\"Provided key {} for dynamic axes is not a valid input/output name\".format(key))\n",
      "/home/hydroxide/.local/lib/python3.7/site-packages/torch/onnx/symbolic_opset9.py:2099: UserWarning: Exporting a model to ONNX with a batch_size other than 1, with a variable length with LSTM can cause an error when running the ONNX model with a different batch size. Make sure to save the model with a batch size of 1, or define the initial states (h0/c0) as inputs of the model. \n",
      "  \"or define the initial states (h0/c0) as inputs of the model. \")\n",
      "/home/hydroxide/.local/lib/python3.7/site-packages/tensorflow_addons/utils/ensure_tf_install.py:67: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.3.0 and strictly below 2.6.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.6.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  UserWarning,\n",
      "2021-09-19 21:38:24.736468: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-09-19 21:38:24.744054: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-09-19 21:38:24.744503: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-09-19 21:38:24.745736: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-19 21:38:24.746122: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-09-19 21:38:24.751568: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-09-19 21:38:24.751866: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-09-19 21:38:25.140124: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-09-19 21:38:25.140511: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-09-19 21:38:25.140809: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-09-19 21:38:25.141081: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7394 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From /home/hydroxide/.local/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py:463: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /home/hydroxide/.local/lib/python3.7/site-packages/tensorflow/python/ops/rnn.py:447: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /home/hydroxide/.local/lib/python3.7/site-packages/keras/layers/legacy_rnn/rnn_cell_impl.py:979: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hydroxide/.local/lib/python3.7/site-packages/keras/layers/legacy_rnn/rnn_cell_impl.py:901: UserWarning: `tf.nn.rnn_cell.LSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
      "  warnings.warn(\"`tf.nn.rnn_cell.LSTMCell` is deprecated and will be \"\n",
      "/home/hydroxide/.local/lib/python3.7/site-packages/keras/engine/base_layer.py:2223: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
      "  warnings.warn('`layer.add_variable` is deprecated and '\n",
      "/home/hydroxide/.local/lib/python3.7/site-packages/keras/engine/base_layer.py:1348: UserWarning: `layer.updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`layer.updates` will be removed in a future version. '\n",
      "WARNING:absl:Function `__call__` contains input name(s) input.1 with unsupported characters which will be renamed to input_1 in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-19 21:38:29.819600: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hydroxide/.local/lib/python3.7/site-packages/keras/layers/legacy_rnn/rnn_cell_impl.py:901: UserWarning: `tf.nn.rnn_cell.LSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
      "  warnings.warn(\"`tf.nn.rnn_cell.LSTMCell` is deprecated and will be \"\n",
      "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
      "/home/hydroxide/.local/lib/python3.7/site-packages/keras/engine/base_layer.py:1348: UserWarning: `layer.updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`layer.updates` will be removed in a future version. '\n",
      "/home/hydroxide/.local/lib/python3.7/site-packages/keras/engine/base_layer.py:2223: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
      "  warnings.warn('`layer.add_variable` is deprecated and '\n",
      "WARNING:absl:Found untraced functions such as gen_tensor_dict while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./tf_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./tf_model/assets\n"
     ]
    }
   ],
   "source": [
    "model = torch.load(\"bilstm.torch\",map_location=torch.device('cpu'))\n",
    "import torch.onnx \n",
    "\n",
    "# set the model to inference mode \n",
    "model.eval() \n",
    "\n",
    "# Let's create a dummy input tensor  \n",
    "dummy_input = torch.randn(1, 119, 2640, requires_grad=False)  \n",
    "\n",
    "# Export the model   \n",
    "torch.onnx.export(model,         # model being run \n",
    "     dummy_input,       # model input (or a tuple for multiple inputs) \n",
    "     \"bilstm.onnx\",       # where to save the model  \n",
    "     export_params=True,  # store the trained parameter weights inside the model file \n",
    "     opset_version=10,    # the ONNX version to export the model to \n",
    "     do_constant_folding=True,  # whether to execute constant folding for optimization \n",
    "     #input_names = ['modelInput'],   # the model's input names \n",
    "     #output_names = ['modelOutput'], # the model's output names \n",
    "     dynamic_axes={'modelInput' : {0 : 'batch_size'},    # variable length axes \n",
    "    'modelOutput' : {0 : 'batch_size'}}\n",
    "                 ) \n",
    "import onnx\n",
    "from onnx_tf.backend import prepare\n",
    "import onnxruntime\n",
    "model_onnx = onnx.load('bilstm.onnx')\n",
    "tf_rep = prepare(model_onnx)\n",
    "tf_rep.export_graph('./tf_model')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df4666c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_model_optimization as tfmot\n",
    "\n",
    "quant_aware_model = tfmot.quantization.keras.quantize_model(tf_rep)\n",
    "quant_aware_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "953bfeb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-19 21:38:36.705918: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:351] Ignored output_format.\n",
      "2021-09-19 21:38:36.705948: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:354] Ignored drop_control_dependency.\n",
      "2021-09-19 21:38:36.705955: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:360] Ignored change_concat_input_ranges.\n",
      "2021-09-19 21:38:36.706713: I tensorflow/cc/saved_model/reader.cc:38] Reading SavedModel from: ./tf_model\n",
      "2021-09-19 21:38:36.733441: I tensorflow/cc/saved_model/reader.cc:90] Reading meta graph with tags { serve }\n",
      "2021-09-19 21:38:36.733478: I tensorflow/cc/saved_model/reader.cc:132] Reading SavedModel debug info (if present) from: ./tf_model\n",
      "2021-09-19 21:38:36.742094: I tensorflow/cc/saved_model/loader.cc:211] Restoring SavedModel bundle.\n",
      "2021-09-19 21:38:36.776605: I tensorflow/cc/saved_model/loader.cc:195] Running initialization op on SavedModel bundle at path: ./tf_model\n",
      "2021-09-19 21:38:36.791165: I tensorflow/cc/saved_model/loader.cc:283] SavedModel load for tags { serve }; Status: success: OK. Took 84453 microseconds.\n",
      "2021-09-19 21:38:36.894516: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:210] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2021-09-19 21:38:39.238587: W tensorflow/compiler/mlir/lite/flatbuffer_export.cc:1827] Graph contains the following resource op(s), that use(s) resource type. Currently, the resource type is not natively supported in TFLite. Please consider not using the resource type if there are issues with either TFLite converter or TFLite runtime:\n",
      "Resource ops: AssignVariableOp, ReadVariableOp, VarHandleOp\n",
      "Details:\n",
      "\ttf.AssignVariableOp(tensor<!tf.resource<tensor<2048xf32>>>, tensor<2048xf32>) -> ()\n",
      "\ttf.AssignVariableOp(tensor<!tf.resource<tensor<2048xf32>>>, tensor<2048xf32>) -> () : {device = \"\"}\n",
      "\ttf.AssignVariableOp(tensor<!tf.resource<tensor<?x?xf32>>>, tensor<1x1xf32>) -> ()\n",
      "\ttf.AssignVariableOp(tensor<!tf.resource<tensor<?x?xf32>>>, tensor<3152x2048xf32>) -> () : {device = \"\"}\n",
      "\ttf.ReadVariableOp(tensor<!tf.resource<tensor<2048xf32>>>) -> (tensor<2048xf32>) : {device = \"\"}\n",
      "\ttf.ReadVariableOp(tensor<!tf.resource<tensor<?x?xf32>>>) -> (tensor<?x?xf32>) : {device = \"\"}\n",
      "\ttf.VarHandleOp() -> (tensor<!tf.resource<tensor<2048xf32>>>) : {allowed_devices = [], container = \"\", device = \"\", shared_name = \"lstm_bias_lstm_9\"}\n",
      "\ttf.VarHandleOp() -> (tensor<!tf.resource<tensor<?x?xf32>>>) : {allowed_devices = [], container = \"\", device = \"\", shared_name = \"lstm_kernel_lstm_9\"}\n",
      "2021-09-19 21:38:39.238619: W tensorflow/compiler/mlir/lite/flatbuffer_export.cc:1838] TFLite interpreter needs to link Flex delegate in order to run the model since it contains the following flex op(s):\n",
      "Flex ops: FlexAssignVariableOp, FlexReadVariableOp, FlexVarHandleOp\n",
      "Details:\n",
      "\ttf.AssignVariableOp(tensor<!tf.resource<tensor<2048xf32>>>, tensor<2048xf32>) -> ()\n",
      "\ttf.AssignVariableOp(tensor<!tf.resource<tensor<2048xf32>>>, tensor<2048xf32>) -> () : {device = \"\"}\n",
      "\ttf.AssignVariableOp(tensor<!tf.resource<tensor<?x?xf32>>>, tensor<1x1xf32>) -> ()\n",
      "\ttf.AssignVariableOp(tensor<!tf.resource<tensor<?x?xf32>>>, tensor<3152x2048xf32>) -> () : {device = \"\"}\n",
      "\ttf.ReadVariableOp(tensor<!tf.resource<tensor<2048xf32>>>) -> (tensor<2048xf32>) : {device = \"\"}\n",
      "\ttf.ReadVariableOp(tensor<!tf.resource<tensor<?x?xf32>>>) -> (tensor<?x?xf32>) : {device = \"\"}\n",
      "\ttf.VarHandleOp() -> (tensor<!tf.resource<tensor<2048xf32>>>) : {allowed_devices = [], container = \"\", device = \"\", shared_name = \"lstm_bias_lstm_9\"}\n",
      "\ttf.VarHandleOp() -> (tensor<!tf.resource<tensor<?x?xf32>>>) : {allowed_devices = [], container = \"\", device = \"\", shared_name = \"lstm_kernel_lstm_9\"}\n",
      "2021-09-19 21:38:39.309520: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor onnx_tf_prefix_MatMul_642 because it has no allocated buffer.\n",
      "2021-09-19 21:38:39.309549: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor onnx_tf_prefix_MatMul_663 because it has no allocated buffer.\n",
      "2021-09-19 21:38:39.323833: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor arg7 because it has no allocated buffer.\n",
      "2021-09-19 21:38:39.323857: I tensorflow/lite/tools/optimize/quantize_weights.cc:225] Skipping quantization of tensor LSTM_99b581af/bidirectional_rnn/fw/fw/while/fw/multi_rnn_cell/cell_0/lstm_cell/MatMul because it has fewer than 1024 elements (1).\n",
      "2021-09-19 21:38:39.323863: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor arg7 because it has no allocated buffer.\n",
      "2021-09-19 21:38:39.323867: I tensorflow/lite/tools/optimize/quantize_weights.cc:225] Skipping quantization of tensor LSTM_99b581af/bidirectional_rnn/bw/bw/while/bw/multi_rnn_cell/cell_0/lstm_cell/MatMul because it has fewer than 1024 elements (1).\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(\"./tf_model\")\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,\n",
    "                                       tf.lite.OpsSet.SELECT_TF_OPS]\n",
    "converter.allow_custom_ops=False\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.experimental_enable_resource_variables = True\n",
    "\n",
    "converter.experimental_new_converter =True\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the model\n",
    "with open(\"bilstm.tflite\", 'wb') as f:\n",
    "    f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3efc8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter = tf.lite.Interpreter(model_path=\"bilstm.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "    \n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "input_shape = input_details[0]['shape']\n",
    "output_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f1f546",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv = pd.read_csv(\"data/training/speaker_1/20210824_1/61.csv\")\n",
    "columns = [x for x in list(csv.columns) if \"Eye\" not in x]\n",
    "columns.remove(\"Timecode\")\n",
    "columns.remove(\"BlendShapeCount\")\n",
    "csv[columns].var().sort_values()\n",
    "#df = preprocess_viseme(\"data/training/speaker_1/20210824_1/61.csv\", pad_len_in_secs=pad_len_in_secs, \n",
    "#                                   resample_to=target_framerate, blendshapes=[\"MouthClose\",\"MouthFunnel\"])\n",
    "#df.shape\n",
    "#[df.iloc[0][\"EyeLookInLeft\"]]\n",
    "    #csv[columns] = pd.np.digitize(csv[columns], np.linspace(0,1,11))\n",
    "    \n",
    "    #split = csv[\"Timecode\"].str.split(':')\n",
    "    #minute = split.str[1].astype(int)\n",
    "    #second = split.str[2].astype(int)\n",
    "    #frame = split.str[3].astype(float)\n",
    "    #minute -= minute[0]\n",
    "    #ms\n",
    "    #step = minute * 60 + second\n",
    "    #csv[\"step\"] = step\n",
    "    #return csv.drop_duplicates([\"step\"])[[\"step\", \"MouthClose\",\"MouthFunnel\",\"MouthPucker\",\"JawOpen\"]]\n",
    "    \n",
    "# if we want to use softmax across each blendshape as a one-hot\n",
    "    #return np.reshape(vals, (vals.shape[0], vals.shape[1], 1))\n",
    "    #one_hot = np.zeros((vals.shape[0], vals.shape[1], 11, 1))\n",
    "    #oh = np.eye(11)\n",
    "    #for row in range(vals.shape[0]):\n",
    "    #    for t in range(vals.shape[1]):\n",
    "    #        one_hot[row, :, :, 0] = np.eye(11)[int(vals[row,t])-1]\n",
    "    #return one_hot"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
