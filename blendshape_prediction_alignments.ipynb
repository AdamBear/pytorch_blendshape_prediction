{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fbbd334",
   "metadata": {},
   "source": [
    "RNN model to transform a sequence of phones:\n",
    "\n",
    "sil sil sil h h h eh eh eh eh eh eh l l l oh oh oh oh oh oh\n",
    "\n",
    "to a corresponding sequence of blendshape weights:\n",
    "\n",
    "[ [0.2,0.0,0.9,0.1], [0.03,0.0,0.0,0.6], ... ]\n",
    "\n",
    "The phone sequence (alignments) can be generated by forced alignment (for training data) or a TTS model (for inference data).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d29d14c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hydroxide/.local/lib/python3.7/site-packages/tensorflow_addons/utils/ensure_tf_install.py:67: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.3.0 and strictly below 2.6.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.6.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  UserWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "from src.dataset import VisemeAlignmentDataset\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from collections import OrderedDict\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import scipy\n",
    "import functools\n",
    "from torch import nn\n",
    "import math\n",
    "import math\n",
    "import yaml\n",
    "from tensorflow_tts.inference import AutoConfig\n",
    "import json\n",
    "\n",
    "import torch.onnx \n",
    "import onnx\n",
    "from onnx_tf.backend import prepare\n",
    "import onnxruntime\n",
    "\n",
    "from src.phones import load_symbols, combine_related_phones\n",
    "from src.config import VisemeModelConfiguration\n",
    "\n",
    "torch.__version__\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print('Using {} device'.format(device))\n",
    "\n",
    "config = VisemeModelConfiguration(batch_size=20)\n",
    "config.save(\"output/viseme_model.json\")\n",
    "config.model_config\n",
    "\n",
    "symbol_ids = load_symbols(\"/home/hydroxide/projects/polyvox/polyvox_framework/assets/symbol_ids.txt\")\n",
    "symbol_ids_copy, pad_sym = combine_related_phones(symbol_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4560f2f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[56, 56, 56,  ..., 60, 60, 60],\n",
       "        [56, 56, 56,  ..., 60, 60, 60],\n",
       "        [56, 56, 56,  ..., 60, 60, 60],\n",
       "        ...,\n",
       "        [56, 56, 56,  ..., 60, 60, 60],\n",
       "        [56, 56, 56,  ..., 60, 60, 60],\n",
       "        [56, 56, 56,  ..., 60, 60, 60]], dtype=torch.int32)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.viseme import preprocess_viseme \n",
    "from src.alignments import preprocess_alignments\n",
    "\n",
    "from decimal import Decimal\n",
    "import math\n",
    "\n",
    "training_data = VisemeAlignmentDataset(\n",
    "    \"./data/training/\", \n",
    "    functools.partial(\n",
    "        preprocess_viseme, \n",
    "        blendshapes=config.sourceKeys, \n",
    "    ), \n",
    "    functools.partial(\n",
    "        preprocess_alignments, \n",
    "        phone_ids=symbol_ids_copy,\n",
    "        framerate=30\n",
    "    ),\n",
    "    pad_value=len(symbol_ids)+1\n",
    ")\n",
    "test_data = VisemeAlignmentDataset(\n",
    "      \"./data/test/\", \n",
    "        functools.partial(\n",
    "            preprocess_viseme, \n",
    "            blendshapes=config.sourceKeys, \n",
    "        ), \n",
    "        functools.partial(\n",
    "            preprocess_alignments, \n",
    "            phone_ids=symbol_ids_copy,\n",
    "            framerate=30\n",
    "        ),\n",
    "        pad_value=pad_sym\n",
    ")\n",
    "\n",
    "collate_fn=functools.partial(training_data.collate, pad_val=pad_sym)\n",
    "train_dataloader = DataLoader(training_data, batch_size=config.batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "test_dataloader = DataLoader(test_data, batch_size=config.batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "batch = iter(train_dataloader)\n",
    "\n",
    "xs, ys, xlens, ylens, _ = next(batch, (None,None,None,None))\n",
    "xs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98c0faf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiRNNModel(nn.Module):\n",
    "    def __init__(self, phone_edim=128, phone_map=None, hdim=512, num_visemes=4):\n",
    "        super(BiRNNModel, self).__init__()\n",
    "        \n",
    "        self.phone_embedding = nn.Embedding(len(phone_map),phone_edim)\n",
    "        \n",
    "        self.rnn = torch.nn.LSTM(phone_edim, hdim, 1, bidirectional=True, batch_first=True)\n",
    "                \n",
    "        self.proj_out = torch.nn.Sequential(\n",
    "                torch.nn.Linear(hdim*2, hdim*2),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.Linear(hdim*2, num_visemes)\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self, phones):\n",
    "        phone_emb = self.phone_embedding(phones)\n",
    "        out, (h,c) = self.rnn(phone_emb)\n",
    "        return self.proj_out(out)\n",
    "\n",
    "model = BiRNNModel(\n",
    "    num_visemes=len(config.model_config[\"targetNames\"]),\n",
    "    hdim=256,\n",
    "    phone_map=symbol_ids\n",
    ").to(device)\n",
    "\n",
    "\n",
    "learning_rate = 0.0001\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c102a5f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 100 Avg loss: 290.2702086830139\n",
      "Step 200 Avg loss: 3.267869622707367\n",
      "Step 300 Avg loss: 2.018344486951828\n",
      "Step 400 Avg loss: 1.3594227051734924\n",
      "Step 500 Avg loss: 0.926131187081337\n",
      "Step 600 Avg loss: 0.5963400167226791\n",
      "Step 700 Avg loss: 0.4000048930943012\n",
      "Step 800 Avg loss: 0.26594354927539826\n",
      "Step 900 Avg loss: 0.2334109976887703\n",
      "Step 1000 Avg loss: 0.1545709589868784\n",
      "Test loss 5.065923064947128\n",
      "Step 1100 Avg loss: 0.1536829497665167\n",
      "Step 1200 Avg loss: 0.12930784290656447\n",
      "Step 1300 Avg loss: 0.1308026854880154\n",
      "Step 1400 Avg loss: 0.08978305112570524\n",
      "Step 1500 Avg loss: 0.10377374183386565\n",
      "Step 1600 Avg loss: 0.10120152022689581\n",
      "Step 1700 Avg loss: 0.08646522745490075\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_453477/2632740037.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m#loss = torch.nn.functional.huber_loss(preds, ys.to(device))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    147\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_steps = 300000\n",
    "print_loss_every = 100\n",
    "eval_every = 1000\n",
    "\n",
    "accum_loss = 0\n",
    "\n",
    "for t in range(num_steps):\n",
    "    optimizer.zero_grad()\n",
    "        \n",
    "    xs, ys, xlens, ylens, _ = next(batch, (None,None,None,None,None))\n",
    "    \n",
    "    if xs is None:\n",
    "        batch = iter(train_dataloader)\n",
    "        xs, ys, xlens, ylens, _ = next(batch, (None,None,None,None, None))\n",
    "\n",
    "    preds = model(xs.to(device)) \n",
    "    # print(preds.size())\n",
    "    # print(ys.size())\n",
    "    \n",
    "    loss = torch.nn.functional.mse_loss(preds, ys.to(device))   \n",
    "    #loss = torch.nn.functional.huber_loss(preds, ys.to(device))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    accum_loss += loss.item()\n",
    "    if t > 0 and t % print_loss_every == 0:\n",
    "        print(f\"Step {t} Avg loss: {accum_loss / print_loss_every}\")\n",
    "        accum_loss = 0\n",
    "\n",
    "    if t > 0 and t % eval_every == 0:\n",
    "        accum_loss = 0\n",
    "        for xs, ys, xlens, ylens, _ in iter(test_dataloader):\n",
    "            preds = model(xs.to(device))\n",
    "            accum_loss += torch.nn.functional.mse_loss(preds, ys.to(device)).item()\n",
    "            #accum_loss += torch.nn.functional.huber_loss(preds, ys.to(device)).item()\n",
    "\n",
    "        print(f\"Test loss {accum_loss}\")\n",
    "        accum_loss = 0\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd9d7f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, f\"output/{config.model_name}.torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70887b32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hydroxide/.local/lib/python3.7/site-packages/torch/onnx/symbolic_opset9.py:2099: UserWarning: Exporting a model to ONNX with a batch_size other than 1, with a variable length with LSTM can cause an error when running the ONNX model with a different batch size. Make sure to save the model with a batch size of 1, or define the initial states (h0/c0) as inputs of the model. \n",
      "  \"or define the initial states (h0/c0) as inputs of the model. \")\n",
      "2022-02-07 16:49:08.186280: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2022-02-07 16:49:08.186372: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: AVINIUM\n",
      "2022-02-07 16:49:08.186385: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: AVINIUM\n",
      "2022-02-07 16:49:08.186624: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 470.57.2\n",
      "2022-02-07 16:49:08.186726: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 470.57.2\n",
      "2022-02-07 16:49:08.186740: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 470.57.2\n",
      "2022-02-07 16:49:08.264045: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From /home/hydroxide/.local/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py:463: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /home/hydroxide/.local/lib/python3.7/site-packages/tensorflow/python/ops/rnn.py:447: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /home/hydroxide/.local/lib/python3.7/site-packages/keras/layers/legacy_rnn/rnn_cell_impl.py:979: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hydroxide/.local/lib/python3.7/site-packages/keras/layers/legacy_rnn/rnn_cell_impl.py:901: UserWarning: `tf.nn.rnn_cell.LSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
      "  warnings.warn(\"`tf.nn.rnn_cell.LSTMCell` is deprecated and will be \"\n",
      "/home/hydroxide/.local/lib/python3.7/site-packages/keras/engine/base_layer.py:2223: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
      "  warnings.warn('`layer.add_variable` is deprecated and '\n",
      "/home/hydroxide/.local/lib/python3.7/site-packages/keras/engine/base_layer.py:1348: UserWarning: `layer.updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`layer.updates` will be removed in a future version. '\n",
      "WARNING:absl:Function `__call__` contains input name(s) input.1 with unsupported characters which will be renamed to input_1 in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-07 16:49:13.068273: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hydroxide/.local/lib/python3.7/site-packages/keras/layers/legacy_rnn/rnn_cell_impl.py:901: UserWarning: `tf.nn.rnn_cell.LSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
      "  warnings.warn(\"`tf.nn.rnn_cell.LSTMCell` is deprecated and will be \"\n",
      "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
      "/home/hydroxide/.local/lib/python3.7/site-packages/keras/engine/base_layer.py:1348: UserWarning: `layer.updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`layer.updates` will be removed in a future version. '\n",
      "/home/hydroxide/.local/lib/python3.7/site-packages/keras/engine/base_layer.py:2223: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
      "  warnings.warn('`layer.add_variable` is deprecated and '\n",
      "WARNING:absl:Found untraced functions such as gen_tensor_dict while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./output/tf_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./output/tf_model/assets\n"
     ]
    }
   ],
   "source": [
    "model = torch.load(f\"output/{config.model_name}.torch\",map_location=torch.device('cpu'))\n",
    "\n",
    "model.eval() \n",
    "\n",
    "dummy_input = torch.zeros(1, 99, requires_grad=False,dtype=torch.int)\n",
    "\n",
    "# Export the model   \n",
    "torch.onnx.export(model,         # model being run \n",
    "     dummy_input,      # model input (or a tuple for multiple inputs) \n",
    "     f\"output/{config.model_name}.onnx\",       # where to save the model  \n",
    "     export_params=True,  # store the trained parameter weights inside the model file \n",
    "     opset_version=10,    # the ONNX version to export the model to \n",
    "     do_constant_folding=True,  # whether to execute constant folding for optimization \n",
    "     #input_names = ['audio_feats', 'text_feats'],   # the model's input names \n",
    "     output_names = ['modelOutput'], # the model's output names ,\n",
    ") \n",
    "\n",
    "model_onnx = onnx.load(f'output/{config.model_name}.onnx')\n",
    "\n",
    "tf_rep = prepare(model_onnx)\n",
    "tf_rep.export_graph('./output/tf_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "953bfeb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built converter\n",
      "Converted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-07 16:49:53.980350: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:351] Ignored output_format.\n",
      "2022-02-07 16:49:53.980388: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:354] Ignored drop_control_dependency.\n",
      "2022-02-07 16:49:53.980395: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:360] Ignored change_concat_input_ranges.\n",
      "2022-02-07 16:49:53.980626: I tensorflow/cc/saved_model/reader.cc:38] Reading SavedModel from: ./output/tf_model\n",
      "2022-02-07 16:49:53.984889: I tensorflow/cc/saved_model/reader.cc:90] Reading meta graph with tags { serve }\n",
      "2022-02-07 16:49:53.984925: I tensorflow/cc/saved_model/reader.cc:132] Reading SavedModel debug info (if present) from: ./output/tf_model\n",
      "2022-02-07 16:49:53.994863: I tensorflow/cc/saved_model/loader.cc:211] Restoring SavedModel bundle.\n",
      "2022-02-07 16:49:54.024315: I tensorflow/cc/saved_model/loader.cc:195] Running initialization op on SavedModel bundle at path: ./output/tf_model\n",
      "2022-02-07 16:49:54.044832: I tensorflow/cc/saved_model/loader.cc:283] SavedModel load for tags { serve }; Status: success: OK. Took 64208 microseconds.\n",
      "2022-02-07 16:49:54.239444: W tensorflow/compiler/mlir/lite/flatbuffer_export.cc:1827] Graph contains the following resource op(s), that use(s) resource type. Currently, the resource type is not natively supported in TFLite. Please consider not using the resource type if there are issues with either TFLite converter or TFLite runtime:\n",
      "Resource ops: AssignVariableOp, ReadVariableOp, VarHandleOp\n",
      "Details:\n",
      "\ttf.AssignVariableOp(tensor<!tf.resource<tensor<1024xf32>>>, tensor<1024xf32>) -> ()\n",
      "\ttf.AssignVariableOp(tensor<!tf.resource<tensor<1024xf32>>>, tensor<1024xf32>) -> () : {device = \"\"}\n",
      "\ttf.AssignVariableOp(tensor<!tf.resource<tensor<?x?xf32>>>, tensor<1x1xf32>) -> ()\n",
      "\ttf.AssignVariableOp(tensor<!tf.resource<tensor<?x?xf32>>>, tensor<384x1024xf32>) -> () : {device = \"\"}\n",
      "\ttf.ReadVariableOp(tensor<!tf.resource<tensor<1024xf32>>>) -> (tensor<1024xf32>) : {device = \"\"}\n",
      "\ttf.ReadVariableOp(tensor<!tf.resource<tensor<?x?xf32>>>) -> (tensor<?x?xf32>) : {device = \"\"}\n",
      "\ttf.VarHandleOp() -> (tensor<!tf.resource<tensor<1024xf32>>>) : {allowed_devices = [], container = \"\", device = \"\", shared_name = \"lstm_bias_lstm_11\"}\n",
      "\ttf.VarHandleOp() -> (tensor<!tf.resource<tensor<?x?xf32>>>) : {allowed_devices = [], container = \"\", device = \"\", shared_name = \"lstm_kernel_lstm_11\"}\n",
      "2022-02-07 16:49:54.239479: W tensorflow/compiler/mlir/lite/flatbuffer_export.cc:1838] TFLite interpreter needs to link Flex delegate in order to run the model since it contains the following flex op(s):\n",
      "Flex ops: FlexAssignVariableOp, FlexReadVariableOp, FlexVarHandleOp\n",
      "Details:\n",
      "\ttf.AssignVariableOp(tensor<!tf.resource<tensor<1024xf32>>>, tensor<1024xf32>) -> ()\n",
      "\ttf.AssignVariableOp(tensor<!tf.resource<tensor<1024xf32>>>, tensor<1024xf32>) -> () : {device = \"\"}\n",
      "\ttf.AssignVariableOp(tensor<!tf.resource<tensor<?x?xf32>>>, tensor<1x1xf32>) -> ()\n",
      "\ttf.AssignVariableOp(tensor<!tf.resource<tensor<?x?xf32>>>, tensor<384x1024xf32>) -> () : {device = \"\"}\n",
      "\ttf.ReadVariableOp(tensor<!tf.resource<tensor<1024xf32>>>) -> (tensor<1024xf32>) : {device = \"\"}\n",
      "\ttf.ReadVariableOp(tensor<!tf.resource<tensor<?x?xf32>>>) -> (tensor<?x?xf32>) : {device = \"\"}\n",
      "\ttf.VarHandleOp() -> (tensor<!tf.resource<tensor<1024xf32>>>) : {allowed_devices = [], container = \"\", device = \"\", shared_name = \"lstm_bias_lstm_11\"}\n",
      "\ttf.VarHandleOp() -> (tensor<!tf.resource<tensor<?x?xf32>>>) : {allowed_devices = [], container = \"\", device = \"\", shared_name = \"lstm_kernel_lstm_11\"}\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(\"./output/tf_model\")\n",
    "print(\"Built converter\")\n",
    "\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,\n",
    "                                       tf.lite.OpsSet.SELECT_TF_OPS]\n",
    "converter.allow_custom_ops=False\n",
    "#converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.experimental_enable_resource_variables = True\n",
    "\n",
    "converter.experimental_new_converter =True\n",
    "tflite_model = converter.convert()\n",
    "print(\"Converted\")\n",
    "\n",
    "# Save the model\n",
    "outfile=f'./output/{config.model_config[\"modelPath\"]}'\n",
    "with open(outfile, 'wb') as f:\n",
    "    f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3efc8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter = tf.lite.Interpreter(model_path=outfile)\n",
    "interpreter.allocate_tensors()\n",
    "    \n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "#input_shape = input_details[0]['shape']\n",
    "#output_details\n",
    "input_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f1f546",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv = pd.read_csv(\"data/training/speaker_1/20210824_1/61.csv\")\n",
    "columns = [x for x in list(csv.columns) if \"Eye\" not in x]\n",
    "columns.remove(\"Timecode\")\n",
    "columns.remove(\"BlendShapeCount\")\n",
    "csv[columns].var().sort_values()\n",
    "#df = preprocess_viseme(\"data/training/speaker_1/20210824_1/61.csv\", pad_len_in_secs=pad_len_in_secs, \n",
    "#                                   resample_to=target_framerate, blendshapes=[\"MouthClose\",\"MouthFunnel\"])\n",
    "#df.shape\n",
    "#[df.iloc[0][\"EyeLookInLeft\"]]\n",
    "    #csv[columns] = pd.np.digitize(csv[columns], np.linspace(0,1,11))\n",
    "    \n",
    "    #split = csv[\"Timecode\"].str.split(':')\n",
    "    #minute = split.str[1].astype(int)\n",
    "    #second = split.str[2].astype(int)\n",
    "    #frame = split.str[3].astype(float)\n",
    "    #minute -= minute[0]\n",
    "    #ms\n",
    "    #step = minute * 60 + second\n",
    "    #csv[\"step\"] = step\n",
    "    #return csv.drop_duplicates([\"step\"])[[\"step\", \"MouthClose\",\"MouthFunnel\",\"MouthPucker\",\"JawOpen\"]]\n",
    "    \n",
    "# if we want to use softmax across each blendshape as a one-hot\n",
    "    #return np.reshape(vals, (vals.shape[0], vals.shape[1], 1))\n",
    "    #one_hot = np.zeros((vals.shape[0], vals.shape[1], 11, 1))\n",
    "    #oh = np.eye(11)\n",
    "    #for row in range(vals.shape[0]):\n",
    "    #    for t in range(vals.shape[1]):\n",
    "    #        one_hot[row, :, :, 0] = np.eye(11)[int(vals[row,t])-1]\n",
    "    #return one_hot"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
