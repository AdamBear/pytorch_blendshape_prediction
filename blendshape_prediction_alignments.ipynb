{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fbbd334",
   "metadata": {},
   "source": [
    "RNN model to transform a sequence of phones:\n",
    "\n",
    "sil sil sil h h h eh eh eh eh eh eh l l l oh oh oh oh oh oh\n",
    "\n",
    "to a corresponding sequence of blendshape weights:\n",
    "\n",
    "[ [0.2,0.0,0.9,0.1], [0.03,0.0,0.0,0.6], ... ]\n",
    "\n",
    "The phone sequence (alignments) can be generated by forced alignment (for training data) or a TTS model (for inference data).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d29d14c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hydroxide/.local/lib/python3.7/site-packages/tensorflow_addons/utils/ensure_tf_install.py:67: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.3.0 and strictly below 2.6.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.8.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  UserWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Training dataset length : 1682\n"
     ]
    }
   ],
   "source": [
    "from src.dataset import VisemeAlignmentDataset\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from collections import OrderedDict\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import functools\n",
    "from torch import nn\n",
    "import math\n",
    "import math\n",
    "import yaml\n",
    "\n",
    "import torch.onnx \n",
    "import onnx\n",
    "from onnx_tf.backend import prepare\n",
    "\n",
    "from src.phones import load_symbols, combine_related_phones\n",
    "from src.config import VisemeModelConfiguration\n",
    "\n",
    "torch.__version__\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print('Using {} device'.format(device))\n",
    "\n",
    "config = VisemeModelConfiguration(batch_size=6)\n",
    "config.save(\"output/viseme_model.json\")\n",
    "config.model_config\n",
    "\n",
    "# we need to load the raw phonetic symbols, then merge \n",
    "symbol_ids = load_symbols(\"/home/hydroxide/projects/polyvox/polyvox_framework/assets/symbol_ids.txt\")\n",
    "symbol_ids_copy, num_syms = combine_related_phones(symbol_ids)\n",
    "pad_id = num_syms -1\n",
    "\n",
    "with open(\"output/symbol_ids.txt\", \"w\") as outfile:\n",
    "    for symbol in symbol_ids_copy:\n",
    "        outfile.write(\"%s %d\\n\" % (symbol, symbol_ids_copy[symbol]))\n",
    "\n",
    "from src.viseme import preprocess_viseme \n",
    "from src.alignments import preprocess_alignments\n",
    "\n",
    "import math\n",
    "\n",
    "training_data = VisemeAlignmentDataset(\n",
    "    \"./data/training/\", \n",
    "    functools.partial(\n",
    "        preprocess_viseme, \n",
    "        blendshapes=config.sourceKeys, \n",
    "        framerate=30\n",
    "    ), \n",
    "    functools.partial(\n",
    "        preprocess_alignments, \n",
    "        phone_ids=symbol_ids_copy,\n",
    "        framerate=30\n",
    "    ),\n",
    "    pad_value=pad_id\n",
    ")\n",
    "test_data = VisemeAlignmentDataset(\n",
    "      \"./data/test/\", \n",
    "        functools.partial(\n",
    "            preprocess_viseme, \n",
    "            blendshapes=config.sourceKeys, \n",
    "            framerate=30\n",
    "        ), \n",
    "        functools.partial(\n",
    "            preprocess_alignments, \n",
    "            phone_ids=symbol_ids_copy,\n",
    "            framerate=30\n",
    "        ),\n",
    "        pad_value=pad_id\n",
    ")\n",
    "\n",
    "collate_fn=functools.partial(training_data.collate, pad_val=pad_id)\n",
    "train_dataloader = DataLoader(training_data, batch_size=config.batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "test_dataloader = DataLoader(test_data, batch_size=config.batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "batch = iter(train_dataloader)\n",
    "\n",
    "xs, ys, xlens, ylens, _ = next(batch, (None,None,None,None))\n",
    "xs\n",
    "\n",
    "class BiRNNModel(nn.Module):\n",
    "    def __init__(self, phone_edim=128, num_phones=None, hdim=512, num_visemes=4, bidirectional=True):\n",
    "        super(BiRNNModel, self).__init__()\n",
    "        \n",
    "        self.phone_embedding = nn.Embedding(num_phones,phone_edim)\n",
    "        \n",
    "        self.rnn = torch.nn.LSTM(phone_edim, hdim, 1, bidirectional=bidirectional, batch_first=True)\n",
    "        proj_dim = hdim*2 if bidirectional else hdim\n",
    "        self.proj_out = torch.nn.Sequential(\n",
    "                torch.nn.Linear(proj_dim, proj_dim),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.Linear(proj_dim, num_visemes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, phones):\n",
    "        phone_emb = self.phone_embedding(phones)\n",
    "        out, _ = self.rnn(phone_emb)\n",
    "        return torch.clamp(\n",
    "            self.proj_out(out),\n",
    "            min=0,\n",
    "            max=1)\n",
    "\n",
    "model = BiRNNModel(\n",
    "    num_visemes=len(config.model_config[\"targetNames\"]),\n",
    "    hdim=256,\n",
    "    num_phones=num_syms,\n",
    "    bidirectional=True\n",
    ").to(device)\n",
    "\n",
    "\n",
    "learning_rate = 0.000001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "model\n",
    "print(f\"Training dataset length : {len(training_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3f45231",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pad_mask(lengths):\n",
    "    bs = lengths.size(0)\n",
    "    maxlen = lengths.max()\n",
    "\n",
    "    seq_range = torch.arange(0, maxlen, dtype=torch.int64).to(lengths.device)\n",
    "    \n",
    "    seq_range_expand = seq_range.unsqueeze(0).repeat(bs, 1)\n",
    "    seq_length_expand = lengths.unsqueeze(1).repeat(1, maxlen)\n",
    "\n",
    "    mask = seq_range_expand >= seq_length_expand\n",
    "    \n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c102a5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter()\n",
    "\n",
    "epoch = 0\n",
    "num_steps = 300000\n",
    "log_train_steps = 25\n",
    "\n",
    "accum_loss = 0\n",
    "mse_loss = nn.MSELoss(reduction='none')\n",
    "\n",
    "for t in range(num_steps):\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "        \n",
    "    xs, ys, xlens, ylens, _ = next(batch, (None,None,None,None,None))\n",
    "\n",
    "    if xs is None:\n",
    "        accum_loss = 0\n",
    "        for xs, ys, xlens, ylens, _ in iter(test_dataloader):            \n",
    "            preds = model(xs.to(device))\n",
    "            loss = mse_loss(preds, ys.to(device))\n",
    "            mask = make_pad_mask(torch.tensor(xlens))\n",
    "            loss[mask] = 0\n",
    "            accum_loss += loss.sum()\n",
    "        writer.add_scalar('Loss/test', accum_loss, epoch)\n",
    "        epoch += 1\n",
    "        \n",
    "        accum_loss = 0\n",
    "        batch = iter(train_dataloader)\n",
    "        xs, ys, xlens, ylens, _ = next(batch, (None,None,None,None, None))\n",
    "\n",
    "    preds = model(xs.to(device)) \n",
    "    \n",
    "    loss = mse_loss(preds, ys.to(device))\n",
    "    \n",
    "    mask = make_pad_mask(torch.tensor(xlens))\n",
    "\n",
    "    loss[mask] = 0\n",
    "    loss = loss.sum()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    accum_loss += loss.item()\n",
    "    if t > 0 and t % log_train_steps == 0:\n",
    "        writer.add_scalar('Loss/train', accum_loss / log_train_steps, t)\n",
    "        accum_loss = 0\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399bf36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_alignments(\"/home/hydroxide/projects/polyvox/viseme_prediction/data/training/NickF4/MySlate_45_Nic1.2_3.ctm\", phone_ids=symbol_ids_copy,framerate=14)\n",
    "preprocess_viseme(\"/home/hydroxide/projects/polyvox/viseme_prediction/data/training/NickF4/MySlate_45_Nic1.2_3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd9d7f67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.10.2+cu102'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.save(model, f\"output/{config.model_name}.torch\")\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70887b32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hydroxide/.local/lib/python3.7/site-packages/torch/onnx/utils.py:1297: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input phones\n",
      "  \"Automatically generated names will be applied to each dynamic axes of input {}\".format(key))\n",
      "/home/hydroxide/.local/lib/python3.7/site-packages/torch/onnx/symbolic_opset9.py:2123: UserWarning: Exporting a model to ONNX with a batch_size other than 1, with a variable length with LSTM can cause an error when running the ONNX model with a different batch size. Make sure to save the model with a batch size of 1, or define the initial states (h0/c0) as inputs of the model. \n",
      "  \"or define the initial states (h0/c0) as inputs of the model. \")\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n"
     ]
    }
   ],
   "source": [
    "model = torch.load(f\"output/{config.model_name}.torch\",map_location=torch.device('cpu'))\n",
    "\n",
    "model.eval() \n",
    "\n",
    "# Export the model   \n",
    "torch.onnx.export(model,         # model being run \n",
    "     torch.zeros(1, 181,dtype=torch.int),      # model input (or a tuple for multiple inputs) \n",
    "     f\"output/{config.model_name}.onnx\",       # where to save the model  \n",
    "     export_params=True,  # store the trained parameter weights inside the model file \n",
    "     opset_version=10,    # the ONNX version to export the model to \n",
    "     do_constant_folding=True,  # whether to execute constant folding for optimization \n",
    "     input_names = ['phones'],   # the model's input names \n",
    "     dynamic_axes={\"phones\":[1]},\n",
    "     output_names = ['modelOutput'], # the model's output names ,\n",
    ") \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a4f0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "tensorflow.__version__\n",
    "model_onnx = onnx.load(f'output/{config.model_name}.onnx')\n",
    "\n",
    "tf_rep = prepare(model_onnx)\n",
    "tf_rep.export_graph('./output/tf_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf97209",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_onnx.graph.input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953bfeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(\"./output/tf_model\")\n",
    "print(\"Built converter\")\n",
    "\n",
    "converter.target_spec.supported_ops = [\n",
    "  tf.lite.OpsSet.TFLITE_BUILTINS,\n",
    "#  tf.lite.OpsSet.SELECT_TF_OPS\n",
    "]\n",
    "#converter._experimental_lower_tensor_list_ops = False\n",
    "\n",
    "converter.allow_custom_ops=False\n",
    "#converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "#converter.experimental_enable_resource_variables = True\n",
    "#converter.experimental_new_quantizer = False\n",
    "\n",
    "#converter.experimental_new_converter =True\n",
    "tflite_model = converter.convert()\n",
    "print(\"Converted\")\n",
    "\n",
    "# Save the model\n",
    "#outfile=f'./output/{config.model_config[\"modelPath\"]}'\n",
    "outfile=f'./output/bilstm.tflite'\n",
    "with open(outfile, 'wb') as f:\n",
    "    f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3efc8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter = tf.lite.Interpreter(model_path=outfile)\n",
    "    \n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "interpreter.resize_tensor_input(input_details[0][\"index\"],[1,181])\n",
    "\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "xs = tf.zeros([1,181], tf.int32)\n",
    "\n",
    "interpreter.set_tensor(input_details[0]['index'], xs)\n",
    "interpreter.invoke()\n",
    "\n",
    "input_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87be302b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f1f546",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv = pd.read_csv(\"data/training/speaker_1/20210824_1/61.csv\")\n",
    "columns = [x for x in list(csv.columns) if \"Eye\" not in x]\n",
    "columns.remove(\"Timecode\")\n",
    "columns.remove(\"BlendShapeCount\")\n",
    "csv[columns].var().sort_values()\n",
    "#df = preprocess_viseme(\"data/training/speaker_1/20210824_1/61.csv\", pad_len_in_secs=pad_len_in_secs, \n",
    "#                                   resample_to=target_framerate, blendshapes=[\"MouthClose\",\"MouthFunnel\"])\n",
    "#df.shape\n",
    "#[df.iloc[0][\"EyeLookInLeft\"]]\n",
    "    #csv[columns] = pd.np.digitize(csv[columns], np.linspace(0,1,11))\n",
    "    \n",
    "    #split = csv[\"Timecode\"].str.split(':')\n",
    "    #minute = split.str[1].astype(int)\n",
    "    #second = split.str[2].astype(int)\n",
    "    #frame = split.str[3].astype(float)\n",
    "    #minute -= minute[0]\n",
    "    #ms\n",
    "    #step = minute * 60 + second\n",
    "    #csv[\"step\"] = step\n",
    "    #return csv.drop_duplicates([\"step\"])[[\"step\", \"MouthClose\",\"MouthFunnel\",\"MouthPucker\",\"JawOpen\"]]\n",
    "    \n",
    "# if we want to use softmax across each blendshape as a one-hot\n",
    "    #return np.reshape(vals, (vals.shape[0], vals.shape[1], 1))\n",
    "    #one_hot = np.zeros((vals.shape[0], vals.shape[1], 11, 1))\n",
    "    #oh = np.eye(11)\n",
    "    #for row in range(vals.shape[0]):\n",
    "    #    for t in range(vals.shape[1]):\n",
    "    #        one_hot[row, :, :, 0] = np.eye(11)[int(vals[row,t])-1]\n",
    "    #return one_hot"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
