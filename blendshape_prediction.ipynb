{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fae648b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: soundfile in /home/hydroxide/.local/lib/python3.7/site-packages (0.10.3.post1)\n",
      "Requirement already satisfied: cffi>=1.0 in /home/hydroxide/.local/lib/python3.7/site-packages (from soundfile) (1.14.6)\n",
      "Requirement already satisfied: pycparser in /home/hydroxide/.local/lib/python3.7/site-packages (from cffi>=1.0->soundfile) (2.20)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in /home/hydroxide/.local/lib/python3.7/site-packages (1.3.2)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /home/hydroxide/.local/lib/python3.7/site-packages (from pandas) (1.21.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/hydroxide/.local/lib/python3.7/site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/hydroxide/.local/lib/python3.7/site-packages (from pandas) (2021.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/hydroxide/.local/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: scipy in /home/hydroxide/.local/lib/python3.7/site-packages (1.7.1)\n",
      "Requirement already satisfied: numpy<1.23.0,>=1.16.5 in /home/hydroxide/.local/lib/python3.7/site-packages (from scipy) (1.21.2)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: librosa in /home/hydroxide/.local/lib/python3.7/site-packages (0.8.1)\n",
      "Requirement already satisfied: joblib>=0.14 in /home/hydroxide/.local/lib/python3.7/site-packages (from librosa) (1.0.1)\n",
      "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /home/hydroxide/.local/lib/python3.7/site-packages (from librosa) (0.24.2)\n",
      "Requirement already satisfied: scipy>=1.0.0 in /home/hydroxide/.local/lib/python3.7/site-packages (from librosa) (1.7.1)\n",
      "Requirement already satisfied: resampy>=0.2.2 in /home/hydroxide/.local/lib/python3.7/site-packages (from librosa) (0.2.2)\n",
      "Requirement already satisfied: numba>=0.43.0 in /home/hydroxide/.local/lib/python3.7/site-packages (from librosa) (0.48.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/hydroxide/.local/lib/python3.7/site-packages (from librosa) (21.0)\n",
      "Requirement already satisfied: decorator>=3.0.0 in /home/hydroxide/.local/lib/python3.7/site-packages (from librosa) (5.0.9)\n",
      "Requirement already satisfied: pooch>=1.0 in /home/hydroxide/.local/lib/python3.7/site-packages (from librosa) (1.4.0)\n",
      "Requirement already satisfied: audioread>=2.0.0 in /home/hydroxide/.local/lib/python3.7/site-packages (from librosa) (2.1.9)\n",
      "Requirement already satisfied: soundfile>=0.10.2 in /home/hydroxide/.local/lib/python3.7/site-packages (from librosa) (0.10.3.post1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/hydroxide/.local/lib/python3.7/site-packages (from librosa) (1.21.2)\n",
      "Requirement already satisfied: setuptools in /home/hydroxide/.local/lib/python3.7/site-packages (from numba>=0.43.0->librosa) (58.0.4)\n",
      "Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /home/hydroxide/.local/lib/python3.7/site-packages (from numba>=0.43.0->librosa) (0.31.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/hydroxide/.local/lib/python3.7/site-packages (from packaging>=20.0->librosa) (2.4.7)\n",
      "Requirement already satisfied: appdirs in /home/hydroxide/.local/lib/python3.7/site-packages (from pooch>=1.0->librosa) (1.4.4)\n",
      "Requirement already satisfied: requests in /home/hydroxide/.local/lib/python3.7/site-packages (from pooch>=1.0->librosa) (2.25.1)\n",
      "Requirement already satisfied: six>=1.3 in /home/hydroxide/.local/lib/python3.7/site-packages (from resampy>=0.2.2->librosa) (1.15.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/hydroxide/.local/lib/python3.7/site-packages (from scikit-learn!=0.19.0,>=0.14.0->librosa) (2.2.0)\n",
      "Requirement already satisfied: cffi>=1.0 in /home/hydroxide/.local/lib/python3.7/site-packages (from soundfile>=0.10.2->librosa) (1.14.6)\n",
      "Requirement already satisfied: pycparser in /home/hydroxide/.local/lib/python3.7/site-packages (from cffi>=1.0->soundfile>=0.10.2->librosa) (2.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/hydroxide/.local/lib/python3.7/site-packages (from requests->pooch>=1.0->librosa) (2021.5.30)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/hydroxide/.local/lib/python3.7/site-packages (from requests->pooch>=1.0->librosa) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/hydroxide/.local/lib/python3.7/site-packages (from requests->pooch>=1.0->librosa) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/hydroxide/.local/lib/python3.7/site-packages (from requests->pooch>=1.0->librosa) (1.26.6)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pykaldi in /home/hydroxide/.local/lib/python3.7/site-packages (0.2.1)\n",
      "Requirement already satisfied: numpy in /home/hydroxide/.local/lib/python3.7/site-packages (from pykaldi) (1.21.2)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torchaudio in /home/hydroxide/.local/lib/python3.7/site-packages (0.9.0)\n",
      "Requirement already satisfied: torch==1.9.0 in /home/hydroxide/.local/lib/python3.7/site-packages (from torchaudio) (1.9.0)\n",
      "Requirement already satisfied: typing-extensions in /home/hydroxide/.local/lib/python3.7/site-packages (from torch==1.9.0->torchaudio) (3.7.4.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "################################################################################\n",
      "### WARNING, path does not exist: KALDI_ROOT=/mnt/matylda5/iveselyk/Tools/kaldi-trunk\n",
      "###          (please add 'export KALDI_ROOT=<your_path>' in your $HOME/.profile)\n",
      "###          (or run as: KALDI_ROOT=<your_path> python <your_script>.py)\n",
      "################################################################################\n",
      "\n",
      "/home/hydroxide/.local/lib/python3.7/site-packages/tensorflow_addons/utils/ensure_tf_install.py:67: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.3.0 and strictly below 2.6.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.6.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  UserWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "!pip install soundfile\n",
    "!pip install pandas\n",
    "!pip install scipy\n",
    "!pip install librosa\n",
    "!pip install pykaldi\n",
    "!pip install torchaudio\n",
    "\n",
    "from models import BiLSTMModel\n",
    "from dataset import preprocess_viseme, VisemeDataset\n",
    "from audio import load_and_pad_audio\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import scipy\n",
    "from scipy import fft\n",
    "import functools\n",
    "from torch import nn\n",
    "import librosa\n",
    "import math\n",
    "import torchaudio \n",
    "import math\n",
    "import yaml\n",
    "from tensorflow_tts.inference import AutoConfig\n",
    "import json\n",
    "\n",
    "\n",
    "from dataset import preprocess_viseme, VisemeDataset\n",
    "\n",
    "torch.__version__\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print('Using {} device'.format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79c5e19",
   "metadata": {},
   "source": [
    "\\---S1---/\\---S2---/\\---S3---/\n",
    "WWWWWWWWWWWWWWWWWWWWWWWWWWWWWW\n",
    "____________\\---V1---/\n",
    "_____________AAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\n",
    "\n",
    "             \\---S1---/\\---S2---/\\---S3---/\n",
    "             WWWWWWWWWWWWWWWWWWWWWWWWWWWWWW\n",
    "______________________\\---V2---/\n",
    "_____________AAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\n",
    "\n",
    "\n",
    "A = audio sample\n",
    "V1, V2, etc = viseme 1, viseme 2, etc (aka \"frame\", but not in the MFCC sense)\n",
    "W = a window of audio samples that will be used as input (aka \"frame\" in the MFCC sense. to avoid confusion, we refer to this as the \"window\" and the viseme as the \"frame\")\n",
    "S1, S2, S3 = STFT of a window \n",
    "num_frames == num_windows\n",
    "\n",
    "audio_bins_per_window = the number of S per W\n",
    "samples_per_bin = the length of each S (i.e. number of As)\n",
    "bin_hop_length = the number of A between S1 and S2 (in the picture above, hop_length == len(S1) == len(S2) == samples_per_bin\n",
    "\n",
    "in practice, we calculate as such:\n",
    "          /-----------V2--------------\\\n",
    "/--------------V1------------\\\n",
    "\\---S1---/\\---S2---/\\---S3---/\\---S4---/\\---S5---/\\---S6---/\n",
    "_______________AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\n",
    "\n",
    "then at V1, we take {S1,S2,S3}, at V2 we take {S4,S5,S6}, etc\n",
    "The STFT \n",
    "STFT hop length should therefore equal the length of a single viseme frame, in samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0de78e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let num_stft_frames be the number of STFT frames per audio sample \n",
    "# num_stft_frames == (audio_len / n_fft)\n",
    "# (assuming FFT window length == hop length == n_ffts)\n",
    "# coeffs will be B x num_stft_frames x num_mels\n",
    "# every \n",
    "# if we've calculated the FFT size correctly, the hop length will be half \n",
    "# so stft_frames_per_window should be odd\n",
    "\n",
    "def coeffs_to_windows(coeffs, num_frames, stft_frames_per_window, n_mels):\n",
    "    \n",
    "    output = np.zeros((num_frames, stft_frames_per_window*n_mels))\n",
    "    \n",
    "    # the number of frames to hop is just half the number of STFT frames per window\n",
    "    hop_len = int(stft_frames_per_window / 2)\n",
    "    \n",
    "    for i in range(num_frames):\n",
    "        c = coeffs[:, (i*hop_len):(i*hop_len)+stft_frames_per_window]\n",
    "        # padding\n",
    "        if(c.shape[1] < stft_frames_per_window):\n",
    "            c = np.pad(c, [(0,0), (0, stft_frames_per_window - c.shape[1])], constant_values=0)\n",
    "        output[i, :] = np.reshape(c, stft_frames_per_window*n_mels)\n",
    "    return output\n",
    "\n",
    "\n",
    "# featurize audio in exactly the same way as TensorflowTTS\n",
    "# this enables mels from the synthesis step to be reused for viseme prediction\n",
    "def kaldi_mels(filepath, \n",
    "               config=None):\n",
    "    audio = load_and_pad_audio(filepath, config[\"sampleRate\"], config[\"paddedAudioLength\"])\n",
    "    audio = torch.unsqueeze(torch.tensor(audio, dtype=torch.float32), dim=0)\n",
    "    \n",
    "    fbank_framelength = 1000 * config[\"fftSize\"] / config[\"sampleRate\"];\n",
    "    fbank_frameshift =fbank_framelength #1000 *config[\"hopSize\"] / config[\"sampleRate\"]\n",
    "    \n",
    "    output = torchaudio.compliance.kaldi.fbank(audio,\n",
    "                                  frame_length=fbank_framelength,\n",
    "                                  frame_shift =fbank_frameshift,\n",
    "                                  num_mel_bins=config[\"numMels\"],\n",
    "                                  high_freq=config[\"fmax\"], \n",
    "                                  low_freq=config[\"fmin\"], \n",
    "                                  sample_frequency=config[\"sampleRate\"],\n",
    "                                  use_energy=False,\n",
    "                                  htk_compat=False,\n",
    "                                  use_log_fbank=True,\n",
    "                                  use_power=False,\n",
    "                                  energy_floor=0.0,\n",
    "                                  window_type='povey'\n",
    "                                 )\n",
    "    num_frames = (config[\"sampleRate\"] * config[\"paddedAudioLength\"]) // config[\"frameLength\"]\n",
    "    output = output.transpose(0,1)\n",
    "    #print(f\"output shape {output.size()} and num_frames {num_frames}, stft frames perindow {config['stftFramesPerWindow']}\")\n",
    "    output = coeffs_to_windows(output, num_frames,  config[\"stftFramesPerWindow\"], config[\"numMels\"])\n",
    "\n",
    "\n",
    "    return torch.tensor(output,dtype=torch.float32).to(device), audio.size()[0], [1] * audio.size()[0]\n",
    "#kaldi_mels(\"tmp/original.wav\", model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07ffb6e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MouthClose', 'MouthFunnel', 'MouthPucker', 'JawOpen', 'EyeBlinkLeft', 'EyeBlinkRight', 'EyeSquintLeft', 'EyeSquintRight', 'BrowDownLeft', 'BrowDownRight', 'MouthUpperUpLeft', 'MouthUpperUpRight', 'MouthLowerDownLeft', 'MouthLowerDownRight', 'MouthRollUpper']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'modelPath': 'bilstm.tflite',\n",
       " 'targetNames': ['A37_Mouth_Close',\n",
       "  'A29_Mouth_Funnel',\n",
       "  'A30_Mouth_Pucker',\n",
       "  'Merged_Open_Mouth',\n",
       "  'Eye_Blink_L',\n",
       "  'Eye_Blink_R',\n",
       "  'Eye_Squint_L',\n",
       "  'Eye_Squint_R',\n",
       "  'Brow_Drop_L',\n",
       "  'Brow_Drop_R',\n",
       "  'Mouth_Snarl_Upper_L',\n",
       "  'Mouth_Snarl_Upper_R',\n",
       "  'Mouth_Snarl_Lower_L',\n",
       "  'Mouth_Snarl_Lower_R',\n",
       "  'Mouth_Top_Lip_Under'],\n",
       " 'frameRate': 19.99,\n",
       " 'sampleRate': 22050,\n",
       " 'frameLength': 1104,\n",
       " 'paddedAudioLength': 10,\n",
       " 'windowLength': 22050,\n",
       " 'stftFramesPerWindow': 33,\n",
       " 'hopSize': 668,\n",
       " 'numMels': 40,\n",
       " 'fmin': 1000,\n",
       " 'fmax': 6000,\n",
       " 'fftSize': 512}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "mappings = OrderedDict()\n",
    "mappings[\"MouthClose\"] = \"A37_Mouth_Close\"\n",
    "mappings[\"MouthFunnel\"] = \"A29_Mouth_Funnel\"\n",
    "mappings[\"MouthPucker\"] = \"A30_Mouth_Pucker\"\n",
    "#mappings[\"JawOpen\"] = \"Mouth_Open\"\n",
    "mappings[\"JawOpen\"] = \"Merged_Open_Mouth\"\n",
    "mappings[\"EyeBlinkLeft\"] = \"Eye_Blink_L\"\n",
    "mappings[\"EyeBlinkRight\"] = \"Eye_Blink_R\"\n",
    "mappings[\"EyeSquintLeft\"] = \"Eye_Squint_L\"\n",
    "mappings[\"EyeSquintRight\"] = \"Eye_Squint_R\"\n",
    "mappings[\"BrowDownLeft\"] = \"Brow_Drop_L\"\n",
    "mappings[\"BrowDownRight\"] = \"Brow_Drop_R\"\n",
    "mappings[\"MouthUpperUpLeft\"] = \"Mouth_Snarl_Upper_L\"\n",
    "mappings[\"MouthUpperUpRight\"] = \"Mouth_Snarl_Upper_R\"\n",
    "mappings[\"MouthLowerDownLeft\"] = \"Mouth_Snarl_Lower_L\"\n",
    "mappings[\"MouthLowerDownRight\"] = \"Mouth_Snarl_Lower_R\"\n",
    "mappings[\"MouthRollUpper\"] = \"Mouth_Top_Lip_Under\" # not exact\n",
    "\n",
    "#mappings[\"CheekBlow\"] = \"Cheek_Blow_L\", \"Cheek_Blow_R\"\n",
    "#Nose_Scrunch = Nose_Sneer_L + Nose_Sneer_R\n",
    "print(list(mappings.keys()))\n",
    "# config for the input blendshapes\n",
    "source_config = {\n",
    "    # source framerate for raw viseme label input\n",
    "    \"framerate\":59.97,\n",
    "    # we need to map the blendshapes from the incoming CSV from LiveLinkFace to CC3 blendshapes\n",
    "    # unfortunately not 1-to-1, but seems to work well enough\n",
    "    \"mappings\":mappings\n",
    "}\n",
    " \n",
    "# config for the viseme model\n",
    "model_config = {}\n",
    "# the filename for the trained model that we will export below\n",
    "model_config[\"modelPath\"] = \"bilstm.tflite\"\n",
    "# the blendshapes that will be predicted\n",
    "# we also need to export this so the gltf animator can match the model outputs indices to morph target indices\n",
    "model_config[\"targetNames\"] = list(mappings.values()) #[\"A37_Mouth_Close\", \"A29_Mouth_Funnel\", \"A30_Mouth_Pucker\", \"Mouth_Open\"]\n",
    "\n",
    "# actual framerate to use for viseme labels. \n",
    "# raw labels will be resampled/transformed (either averaged or simply dropped).\n",
    "model_config[\"frameRate\"] = source_config[\"framerate\"] / 3\n",
    "# the sample rate that audio will be resampled to\n",
    "model_config[\"sampleRate\"] = 22050\n",
    "\n",
    "# the duration of a viseme frame is (1 / target_framerate) seconds\n",
    "model_config[\"frameLength\"] = math.ceil(model_config[\"sampleRate\"] * (1 / model_config[\"frameRate\"]))\n",
    "# all audio will be padded to the following size\n",
    "model_config[\"paddedAudioLength\"] = 10\n",
    "# the raw input for each viseme frame will be an audio window of size X \n",
    "# the middle sample of the viseme frame is aligned with the middle sample of the audio window\n",
    "# this means, at the nominal \"anchor sample\" of the viseme frame, there will be \n",
    "# X/2 samples to the left and X/2 samples to the right\n",
    "model_config[\"windowLength\"] = int(1 * model_config[\"sampleRate\"]) \n",
    "\n",
    "# this raw audio input will then be transformed into a number of STFT frames/coefficients\n",
    "# each viseme frame will have this number of STFT frames, which will be the actual input at each timestep\n",
    "# Since audio windows overlap, we won't want to waste cycles repeatedly computing the STFT across the whole audio sequence\n",
    "# So in practice, we pre-calculate the STFT for the whole sequence, then just sub-sample the coefficients at each timestep\n",
    "# when assigning STFT frames, the hop length will then just be half this value\n",
    "model_config[\"stftFramesPerWindow\"] = 33\n",
    "\n",
    "model_config[\"hopSize\"] = int(model_config[\"windowLength\"] // model_config[\"stftFramesPerWindow\"])\n",
    "model_config[\"numMels\"] = 40\n",
    "model_config[\"fmin\"] = 1000\n",
    "model_config[\"fmax\"] = 6000\n",
    "model_config[\"fftSize\"] = 512\n",
    "\n",
    "# load the TTS config so we can match the same parameters \n",
    "# this may override some of the parameters set above\n",
    "USE_TFTTS_CONFIG=False\n",
    "\n",
    "USE_KALDI_FBANK=True\n",
    "\n",
    "\n",
    "if USE_TFTTS_CONFIG:\n",
    "    with open(\"/mnt/hdd_2tb/home/hydroxide/projects/TensorFlowTTS/preprocess/baker_preprocess.yaml\", \"r\") as f:\n",
    "        tftts_config = yaml.safe_load(f)\n",
    "        model_config[\"fftSize\"] = tftts_config[\"fft_size\"]\n",
    "        model_config[\"numMels\"] = tftts_config[\"num_mels\"]       \n",
    "        if \"window_length\" in tftts_config:\n",
    "            assert(tftts_config[\"window_length\"] == model_config[\"windowLength\"])\n",
    "        \n",
    "        model_config[\"window\"] = tftts_config[\"window\"]\n",
    "        model_config[\"fmin\"] = tftts_config[\"fmin\"]\n",
    "        model_config[\"fmax\"] = tftts_config[\"fmax\"]\n",
    "        assert(tftts_config[\"sampling_rate\"] == model_config[\"sampleRate\"])\n",
    "        \n",
    "        #config={\"num_mels\":80, \"sampling_rate\":22050,\"fmin\":80,\"fmax\":6000, \"window\":\"hann\", \"fft_size\":512, \"hop_size\":hop_size})\n",
    "\n",
    "#seq_length = math.ceil((pad_len_in_secs * resample_to) / frame_len)\n",
    "\n",
    "# save the config so we can pass\n",
    "with open(\"viseme_model.json\", \"w\",encoding=\"utf-8\") as outfile:\n",
    "    json.dump(model_config, outfile)\n",
    "    \n",
    "process_audio = None\n",
    "\n",
    "if USE_KALDI_FBANK:\n",
    "    process_audio = functools.partial(kaldi_mels, \n",
    "                                          config=model_config)  \n",
    "elif USE_TFTTS_CONFIG:\n",
    "    process_audio = functools.partial(tftts_mels, \n",
    "                                          config=model_config)\n",
    "else:\n",
    "    num_mels=39\n",
    "    process_audio = functools.partial(stft, model_config);\n",
    "        #viseme_frame_len_in_samples=viseme_frame_len_in_samples, # this refers to the size of the viseme/audio window,\n",
    "        #audio_window_in_samples=audio_window_in_samples, # TODO - update these\n",
    "        #stft_frames_per_window=stft_frames_per_window,\n",
    "        #resample_to=resample_to, \n",
    "        #pad_len_in_secs=pad_len_in_secs,\n",
    "        #n_mels=num_mels)\n",
    "        \n",
    "batch_size = 20\n",
    "    \n",
    "process_viseme = functools.partial(preprocess_viseme, \n",
    "                                   pad_len_in_secs=model_config[\"paddedAudioLength\"], \n",
    "                                   blendshapes=source_config[\"mappings\"], \n",
    "                                   target_framerate=model_config[\"frameRate\"])\n",
    "\n",
    "training_data = VisemeDataset(\"./data/training/speaker_3/\", \n",
    "                              process_audio, \\\n",
    "                              process_viseme, num_ipa_symbols=157)\n",
    "test_data = VisemeDataset(\"./data/test/speaker_3/\", \n",
    "                              process_audio, \\\n",
    "                              process_viseme,num_ipa_symbols=157)\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "input_dim = int(model_config[\"windowLength\"] / model_config[\"hopSize\"] * model_config[\"numMels\"])\n",
    "\n",
    "model_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e493a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeparableConv1d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, bias=False):\n",
    "        super(SeparableConv1d, self).__init__()\n",
    "        self.depthwise = nn.Conv1d(in_channels, in_channels, kernel_size=kernel_size,\n",
    "                               groups=in_channels, bias=bias, padding=1)\n",
    "        self.pointwise = nn.Conv1d(in_channels, out_channels,\n",
    "                               kernel_size=1, bias=bias)\n",
    "    def forward(self, x):\n",
    "        out = self.depthwise(x)\n",
    "        out = self.pointwise(out)\n",
    "        return out\n",
    "    \n",
    "class Conv1dModel(nn.Module):\n",
    "    def __init__(self, seq_length=633, input_dim=None, hidden_size=1000, num_visemes=4, ks=3):\n",
    "        super(Conv1dModel, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            SeparableConv1d(input_dim,input_dim,ks),\n",
    "            nn.ReLU(),\n",
    "            #SeparableConv2d(input_dim,input_dim,ks),\n",
    "            #nn.ReLU(),\n",
    "        )\n",
    "        #self.attention = nn.MultiheadAttention(1951, 1, batch_first=True)\n",
    "        self.linear_out = nn.Linear(1, num_visemes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        o1 = self.conv(x)\n",
    "        o1 = o1.sum(1,keepdim=True)\n",
    "        o1 = torch.transpose(o1, 1,2)\n",
    "        #print(o1.size())\n",
    "        #attn_output, attn_output_weights = self.attention(o1, o1, o1)\n",
    "        #attn_output = attn_output.tile((4,1,1,1)).transpose(0,1)\n",
    "        return self.linear_out(o1)\n",
    "\n",
    "#20x199x5040\n",
    "seq_length = 199\n",
    "#model = Conv1dModel(hidden_size=256, input_dim=input_dim, num_visemes=len(model_config[\"targetNames\"])).to(device)\n",
    "#model.forward(torch.transpose(train_features,1,2)).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae747c49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[-3.6338e+00, -4.0910e+00, -4.7058e+00,  ..., -7.0782e-01,\n",
       "           -1.8025e+00, -5.3995e-01],\n",
       "          [-3.9825e+00, -4.3312e+00, -3.8664e+00,  ..., -2.5225e+00,\n",
       "           -3.4502e+00, -3.0385e+00],\n",
       "          [-6.4948e-01, -2.0001e+00, -1.3038e+00,  ..., -1.7895e+00,\n",
       "           -2.0766e+00, -2.0787e+00],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00]],\n",
       " \n",
       "         [[-3.8316e+00, -3.9283e+00, -5.1874e+00,  ...,  1.1198e-01,\n",
       "            2.6871e-01, -3.3531e-01],\n",
       "          [-3.4838e+00, -4.0770e+00, -4.7193e+00,  ..., -1.9681e+00,\n",
       "           -1.9495e+00, -1.7991e+00],\n",
       "          [-2.9308e+00, -2.7699e+00, -2.6918e+00,  ..., -1.5642e+00,\n",
       "           -9.9894e-01, -1.0740e+00],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00]],\n",
       " \n",
       "         [[-3.7216e+00, -4.0399e+00, -4.3579e+00,  ...,  2.7153e-01,\n",
       "           -8.4361e-01, -1.3596e+00],\n",
       "          [-4.0266e+00, -4.1570e+00, -4.2686e+00,  ..., -2.0168e+00,\n",
       "            6.0215e-01, -2.7818e-01],\n",
       "          [-1.4184e+00, -1.6372e+00, -1.2835e+00,  ..., -3.4924e-02,\n",
       "           -2.7270e-01, -1.2959e+00],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[-5.3519e+00, -4.8090e+00, -4.7885e+00,  ..., -2.9321e+00,\n",
       "           -2.7950e+00, -8.8520e-01],\n",
       "          [-4.4911e+00, -4.8015e+00, -4.5716e+00,  ..., -2.0051e+00,\n",
       "           -1.1815e+00, -8.1130e-01],\n",
       "          [-2.8368e+00, -3.1011e+00, -2.5999e+00,  ..., -5.6938e-01,\n",
       "           -3.9047e-01, -2.2705e-01],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00]],\n",
       " \n",
       "         [[-3.8052e+00, -4.0476e+00, -3.6048e+00,  ..., -1.3490e+00,\n",
       "           -1.7110e+00, -1.9380e+00],\n",
       "          [-4.3253e+00, -3.8840e+00, -4.9501e+00,  ..., -1.7003e+00,\n",
       "           -5.9720e-01,  1.3022e-02],\n",
       "          [-3.3839e+00, -2.3561e+00, -2.6714e+00,  ..., -6.8462e-01,\n",
       "           -1.2734e+00, -1.4615e+00],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00]],\n",
       " \n",
       "         [[-4.5978e+00, -4.5707e+00, -4.0073e+00,  ..., -8.4903e-01,\n",
       "           -1.8047e+00, -1.7224e+00],\n",
       "          [-3.7843e+00, -4.3882e+00, -4.0830e+00,  ...,  1.6149e+00,\n",
       "           -3.8307e-03, -4.4077e-01],\n",
       "          [-4.7313e+00, -3.7622e+00, -4.0790e+00,  ...,  7.2383e-01,\n",
       "            5.7213e-01,  5.9120e-01],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00]]], device='cuda:0'),\n",
       " tensor([[  6, 157],\n",
       "         [  6, 157],\n",
       "         [  6, 157],\n",
       "         [  3, 157],\n",
       "         [  7, 157],\n",
       "         [  6, 157],\n",
       "         [  7, 157],\n",
       "         [  6, 157],\n",
       "         [  6, 157],\n",
       "         [  7, 157],\n",
       "         [  3, 157],\n",
       "         [  1, 157],\n",
       "         [  3, 157],\n",
       "         [  6, 157],\n",
       "         [  6, 157],\n",
       "         [  7, 157],\n",
       "         [  1, 157],\n",
       "         [  6, 157],\n",
       "         [  6, 157],\n",
       "         [  6, 157]]),\n",
       " tensor([[1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1]]),\n",
       " tensor([[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       " \n",
       "         [[0.1307, 0.0417, 0.0593,  ..., 0.1443, 0.1568, 0.1356],\n",
       "          [0.1310, 0.0417, 0.0594,  ..., 0.1439, 0.1564, 0.1360],\n",
       "          [0.1316, 0.0417, 0.0595,  ..., 0.1437, 0.1562, 0.1376],\n",
       "          ...,\n",
       "          [0.0825, 0.1323, 0.0628,  ..., 0.2105, 0.2207, 0.0347],\n",
       "          [0.0748, 0.1161, 0.0596,  ..., 0.2727, 0.2826, 0.0550],\n",
       "          [0.0709, 0.1044, 0.0580,  ..., 0.3487, 0.3614, 0.0889]],\n",
       " \n",
       "         [[0.1403, 0.0404, 0.0715,  ..., 0.1306, 0.1445, 0.1318],\n",
       "          [0.1414, 0.0405, 0.0721,  ..., 0.1308, 0.1444, 0.1332],\n",
       "          [0.1422, 0.0406, 0.0727,  ..., 0.1308, 0.1443, 0.1338],\n",
       "          ...,\n",
       "          [0.3588, 0.5383, 0.4878,  ..., 0.1193, 0.1204, 0.0462],\n",
       "          [0.3325, 0.5314, 0.4373,  ..., 0.1191, 0.1205, 0.0456],\n",
       "          [0.3023, 0.5379, 0.3982,  ..., 0.1193, 0.1209, 0.0449]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[0.1234, 0.0682, 0.0697,  ..., 0.1441, 0.1545, 0.0878],\n",
       "          [0.1236, 0.0684, 0.0696,  ..., 0.1441, 0.1547, 0.0880],\n",
       "          [0.1235, 0.0694, 0.0696,  ..., 0.1443, 0.1548, 0.0878],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       " \n",
       "         [[0.1249, 0.0776, 0.0957,  ..., 0.1441, 0.1561, 0.0539],\n",
       "          [0.1255, 0.0775, 0.0962,  ..., 0.1443, 0.1563, 0.0543],\n",
       "          [0.1261, 0.0776, 0.0967,  ..., 0.1446, 0.1565, 0.0547],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       " \n",
       "         [[0.1626, 0.0286, 0.0596,  ..., 0.1414, 0.1679, 0.2703],\n",
       "          [0.1626, 0.0285, 0.0596,  ..., 0.1413, 0.1678, 0.2704],\n",
       "          [0.1628, 0.0285, 0.0597,  ..., 0.1413, 0.1678, 0.2708],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]]),\n",
       " ('data/training/speaker_3/394.csv',\n",
       "  'data/training/speaker_3/517.csv',\n",
       "  'data/training/speaker_3/282.csv',\n",
       "  'data/training/speaker_3/265.csv',\n",
       "  'data/training/speaker_3/300.csv',\n",
       "  'data/training/speaker_3/362.csv',\n",
       "  'data/training/speaker_3/367.csv',\n",
       "  'data/training/speaker_3/400.csv',\n",
       "  'data/training/speaker_3/372.csv',\n",
       "  'data/training/speaker_3/295.csv',\n",
       "  'data/training/speaker_3/244.csv',\n",
       "  'data/training/speaker_3/415.csv',\n",
       "  'data/training/speaker_3/100.csv',\n",
       "  'data/training/speaker_3/146.csv',\n",
       "  'data/training/speaker_3/439.csv',\n",
       "  'data/training/speaker_3/168.csv',\n",
       "  'data/training/speaker_3/361.csv',\n",
       "  'data/training/speaker_3/11.csv',\n",
       "  'data/training/speaker_3/471.csv',\n",
       "  'data/training/speaker_3/166.csv')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = iter(train_dataloader)\n",
    "\n",
    "next(batch, (None,None,None,None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c102a5f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 Avg loss: 6.202145385742187\n",
      "Step 25 Avg loss: 0.963445328772068\n",
      "Step 50 Avg loss: 0.09911985695362091\n",
      "Test loss 0.16701239347457886\n",
      "Step 75 Avg loss: 0.08647215127944946\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_208359/233358432.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Step {t} Avg loss: {accum_loss / print_loss_every}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0maccum_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    147\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "#model = Conv2dModel(hidden_size=256, input_dim=input_dim, num_visemes=len(model_config[\"targetNames\"])).to(device)\n",
    "model = Conv1dModel(hidden_size=256, input_dim=input_dim, num_visemes=len(model_config[\"targetNames\"])).to(device)\n",
    "\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "num_steps = 100000\n",
    "print_loss_every = 25\n",
    "eval_every = 50\n",
    "\n",
    "batch = iter(train_dataloader)\n",
    "accum_loss = 0\n",
    "\n",
    "for t in range(num_steps):\n",
    "    optimizer.zero_grad()\n",
    "    audio_features, text_features, train_mask, train_labels, train_files = next(batch, (None,None,None,None,None))\n",
    "    #print(train_features.size())\n",
    "    if audio_features is None:\n",
    "        batch = iter(train_dataloader)\n",
    "        audio_features, text_features, train_mask, train_labels, _ = next(batch)\n",
    "    \n",
    "    #train_mask = torch.unsqueeze(train_mask, 2)\n",
    "    \n",
    "    x = audio_features.to(device)     \n",
    "    x = torch.transpose(x,1,2)\n",
    "    \n",
    "    y = train_labels.to(device) #* train_mask.to(device)\n",
    "    #print(y.shape)\n",
    "       \n",
    "    preds = model(x) #* train_mask.to(device)\n",
    "#    print(preds.shape)\n",
    "       \n",
    "    #preds = torch.transpose(preds, 1,3).squeeze()\n",
    "    #print(f\"x {x.shape} preds {preds.shape} y {y.shape}\")    \n",
    "    #loss = torch.nn.functional.cross_entropy(preds, y)\n",
    "    loss = torch.nn.functional.huber_loss(preds, y)\n",
    "    if len(preds.shape) < 3:\n",
    "        preds = torch.unsqueeze(preds, 0)\n",
    "    #print(preds.shape)\n",
    "    for i in range(preds.shape[1] - 1):\n",
    "        loss += torch.nn.functional.cosine_embedding_loss(preds[:,i,:], preds[:,i+1,:], (torch.ones(preds.shape[0])).to(device))\n",
    "    accum_loss += loss.item()\n",
    "    if t % print_loss_every == 0:\n",
    "        print(f\"Step {t} Avg loss: {accum_loss / print_loss_every}\")\n",
    "        accum_loss = 0\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    \n",
    "    \n",
    "    if t > 0 and t % eval_every == 0:\n",
    "        accum_loss = 0\n",
    "        for test_features, test_text_features, test_mask, test_labels, _ in iter(test_dataloader):\n",
    "            x = test_features.to(device)\n",
    "            x = torch.transpose(x, 1, 2)\n",
    "            #x = torch.unsqueeze(x, dim=3)\n",
    "            y = test_labels.to(device)\n",
    "            preds = model(x)\n",
    "            #preds = torch.transpose(preds, 1,3)\n",
    "            accum_loss += torch.nn.functional.mse_loss(preds, y).item()\n",
    "            #accum_loss = torch.nn.functional.cross_entropy(preds, y)\n",
    "\n",
    "        print(f\"Test loss {accum_loss}\")\n",
    "        accum_loss = 0\n",
    "    \n",
    "#pred_probab = nn.Softmax(dim=1)(logits)\n",
    "#y_pred = pred_probab.argmax(1)\n",
    "#print(f\"Predicted class: {y_pred}\")199"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870a3d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41510411",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_features, test_mask, test_labels, test_files = next(iter(test_dataloader))\n",
    "#test_features, test_mask, test_labels, test_files = next(iter(train_dataloader))\n",
    "##test_features = torch.transpose(test_features, 1, 2)\n",
    "test_features = train_features\n",
    "export_y_batch = model(test_features.to(device)) \n",
    "export_y = export_y_batch[0,:,:]\n",
    "\n",
    "header = \"Timecode,BlendShapeCount,eyeBlinkRight,eyeLookDownRight,eyeLookInRight,eyeLookOutRight,eyeLookUpRight,eyeSquintRight,eyeWideRight,eyeBlinkLeft,eyeLookDownLeft,eyeLookInLeft,eyeLookOutLeft,eyeLookUpLeft,eyeSquintLeft,eyeWideLeft,jawForward,jawRight,jawLeft,jawOpen,mouthClose,mouthFunnel,mouthPucker,mouthRight,mouthLeft,mouthSmileRight,mouthSmileLeft,mouthFrownRight,mouthFrownLeft,mouthDimpleRight,mouthDimpleLeft,mouthStretchRight,mouthStretchLeft,mouthRollLower,mouthRollUpper,mouthShrugLower,mouthShrugUpper,mouthPressRight,mouthPressLeft,mouthLowerDownRight,mouthLowerDownLeft,mouthUpperUpRight,mouthUpperUpLeft,browDownRight,browDownLeft,browInnerUp,browOuterUpRight,browOuterUpLeft,cheekPuff,cheekSquintRight,cheekSquintLeft,noseSneerRight,noseSneerLeft,tongueOut,HeadYaw,HeadPitch,HeadRoll,LeftEyeYaw,LeftEyePitch,LeftEyeRoll,RightEyeYaw,RightEyePitch,RightEyeRoll\".split(',')\n",
    "selected_output_indices = [header.index(x[0].lower() + x[1:]) for x in source_config[\"mappings\"].keys()]\n",
    "num_visemes = len(source_config[\"mappings\"])\n",
    "with open(\"output.csv\", \"w\") as outfile:\n",
    "    outfile.write(\",\".join(header) + \"\\n\")\n",
    "    timer_ms = 0\n",
    "    print(export_y)\n",
    "    for t in range(export_y.shape[1]):\n",
    "        output = [str(0)] * len(header)\n",
    "        second = str(int(timer_ms // 1000)).zfill(2)\n",
    "        frame = (timer_ms % 1000) * model_config[\"frameRate\"] / 1000\n",
    "        output[0] = f\"00:00:{second}:{frame}\"\n",
    "        for viseme in range(num_visemes): \n",
    "            output[selected_output_indices[viseme]] = str(export_y[viseme,t,:].item())\n",
    "        timer_ms += (1 / model_config[\"frameRate\"]) * 1000\n",
    "        outfile.write(\",\".join(output) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16652a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_header = \"Timecode,BlendShapeCount,EyeBlinkLeft,EyeLookDownLeft,EyeLookInLeft,EyeLookOutLeft,EyeLookUpLeft,EyeSquintLeft,EyeWideLeft,EyeBlinkRight,EyeLookDownRight,EyeLookInRight,EyeLookOutRight,EyeLookUpRight,EyeSquintRight,EyeWideRight,JawForward,JawRight,JawLeft,JawOpen,MouthClose,MouthFunnel,MouthPucker,MouthRight,MouthLeft,MouthSmileLeft,MouthSmileRight,MouthFrownLeft,MouthFrownRight,MouthDimpleLeft,MouthDimpleRight,MouthStretchLeft,MouthStretchRight,MouthRollLower,MouthRollUpper,MouthShrugLower,MouthShrugUpper,MouthPressLeft,MouthPressRight,MouthLowerDownLeft,MouthLowerDownRight,MouthUpperUpLeft,MouthUpperUpRight,BrowDownLeft,BrowDownRight,BrowInnerUp,BrowOuterUpLeft,BrowOuterUpRight,CheekPuff,CheekSquintLeft,CheekSquintRight,NoseSneerLeft,NoseSneerRight,TongueOut,HeadYaw,HeadPitch,HeadRoll,LeftEyeYaw,LeftEyePitch,LeftEyeRoll,RightEyeYaw,RightEyePitch,RightEyeRoll\"\n",
    "remap = {h:(h[0].lower() + h[1:]) if h not in [\"Timecode\",\"BlendShapeCount\",\"HeadYaw\",\"HeadPitch\",\"HeadRoll\",\"LeftEyeYaw\",\"LeftEyePitch\",\"LeftEyeRoll\",\"RightEyeYaw\",\"RightEyePitch\",\"RightEyeRoll\"]  else h for h in new_header.split(\",\") }\n",
    "for oh in remap.values():\n",
    "    if oh not in header:\n",
    "        print(oh)\n",
    "\n",
    "def new_to_old(csv_df):\n",
    "    return csv_df.rename(columns=remap)\n",
    "df = preprocess_viseme(\"data/training/speaker_1/20210824_1/61.csv\", pad_len_in_secs=model_config[\"paddedAudioLength\"], \n",
    "                                   target_framerate=model_config[\"frameRate\"])\n",
    "df = new_to_old(df)\n",
    "#cols = [c for c in df.columns if c not in [\"jawOpen\", \"mouthClose\", \"Timecode\"]]\n",
    "#df[cols] = 0\n",
    "df = df[df[\"Timecode\"] != 0]\n",
    "df.to_csv(\"original.csv\", index=False)\n",
    "#new_to_old(df)[header].to_csv(\"original.csv\", index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666b8680",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = preprocess_viseme(\"data/training/speaker_1/20210824_1/61.csv\", pad_len_in_secs=model_config[\"paddedAudioLength\"], \n",
    "                                   target_framerate=\n",
    "                       model_config[\"frameRate\"])\n",
    "print(list(source_config[\"mappings\"].keys()))\n",
    "print(list(source_config[\"mappings\"].values()))\n",
    "df = df[list(source_config[\"mappings\"].keys())]\n",
    "df.columns = list(source_config[\"mappings\"].values())\n",
    "df = df.iloc[::int(59.97 / 29.98)]\n",
    "\n",
    "\n",
    "df.to_csv(\"original.csv\", index=False)\n",
    "#model_config\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05231fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[::int(59.97 / 29.98)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00801822",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"output/conv2d.torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70887b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(\"output/conv2d.torch\",map_location=torch.device('cpu'))\n",
    "import torch.onnx \n",
    "\n",
    "# set the model to inference mode \n",
    "model.eval() \n",
    "\n",
    "# Let's create a dummy input tensor  \n",
    "dummy_input = torch.randn(1, input_dim, 119,  requires_grad=False)  \n",
    "\n",
    "# Export the model   \n",
    "torch.onnx.export(model,         # model being run \n",
    "     dummy_input,       # model input (or a tuple for multiple inputs) \n",
    "     \"output/conv2d.onnx\",       # where to save the model  \n",
    "     export_params=True,  # store the trained parameter weights inside the model file \n",
    "     opset_version=10,    # the ONNX version to export the model to \n",
    "     do_constant_folding=True,  # whether to execute constant folding for optimization \n",
    "     #input_names = ['modelInput'],   # the model's input names \n",
    "     #output_names = ['modelOutput'], # the model's output names \n",
    "     dynamic_axes={'modelInput' : {0 : 'batch_size'},    # variable length axes \n",
    "    'modelOutput' : {0 : 'batch_size'}}\n",
    "                 ) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47648b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "\n",
    "import onnxruntime\n",
    "model_onnx = onnx.load('output/conv2d.onnx')\n",
    "\n",
    "\n",
    "from onnx_tf.backend import prepare\n",
    "tf_rep = prepare(model_onnx)\n",
    "tf_rep.export_graph('./output/tf_model')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df4666c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_model_optimization as tfmot\n",
    "\n",
    "quant_aware_model = tfmot.quantization.keras.quantize_model(tf_rep)\n",
    "quant_aware_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953bfeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(\"./output/tf_model\")\n",
    "print(\"Built converter\")\n",
    "\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,\n",
    "                                       tf.lite.OpsSet.SELECT_TF_OPS]\n",
    "converter.allow_custom_ops=False\n",
    "#converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "#converter.experimental_enable_resource_variables = True\n",
    "\n",
    "converter.experimental_new_converter =True\n",
    "tflite_model = converter.convert()\n",
    "print(\"Converted\")\n",
    "\n",
    "# Save the model\n",
    "with open(\"./output/conv2d.tflite\", 'wb') as f:\n",
    "    f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3efc8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter = tf.lite.Interpreter(model_path=\"bilstm.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "    \n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "input_shape = input_details[0]['shape']\n",
    "output_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f1f546",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv = pd.read_csv(\"data/training/speaker_1/20210824_1/61.csv\")\n",
    "columns = [x for x in list(csv.columns) if \"Eye\" not in x]\n",
    "columns.remove(\"Timecode\")\n",
    "columns.remove(\"BlendShapeCount\")\n",
    "csv[columns].var().sort_values()\n",
    "#df = preprocess_viseme(\"data/training/speaker_1/20210824_1/61.csv\", pad_len_in_secs=pad_len_in_secs, \n",
    "#                                   resample_to=target_framerate, blendshapes=[\"MouthClose\",\"MouthFunnel\"])\n",
    "#df.shape\n",
    "#[df.iloc[0][\"EyeLookInLeft\"]]\n",
    "    #csv[columns] = pd.np.digitize(csv[columns], np.linspace(0,1,11))\n",
    "    \n",
    "    #split = csv[\"Timecode\"].str.split(':')\n",
    "    #minute = split.str[1].astype(int)\n",
    "    #second = split.str[2].astype(int)\n",
    "    #frame = split.str[3].astype(float)\n",
    "    #minute -= minute[0]\n",
    "    #ms\n",
    "    #step = minute * 60 + second\n",
    "    #csv[\"step\"] = step\n",
    "    #return csv.drop_duplicates([\"step\"])[[\"step\", \"MouthClose\",\"MouthFunnel\",\"MouthPucker\",\"JawOpen\"]]\n",
    "    \n",
    "# if we want to use softmax across each blendshape as a one-hot\n",
    "    #return np.reshape(vals, (vals.shape[0], vals.shape[1], 1))\n",
    "    #one_hot = np.zeros((vals.shape[0], vals.shape[1], 11, 1))\n",
    "    #oh = np.eye(11)\n",
    "    #for row in range(vals.shape[0]):\n",
    "    #    for t in range(vals.shape[1]):\n",
    "    #        one_hot[row, :, :, 0] = np.eye(11)[int(vals[row,t])-1]\n",
    "    #return one_hot"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
