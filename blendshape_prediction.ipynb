{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fbbd334",
   "metadata": {},
   "source": [
    "\\---S1---/\\---S2---/\\---S3---/\n",
    "WWWWWWWWWWWWWWWWWWWWWWWWWWWWWW\n",
    "____________\\---V1---/\n",
    "_____________AAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\n",
    "\n",
    "             \\---S1---/\\---S2---/\\---S3---/\n",
    "             WWWWWWWWWWWWWWWWWWWWWWWWWWWWWW\n",
    "______________________\\---V2---/\n",
    "_____________AAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\n",
    "\n",
    "\n",
    "A = audio sample\n",
    "V1, V2, etc = viseme 1, viseme 2, etc (aka \"frame\", but not in the MFCC sense)\n",
    "W = a window of audio samples that will be used as input (aka \"frame\" in the MFCC sense. to avoid confusion, we refer to this as the \"window\" and the viseme as the \"frame\")\n",
    "S1, S2, S3 = STFT of a window \n",
    "num_frames == num_windows\n",
    "\n",
    "audio_bins_per_window = the number of S per W\n",
    "samples_per_bin = the length of each S (i.e. number of As)\n",
    "bin_hop_length = the number of A between S1 and S2 (in the picture above, hop_length == len(S1) == len(S2) == samples_per_bin\n",
    "\n",
    "in practice, we calculate as such:\n",
    "          /-----------V2--------------\\\n",
    "/--------------V1------------\\\n",
    "\\---S1---/\\---S2---/\\---S3---/\\---S4---/\\---S5---/\\---S6---/\n",
    "_______________AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\n",
    "\n",
    "then at V1, we take {S1,S2,S3}, at V2 we take {S4,S5,S6}, etc\n",
    "The STFT \n",
    "TFT hop length should therefore equal the length of a single viseme frame, in samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fae648b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "################################################################################\n",
      "### WARNING, path does not exist: KALDI_ROOT=/mnt/matylda5/iveselyk/Tools/kaldi-trunk\n",
      "###          (please add 'export KALDI_ROOT=<your_path>' in your $HOME/.profile)\n",
      "###          (or run as: KALDI_ROOT=<your_path> python <your_script>.py)\n",
      "################################################################################\n",
      "\n",
      "/home/hydroxide/.local/lib/python3.7/site-packages/tensorflow_addons/utils/ensure_tf_install.py:67: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.3.0 and strictly below 2.6.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.6.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  UserWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'modelPath': 'bilstm.tflite',\n",
       " 'targetNames': ['A37_Mouth_Close',\n",
       "  'A29_Mouth_Funnel',\n",
       "  'A30_Mouth_Pucker',\n",
       "  'Merged_Open_Mouth'],\n",
       " 'frameRate': 4.000667111407605,\n",
       " 'sampleRate': 22050,\n",
       " 'paddedAudioLength': 5,\n",
       " 'frameLength': 5511.580790395198,\n",
       " 'seqLength': 20,\n",
       " 'windowLength': 44100,\n",
       " 'stftFramesPerWindow': 52,\n",
       " 'hopSize': 848,\n",
       " 'numMels': 10,\n",
       " 'fmin': 100,\n",
       " 'fmax': 8000,\n",
       " 'fftSize': 512}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from kaldi_mels import kaldi_mels\n",
    "from models import BiLSTMModel\n",
    "from dataset import preprocess_viseme, VisemeDataset\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from collections import OrderedDict\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import scipy\n",
    "from scipy import fft\n",
    "import functools\n",
    "from torch import nn\n",
    "import librosa\n",
    "import math\n",
    "import torchaudio \n",
    "import math\n",
    "import yaml\n",
    "from tensorflow_tts.inference import AutoConfig\n",
    "import json\n",
    "\n",
    "import torch.onnx \n",
    "import onnx\n",
    "from onnx_tf.backend import prepare\n",
    "import onnxruntime\n",
    "\n",
    "torch.__version__\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print('Using {} device'.format(device))\n",
    "\n",
    "from config import VisemeModelConfiguration\n",
    "config = VisemeModelConfiguration(batch_size=20)\n",
    "config.save(\"output/viseme_model.json\")\n",
    "config.model_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "623909ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from audio import pad_audio, load_and_pad_audio, coeffs_to_windows\n",
    "\n",
    "\n",
    "\n",
    "def coeffs_to_windows(coeffs, config):\n",
    "    output = np.zeros((config[\"seq_length\"], config[\"stftFramesPerWindow\"] *\n",
    "        config[\"numMels\"]))\n",
    "\n",
    "    hop = int(config[\"stftFramesPerWindow\"]/2)\n",
    "\n",
    "    frameIdx = 0;\n",
    "\n",
    "    for i in range(config[\"seq_length\"]):\n",
    "\n",
    "        startIdx = max(frameIdx-hop, 0)\n",
    "        endIdx = min(frameIdx+hop, coeffs.shape[0])\n",
    "        window = coeffs[startIdx:endIdx,:]\n",
    "\n",
    "        if frameIdx - hop < 0:\n",
    "            window = np.pad(window, [((hop*2) - window.shape[0], 0), (0,0)], constant_values=coeffs[0,0])\n",
    "        elif frameIdx + hop > coeffs.shape[0]:\n",
    "            window = np.pad(window, [(0,(hop*2) - window.shape[0]),(0,0)], constant_values=coeffs[0,0])\n",
    "        #print(window.shape)\n",
    "        output[i] = np.reshape(window, config[\"stftFramesPerWindow\"]*config[\"numMels\"])\n",
    "        frameIdx += hop\n",
    "    return output\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "config.model_config[\"window\"] = \"hanning\"\n",
    "#config.model_config[\"hopSize\"] = 100\n",
    "#tftts_mels(\"the_dog_barked_its_way2.wav\", config.model_config)[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b538720e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0.000000\n",
       "14     0.000093\n",
       "28     0.002042\n",
       "42     0.002919\n",
       "56     0.075545\n",
       "70     0.321100\n",
       "84     0.131893\n",
       "98     0.591477\n",
       "112    0.069186\n",
       "126    0.291685\n",
       "140    0.205711\n",
       "154    0.103206\n",
       "168    0.653481\n",
       "182    0.252912\n",
       "196    0.010072\n",
       "210    0.000000\n",
       "224    0.000000\n",
       "238    0.000000\n",
       "252    0.000109\n",
       "266    0.000000\n",
       "280    0.000000\n",
       "294    0.000000\n",
       "Name: JawOpen, dtype: float64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "#base = pd.read_csv(\"data/training/speaker_3/MySlate_14_Nic_88.csv\").head(1)\n",
    "\n",
    "process_audio = functools.partial(kaldi_mels, \n",
    "                                          config=config.model_config)      \n",
    "\n",
    "process_viseme = functools.partial(preprocess_viseme, \n",
    "                                   pad_len_in_secs=config.model_config[\"paddedAudioLength\"], \n",
    "                                   blendshapes=config.sourceKeys, \n",
    "                                   source_framerate=config.source_config[\"framerate\"],\n",
    "                                   collapse_factor=int(config.source_config[\"framerate\"] / config.model_config[\"frameRate\"]),\n",
    "                                  )\n",
    "\n",
    "training_data = VisemeDataset(\"./data/training/speaker_3/\", \n",
    "                              process_audio, \\\n",
    "                              process_viseme, \n",
    "                              num_ipa_symbols=157,\n",
    "                             text_pad_len=config.text_pad_len)\n",
    "test_data = VisemeDataset(\"./data/test/speaker_3/\", \n",
    "                              process_audio, \\\n",
    "                              process_viseme,\n",
    "                              num_ipa_symbols=157,\n",
    "                              text_pad_len=config.text_pad_len\n",
    "                             )\n",
    "train_dataloader = DataLoader(training_data, batch_size=config.batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=config.batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_viseme(path,\n",
    "                      pad_len_in_secs=None,\n",
    "                      collapse_factor=None,\n",
    "                      blendshapes=None,\n",
    "                      source_framerate=None):\n",
    "\n",
    "    csv = pd.read_csv(path)\n",
    "    # remove the Timecode column because we don't need it\n",
    "    columns = list(csv.columns)\n",
    "    columns.remove(\"Timecode\")\n",
    "    if blendshapes is not None:\n",
    "        columns = [c for c in columns if c in blendshapes]\n",
    "    csv = csv[columns]\n",
    "    csv.reset_index()\n",
    "\n",
    "    # pad the visemes to the intended length, using the first row as our base\n",
    "    pad_len = int(pad_len_in_secs * source_framerate)\n",
    "    \n",
    "    if csv.shape[0] < pad_len:\n",
    "        pad_indices = csv.shape[0]\n",
    "        csv = csv.append([csv.head(1)] * (pad_len - csv.shape[0]),ignore_index=True)\n",
    "        csv.iloc[pad_indices:] = 0\n",
    "    else:\n",
    "        csv = csv.iloc[:pad_len]\n",
    "        #print(\"Visemes exceeded max length, truncate?\")\n",
    "    \n",
    "    # reduce the framerate by taking the mean of every X frames\n",
    "    if collapse_factor is not None:\n",
    "        i = 0\n",
    "        while i < csv.shape[0]:\n",
    "            csv.iloc[i] = csv.iloc[i:i+collapse_factor].mean()\n",
    "            i += collapse_factor\n",
    "        csv = csv.iloc[::collapse_factor]\n",
    "    return csv\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df = preprocess_viseme(\n",
    "         \"data/training/speaker_3/20211003_MySlate_27_27.csv\", \n",
    "#     #\"data/training/speaker_3/MySlate_14_Nic_88.csv\",\n",
    "         pad_len_in_secs=config.model_config[\"paddedAudioLength\"], \n",
    "         source_framerate=config.source_config[\"framerate\"],\n",
    "         collapse_factor=int(config.source_config[\"framerate\"] / config.model_config[\"frameRate\"]),\n",
    ")\n",
    "\n",
    "# #df = df[list(config.source_config[\"mappings\"].keys())]#df[sourceKeys]\n",
    "# #df.columns = list(config.source_config[\"mappings\"].keys()) #model_config[\"targetNames\"]\n",
    "\n",
    "# #df.to_csv(\"output/predicted.csv\", index=False)\n",
    "df[\"JawOpen\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e493a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 20, 520])\n"
     ]
    }
   ],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_dim=None):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        \n",
    "        self.attn = nn.MultiheadAttention(input_dim, 8, batch_first=True)\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(input_dim)\n",
    "        \n",
    "        self.ff1 = nn.Sequential(\n",
    "            nn.Linear(input_dim, input_dim*4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(input_dim*4, input_dim)            \n",
    "        )\n",
    "        \n",
    "        self.norm2 = nn.LayerNorm(input_dim)\n",
    "               \n",
    "    def forward(self, feats):\n",
    "        attn_out, _ = self.attn(feats, feats, feats)\n",
    "        out = feats + attn_out\n",
    "        out = self.norm1(out)\n",
    "        f = self.ff1(out)\n",
    "        f += out\n",
    "        out = self.norm2(out)\n",
    "        return out\n",
    "                \n",
    "        \n",
    "class SelfAttnModel(nn.Module):\n",
    "    def __init__(self, \n",
    "                 seq_length=None, \n",
    "                 input_dim=None, \n",
    "                 text_dim=None, \n",
    "                 text_embedding_dim=256,\n",
    "                 text_padding_idx=None,\n",
    "                 num_visemes=15):\n",
    "        super(SelfAttnModel, self).__init__()\n",
    "        self.text_embedding = nn.Sequential(\n",
    "                nn.Embedding(text_dim,text_embedding_dim),                \n",
    "        )\n",
    "        self.text_attention = nn.MultiheadAttention(text_embedding_dim, 4, batch_first=True)\n",
    "        \n",
    "        self.audio_proj = nn.Sequential(\n",
    "            nn.Linear(input_dim,input_dim*4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(input_dim*4,input_dim)\n",
    "        )\n",
    "        \n",
    "        self.t1 = TransformerBlock(input_dim)\n",
    "        self.t2 = TransformerBlock(input_dim)\n",
    "        self.t3 = TransformerBlock(input_dim)\n",
    "        self.t4 = TransformerBlock(input_dim)\n",
    "\n",
    "        self.dropout = torch.nn.Dropout(p=0.1)\n",
    "        \n",
    "        self.text_ff = nn.Sequential(\n",
    "            nn.Linear(text_embedding_dim, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, text_embedding_dim)            \n",
    "        )\n",
    "        \n",
    "        self.linear_out = nn.Sequential(\n",
    "            nn.Linear(input_dim, num_visemes)            \n",
    "        )            \n",
    "        \n",
    "        self.tblock = nn.Sequential(\n",
    "            self.t1,\n",
    "            self.t2,\n",
    "        #    self.t3,\n",
    "         #   self.t4,\n",
    "        )\n",
    "        \n",
    "    def forward(self, audio_feats, text_feats):\n",
    "        a = self.tblock(self.audio_proj(audio_feats))\n",
    "        return self.linear_out(a)\n",
    "\n",
    "input_dim=1248\n",
    "model = SelfAttnModel(\n",
    "     input_dim=input_dim,\n",
    "      text_embedding_dim=2048,\n",
    "     num_visemes=len(config.model_config[\"targetNames\"]),\n",
    "     text_dim=training_data.num_ipa_symbols).to(device)\n",
    "\n",
    "\n",
    "batch = iter(train_dataloader)\n",
    "audio_features, text_features, num_padded, train_labels, train_files = next(batch, (None,None,None,None,None))\n",
    "print(audio_features.size())\n",
    "\n",
    "learning_rate = 0.0005\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "\n",
    "num_steps = 100000\n",
    "print_loss_every = 50\n",
    "eval_every = 100\n",
    "\n",
    "accum_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98c0faf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim=None, text_dim=None, hidden_size=512, num_visemes=4):\n",
    "        super(BiLSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = torch.nn.LSTM(input_dim, hidden_size, 1, bidirectional=True)#,proj_size=int(hidden_size/2))\n",
    "        self.text_embedding = nn.Embedding(text_dim,hidden_size*2)\n",
    "        \n",
    "        self.attention = nn.MultiheadAttention(int(hidden_size)*2, 1, batch_first=True)\n",
    "        \n",
    "        self.text_attention = nn.MultiheadAttention(hidden_size*2, 1, batch_first=True)\n",
    "        self.output_linear = torch.nn.Linear(int(hidden_size)*2, num_visemes)\n",
    "        #self.output_linear = nn.ModuleList([torch.nn.Linear(hidden_size*2, 1) for v in range(num_visemes)])\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        out_f, (h,c) = self.lstm(x)\n",
    "        out_f, attn_output_weights = self.attention(out_f, out_f, out_f)\n",
    "        out_t =self.text_embedding(t)\n",
    "        out_t,_ = self.text_attention(out_t,out_t,out_t)\n",
    "        out_f += out_t\n",
    "        return self.output_linear(out_f)\n",
    "        #return self.output_linear],dim=2),dim=3)\n",
    "\n",
    "input_dim=520\n",
    "\n",
    "model = BiLSTMModel(\n",
    "    input_dim=input_dim,\n",
    "    num_visemes=len(config.model_config[\"targetNames\"]),\n",
    "    hidden_size=128,\n",
    "    text_dim=training_data.num_ipa_symbols\n",
    ").to(device)\n",
    "\n",
    "\n",
    "learning_rate = 0.00005\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "\n",
    "num_steps = 100000\n",
    "print_loss_every = 500\n",
    "eval_every = 1000\n",
    "\n",
    "accum_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01b7f48d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 20, 4])\n",
      "torch.Size([20, 20, 520])\n"
     ]
    }
   ],
   "source": [
    "batch = iter(train_dataloader)\n",
    "\n",
    "audio_features, text_features, num_padded, train_labels, train_files = next(batch, (None,None,None,None,None))\n",
    "train_labels = train_labels[:, :20,]\n",
    "print(train_labels.size())\n",
    "print(audio_features.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c102a5f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 500 Avg loss: 0.014349785579368472\n",
      "Step 1000 Avg loss: 0.01263991227094084\n",
      "Step 1500 Avg loss: 0.012164079545997084\n",
      "Step 2000 Avg loss: 0.0104728421587497\n",
      "Step 2500 Avg loss: 0.010174975720234215\n",
      "Step 3000 Avg loss: 0.008762580586597324\n",
      "Step 3500 Avg loss: 0.008707860549911857\n",
      "Step 4000 Avg loss: 0.007490967528894544\n",
      "Step 4500 Avg loss: 0.007644269648939371\n",
      "Step 5000 Avg loss: 0.006536910380236805\n",
      "Step 5500 Avg loss: 0.0068740289038978514\n",
      "Step 6000 Avg loss: 0.005814374113455415\n",
      "Step 6500 Avg loss: 0.006138323207385838\n",
      "Step 7000 Avg loss: 0.005237848003394901\n",
      "Step 7500 Avg loss: 0.0056132680959999565\n",
      "Step 8000 Avg loss: 0.0047075663055293265\n",
      "Step 8500 Avg loss: 0.0051784727447666224\n",
      "Step 9000 Avg loss: 0.004381308658979833\n",
      "Step 9500 Avg loss: 0.004891385016962886\n",
      "Step 10000 Avg loss: 0.004041455454658717\n",
      "Step 10500 Avg loss: 0.004642288046889007\n",
      "Step 11000 Avg loss: 0.003762567857746035\n",
      "Step 11500 Avg loss: 0.004338051002472639\n",
      "Step 12000 Avg loss: 0.0035140572586096824\n",
      "Step 12500 Avg loss: 0.004068725677672773\n",
      "Step 13000 Avg loss: 0.0033034805064089596\n",
      "Step 13500 Avg loss: 0.0038829610124230386\n",
      "Step 14000 Avg loss: 0.00313683955790475\n",
      "Step 14500 Avg loss: 0.0037334267909172924\n",
      "Step 15000 Avg loss: 0.0029834902321454137\n",
      "Step 15500 Avg loss: 0.0036279537715017797\n",
      "Step 16000 Avg loss: 0.0028266302028205244\n",
      "Step 16500 Avg loss: 0.003485212326515466\n",
      "Step 17000 Avg loss: 0.0027505032618064433\n",
      "Step 17500 Avg loss: 0.0033096078289672733\n",
      "Step 18000 Avg loss: 0.0026257899394258855\n",
      "Step 18500 Avg loss: 0.0032491399236023425\n",
      "Step 19000 Avg loss: 0.0024627171505708246\n",
      "Step 19500 Avg loss: 0.0030923229232430456\n",
      "Step 20000 Avg loss: 0.0023559917525853963\n",
      "Step 20500 Avg loss: 0.002988984626485035\n",
      "Step 21000 Avg loss: 0.0022553295404650273\n",
      "Step 21500 Avg loss: 0.0028627029829658567\n"
     ]
    }
   ],
   "source": [
    "for t in range(num_steps):\n",
    "    optimizer.zero_grad()\n",
    "        \n",
    "    audio_features, text_features, num_padded, train_labels, train_files = next(batch, (None,None,None,None,None))\n",
    "    \n",
    "    if audio_features is None:\n",
    "        batch = iter(train_dataloader)\n",
    "        audio_features, text_features, num_padded, train_labels, _ = next(batch)\n",
    "\n",
    "    train_labels = train_labels[:, :20,]\n",
    "\n",
    "    preds = model(audio_features.to(device),text_features.to(device)) \n",
    "    \n",
    "    train_labels = train_labels.to(device)\n",
    "    \n",
    "    \n",
    "    loss = torch.nn.functional.mse_loss(preds, train_labels.to(device))   \n",
    "    #loss = torch.nn.functional.huber_loss(preds, train_labels.to(device))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if len(preds.shape) < 3:\n",
    "        preds = torch.unsqueeze(preds, 0)\n",
    "    \n",
    "    #for i in range(preds.shape[1] - 1):\n",
    "    #    cosine_loss = torch.nn.functional.cosine_embedding_loss(\n",
    "    #        preds[:,i,:], \n",
    "    #        preds[:,i+1,:], \n",
    "    #        (torch.ones(preds.shape[0])\n",
    "    #    ).to(device))\n",
    "    #    loss += cosine_loss\n",
    "    \n",
    "    accum_loss += loss.item()\n",
    "    if t > 0 and t % print_loss_every == 0:\n",
    "        print(f\"Step {t} Avg loss: {accum_loss / print_loss_every}\")\n",
    "        accum_loss = 0\n",
    "        \n",
    "\n",
    "\n",
    "    if t > 0 and t % eval_every == 0:\n",
    "        accum_loss = 0\n",
    "        for audio_feats, text_feats, test_mask, test_labels, _ in iter(test_dataloader):\n",
    "            text_feats = text_feats#[:,:35]\n",
    "            test_labels = test_labels[:,:20]\n",
    "\n",
    "            y = test_labels.to(device)\n",
    "            preds = model(audio_feats.to(device), text_feats.to(device))\n",
    "            #accum_loss += torch.nn.functional.mse_loss(preds, y).item()\n",
    "            accum_loss += torch.nn.functional.huber_loss(preds, y).item()\n",
    "\n",
    "#         print(f\"Test loss {accum_loss}\")\n",
    "#         accum_loss = 0\n",
    "    \n",
    "#pred_probab = nn.Softmax(dim=1)(logits)\n",
    "#y_pred = pred_probab.argmax(1)\n",
    "#print(f\"Predicted class: {y_pred}\")199"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5fce1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "Step 500 Avg loss: 0.04806631846912205\n",
    "Step 1000 Avg loss: 0.002916151871555485\n",
    "Step 1500 Avg loss: 0.004990221144282259\n",
    "Step 2000 Avg loss: 0.0020618126165063587\n",
    "Step 2500 Avg loss: 0.0005171663404908031\n",
    "Step 3000 Avg loss: 0.0010838660373847233\n",
    "Step 3500 Avg loss: 0.0002996790710603818\n",
    "Step 4000 Avg loss: 0.0010451974497264018\n",
    "Step 4500 Avg loss: 0.0005168943077296717\n",
    "Step 5000 Avg loss: 0.00038338914602172735\n",
    "Step 5500 Avg loss: 0.00023643585942409117\n",
    "Step 6000 Avg loss: 0.0003196997099294094\n",
    "Step 6500 Avg loss: 0.00030932931147981433\n",
    "Step 7000 Avg loss: 0.00029204771798686124\n",
    "Step 7500 Avg loss: 7.222784709119878e-05\n",
    "Step 8000 Avg loss: 0.00017775368443471962\n",
    "Step 8500 Avg loss: 0.00019257872350499384\n",
    "Step 9000 Avg loss: 0.00022026305821873393\n",
    "Step 9500 Avg loss: 6.478172176593944e-05\n",
    "Step 10000 Avg loss: 0.00013493354754427856\n",
    "Step 10500 Avg loss: 5.010930418575299e-05\n",
    "Step 11000 Avg loss: 0.00011357640103597077\n",
    "Step 11500 Avg loss: 8.715347881980051e-05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df094758",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_features[0,0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6011708",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113913a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_labels.size())\n",
    "train_labels[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3359dc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.set_printoptions(profile=\"full\")\n",
    "\n",
    "\n",
    "model(audio_features.to(device), text_features.to(device))[0]\n",
    "\n",
    "#train_labels\n",
    "#audio_features[0,3,:10]\n",
    "#audio_features.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4e80c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(\"output/conv1d.torch\").to(device)\n",
    "model.eval()\n",
    "fft, num_samples = process_audio(\"the_dog_barked_its_way2.wav\")\n",
    "fft = torch.unsqueeze(fft,dim=0)\n",
    "print(fft.size())\n",
    "\n",
    "ipa_indices = [[int (x) for x in \"158 69 76 157 31 108 68 157 29 0 115 98 118 157 70 118 116 157 139 34 157 23 118 157 40 137 157 32 40 157 115 131 100 159 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160\".split(\" \")]]\n",
    "ipa_indices = torch.tensor(ipa_indices,dtype=torch.int32)\n",
    "model(fft.to(device), ipa_indices.to(device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b164b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "header = \"Timecode,BlendShapeCount,eyeBlinkRight,eyeLookDownRight,eyeLookInRight,eyeLookOutRight,eyeLookUpRight,eyeSquintRight,eyeWideRight,eyeBlinkLeft,eyeLookDownLeft,eyeLookInLeft,eyeLookOutLeft,eyeLookUpLeft,eyeSquintLeft,eyeWideLeft,jawForward,jawRight,jawLeft,jawOpen,mouthClose,mouthFunnel,mouthPucker,mouthRight,mouthLeft,mouthSmileRight,mouthSmileLeft,mouthFrownRight,mouthFrownLeft,mouthDimpleRight,mouthDimpleLeft,mouthStretchRight,mouthStretchLeft,mouthRollLower,mouthRollUpper,mouthShrugLower,mouthShrugUpper,mouthPressRight,mouthPressLeft,mouthLowerDownRight,mouthLowerDownLeft,mouthUpperUpRight,mouthUpperUpLeft,browDownRight,browDownLeft,browInnerUp,browOuterUpRight,browOuterUpLeft,cheekPuff,cheekSquintRight,cheekSquintLeft,noseSneerRight,noseSneerLeft,tongueOut,HeadYaw,HeadPitch,HeadRoll,LeftEyeYaw,LeftEyePitch,LeftEyeRoll,RightEyeYaw,RightEyePitch,RightEyeRoll\".split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41510411",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_feats, text_feats, _, _, files = next(iter(test_dataloader))\n",
    "export_y_batch = model(audio_feats.to(device), text_feats.to(device))\n",
    "export_y = export_y_batch[0,:,:]\n",
    "\n",
    "\n",
    "\n",
    "selected_output_indices = [header.index(x[0].lower() + x[1:]) for x in source_config[\"mappings\"].keys()]\n",
    "num_visemes = len(source_config[\"mappings\"])\n",
    "\n",
    "# the prediction in LiveLink Face format\n",
    "with open(\"output/prediction.csv\", \"w\") as outfile:\n",
    "    outfile.write(\",\".join(header) + \"\\n\")\n",
    "    timer_ms = 0\n",
    "    for t in range(export_y.shape[1]):\n",
    "        output = [str(0)] * len(header)\n",
    "        second = str(int(timer_ms // 1000)).zfill(2)\n",
    "        frame = (timer_ms % 1000) * model_config[\"frameRate\"] / 1000\n",
    "        output[0] = f\"00:00:{second}:{frame}\"\n",
    "        for viseme in range(num_visemes): \n",
    "            output[selected_output_indices[viseme]] = str(export_y[viseme,t].item())\n",
    "        timer_ms += (1 / model_config[\"frameRate\"]) * 1000\n",
    "        outfile.write(\",\".join(output) + \"\\n\")\n",
    "        \n",
    "# the prediction in App format\n",
    "with open(\"output/prediction_app.csv\", \"w\") as outfile:\n",
    "    outfile.write(\",\".join(model_config[\"targetNames\"]) + \"\\n\")\n",
    "    for t in range(export_y.shape[0]):\n",
    "        for v in range(export_y.shape[1]):\n",
    "            outfile.write(str(export_y[t,v].item()))\n",
    "            outfile.write(\",\")\n",
    "        outfile.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16652a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_header = \"Timecode,BlendShapeCount,EyeBlinkLeft,EyeLookDownLeft,EyeLookInLeft,EyeLookOutLeft,EyeLookUpLeft,EyeSquintLeft,EyeWideLeft,EyeBlinkRight,EyeLookDownRight,EyeLookInRight,EyeLookOutRight,EyeLookUpRight,EyeSquintRight,EyeWideRight,JawForward,JawRight,JawLeft,JawOpen,MouthClose,MouthFunnel,MouthPucker,MouthRight,MouthLeft,MouthSmileLeft,MouthSmileRight,MouthFrownLeft,MouthFrownRight,MouthDimpleLeft,MouthDimpleRight,MouthStretchLeft,MouthStretchRight,MouthRollLower,MouthRollUpper,MouthShrugLower,MouthShrugUpper,MouthPressLeft,MouthPressRight,MouthLowerDownLeft,MouthLowerDownRight,MouthUpperUpLeft,MouthUpperUpRight,BrowDownLeft,BrowDownRight,BrowInnerUp,BrowOuterUpLeft,BrowOuterUpRight,CheekPuff,CheekSquintLeft,CheekSquintRight,NoseSneerLeft,NoseSneerRight,TongueOut,HeadYaw,HeadPitch,HeadRoll,LeftEyeYaw,LeftEyePitch,LeftEyeRoll,RightEyeYaw,RightEyePitch,RightEyeRoll\"\n",
    "remap = {h:(h[0].lower() + h[1:]) if h not in [\"Timecode\",\"BlendShapeCount\",\"HeadYaw\",\"HeadPitch\",\"HeadRoll\",\"LeftEyeYaw\",\"LeftEyePitch\",\"LeftEyeRoll\",\"RightEyeYaw\",\"RightEyePitch\",\"RightEyeRoll\"]  else h for h in new_header.split(\",\") }\n",
    "for oh in remap.values():\n",
    "    if oh not in header:\n",
    "        print(oh)\n",
    "\n",
    "def new_to_old(csv_df):\n",
    "    return csv_df.rename(columns=remap)\n",
    "df = preprocess_viseme(\"data/training/speaker_3/MySlate_22_Nic_ps-15_20.csv\", pad_len_in_secs=model_config[\"paddedAudioLength\"], \n",
    "                                   target_framerate=model_config[\"frameRate\"])\n",
    "df = new_to_old(df)\n",
    "#cols = [c for c in df.columns if c not in [\"jawOpen\", \"mouthClose\", \"Timecode\"]]\n",
    "#df[cols] = 0\n",
    "df = df[df[\"Timecode\"] != 0]\n",
    "df.to_csv(\"output/predicted.csv\", index=False)\n",
    "#new_to_old(df)[header].to_csv(\"original.csv\", index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12905489",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70887b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = audio_features.size()[1]\n",
    "torch.save(model, \"output/bilstm.torch\")\n",
    "model = torch.load(\"output/bilstm.torch\",map_location=torch.device('cpu'))\n",
    "\n",
    "model.eval() \n",
    "\n",
    "dummy_audio_input = torch.randn(1, seq_length, input_dim, requires_grad=False)  \n",
    "print(dummy_audio_input)\n",
    "dummy_text_input = torch.zeros(1, seq_length, requires_grad=False,dtype=torch.int)\n",
    "print(dummy_text_input)\n",
    "\n",
    "# Export the model   \n",
    "torch.onnx.export(model,         # model being run \n",
    "     (dummy_audio_input,dummy_text_input),      # model input (or a tuple for multiple inputs) \n",
    "     \"output/bilstm.onnx\",       # where to save the model  \n",
    "     export_params=True,  # store the trained parameter weights inside the model file \n",
    "     opset_version=10,    # the ONNX version to export the model to \n",
    "     do_constant_folding=True,  # whether to execute constant folding for optimization \n",
    "     input_names = ['audio_feats', 'text_feats'],   # the model's input names \n",
    "     output_names = ['modelOutput'], # the model's output names ,\n",
    "\n",
    ") \n",
    "\n",
    "model_onnx = onnx.load('output/bilstm.onnx')\n",
    "\n",
    "tf_rep = prepare(model_onnx)\n",
    "tf_rep.export_graph('./output/tf_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953bfeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(\"./output/tf_model\")\n",
    "print(\"Built converter\")\n",
    "\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,\n",
    "                                       tf.lite.OpsSet.SELECT_TF_OPS]\n",
    "converter.allow_custom_ops=False\n",
    "#converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.experimental_enable_resource_variables = True\n",
    "\n",
    "converter.experimental_new_converter =True\n",
    "tflite_model = converter.convert()\n",
    "print(\"Converted\")\n",
    "\n",
    "# Save the model\n",
    "with open(\"./output/bilstm.tflite\", 'wb') as f:\n",
    "    f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3efc8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter = tf.lite.Interpreter(model_path=\"output/bilstm.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "    \n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "#input_shape = input_details[0]['shape']\n",
    "#output_details\n",
    "input_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f1f546",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv = pd.read_csv(\"data/training/speaker_1/20210824_1/61.csv\")\n",
    "columns = [x for x in list(csv.columns) if \"Eye\" not in x]\n",
    "columns.remove(\"Timecode\")\n",
    "columns.remove(\"BlendShapeCount\")\n",
    "csv[columns].var().sort_values()\n",
    "#df = preprocess_viseme(\"data/training/speaker_1/20210824_1/61.csv\", pad_len_in_secs=pad_len_in_secs, \n",
    "#                                   resample_to=target_framerate, blendshapes=[\"MouthClose\",\"MouthFunnel\"])\n",
    "#df.shape\n",
    "#[df.iloc[0][\"EyeLookInLeft\"]]\n",
    "    #csv[columns] = pd.np.digitize(csv[columns], np.linspace(0,1,11))\n",
    "    \n",
    "    #split = csv[\"Timecode\"].str.split(':')\n",
    "    #minute = split.str[1].astype(int)\n",
    "    #second = split.str[2].astype(int)\n",
    "    #frame = split.str[3].astype(float)\n",
    "    #minute -= minute[0]\n",
    "    #ms\n",
    "    #step = minute * 60 + second\n",
    "    #csv[\"step\"] = step\n",
    "    #return csv.drop_duplicates([\"step\"])[[\"step\", \"MouthClose\",\"MouthFunnel\",\"MouthPucker\",\"JawOpen\"]]\n",
    "    \n",
    "# if we want to use softmax across each blendshape as a one-hot\n",
    "    #return np.reshape(vals, (vals.shape[0], vals.shape[1], 1))\n",
    "    #one_hot = np.zeros((vals.shape[0], vals.shape[1], 11, 1))\n",
    "    #oh = np.eye(11)\n",
    "    #for row in range(vals.shape[0]):\n",
    "    #    for t in range(vals.shape[1]):\n",
    "    #        one_hot[row, :, :, 0] = np.eye(11)[int(vals[row,t])-1]\n",
    "    #return one_hot"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
