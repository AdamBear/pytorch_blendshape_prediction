{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fae648b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: soundfile in /home/hydroxide/.local/lib/python3.7/site-packages (0.10.3.post1)\n",
      "Requirement already satisfied: cffi>=1.0 in /home/hydroxide/.local/lib/python3.7/site-packages (from soundfile) (1.14.6)\n",
      "Requirement already satisfied: pycparser in /home/hydroxide/.local/lib/python3.7/site-packages (from cffi>=1.0->soundfile) (2.20)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in /home/hydroxide/.local/lib/python3.7/site-packages (1.3.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/hydroxide/.local/lib/python3.7/site-packages (from pandas) (2021.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/hydroxide/.local/lib/python3.7/site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /home/hydroxide/.local/lib/python3.7/site-packages (from pandas) (1.21.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/hydroxide/.local/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: scipy in /home/hydroxide/.local/lib/python3.7/site-packages (1.7.1)\n",
      "Requirement already satisfied: numpy<1.23.0,>=1.16.5 in /home/hydroxide/.local/lib/python3.7/site-packages (from scipy) (1.21.2)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: librosa in /home/hydroxide/.local/lib/python3.7/site-packages (0.8.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/hydroxide/.local/lib/python3.7/site-packages (from librosa) (1.21.2)\n",
      "Requirement already satisfied: resampy>=0.2.2 in /home/hydroxide/.local/lib/python3.7/site-packages (from librosa) (0.2.2)\n",
      "Requirement already satisfied: joblib>=0.14 in /home/hydroxide/.local/lib/python3.7/site-packages (from librosa) (1.0.1)\n",
      "Requirement already satisfied: soundfile>=0.10.2 in /home/hydroxide/.local/lib/python3.7/site-packages (from librosa) (0.10.3.post1)\n",
      "Requirement already satisfied: scipy>=1.0.0 in /home/hydroxide/.local/lib/python3.7/site-packages (from librosa) (1.7.1)\n",
      "Requirement already satisfied: pooch>=1.0 in /home/hydroxide/.local/lib/python3.7/site-packages (from librosa) (1.4.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/hydroxide/.local/lib/python3.7/site-packages (from librosa) (21.0)\n",
      "Requirement already satisfied: decorator>=3.0.0 in /home/hydroxide/.local/lib/python3.7/site-packages (from librosa) (5.0.9)\n",
      "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /home/hydroxide/.local/lib/python3.7/site-packages (from librosa) (0.24.2)\n",
      "Requirement already satisfied: audioread>=2.0.0 in /home/hydroxide/.local/lib/python3.7/site-packages (from librosa) (2.1.9)\n",
      "Requirement already satisfied: numba>=0.43.0 in /home/hydroxide/.local/lib/python3.7/site-packages (from librosa) (0.48.0)\n",
      "Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /home/hydroxide/.local/lib/python3.7/site-packages (from numba>=0.43.0->librosa) (0.31.0)\n",
      "Requirement already satisfied: setuptools in /home/hydroxide/.local/lib/python3.7/site-packages (from numba>=0.43.0->librosa) (58.0.4)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/hydroxide/.local/lib/python3.7/site-packages (from packaging>=20.0->librosa) (2.4.7)\n",
      "Requirement already satisfied: appdirs in /home/hydroxide/.local/lib/python3.7/site-packages (from pooch>=1.0->librosa) (1.4.4)\n",
      "Requirement already satisfied: requests in /home/hydroxide/.local/lib/python3.7/site-packages (from pooch>=1.0->librosa) (2.25.1)\n",
      "Requirement already satisfied: six>=1.3 in /home/hydroxide/.local/lib/python3.7/site-packages (from resampy>=0.2.2->librosa) (1.15.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/hydroxide/.local/lib/python3.7/site-packages (from scikit-learn!=0.19.0,>=0.14.0->librosa) (2.2.0)\n",
      "Requirement already satisfied: cffi>=1.0 in /home/hydroxide/.local/lib/python3.7/site-packages (from soundfile>=0.10.2->librosa) (1.14.6)\n",
      "Requirement already satisfied: pycparser in /home/hydroxide/.local/lib/python3.7/site-packages (from cffi>=1.0->soundfile>=0.10.2->librosa) (2.20)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/hydroxide/.local/lib/python3.7/site-packages (from requests->pooch>=1.0->librosa) (1.26.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/hydroxide/.local/lib/python3.7/site-packages (from requests->pooch>=1.0->librosa) (2021.5.30)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/hydroxide/.local/lib/python3.7/site-packages (from requests->pooch>=1.0->librosa) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/hydroxide/.local/lib/python3.7/site-packages (from requests->pooch>=1.0->librosa) (2.10)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pykaldi in /home/hydroxide/.local/lib/python3.7/site-packages (0.2.1)\n",
      "Requirement already satisfied: numpy in /home/hydroxide/.local/lib/python3.7/site-packages (from pykaldi) (1.21.2)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torchaudio in /home/hydroxide/.local/lib/python3.7/site-packages (0.9.0)\n",
      "Requirement already satisfied: torch==1.9.0 in /home/hydroxide/.local/lib/python3.7/site-packages (from torchaudio) (1.9.0)\n",
      "Requirement already satisfied: typing-extensions in /home/hydroxide/.local/lib/python3.7/site-packages (from torch==1.9.0->torchaudio) (3.7.4.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "################################################################################\n",
      "### WARNING, path does not exist: KALDI_ROOT=/mnt/matylda5/iveselyk/Tools/kaldi-trunk\n",
      "###          (please add 'export KALDI_ROOT=<your_path>' in your $HOME/.profile)\n",
      "###          (or run as: KALDI_ROOT=<your_path> python <your_script>.py)\n",
      "################################################################################\n",
      "\n",
      "/home/hydroxide/.local/lib/python3.7/site-packages/tensorflow_addons/utils/ensure_tf_install.py:67: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.3.0 and strictly below 2.6.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.6.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  UserWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "!pip install soundfile\n",
    "!pip install pandas\n",
    "!pip install scipy\n",
    "!pip install librosa\n",
    "!pip install pykaldi\n",
    "!pip install torchaudio\n",
    "\n",
    "from models import BiLSTMModel\n",
    "from dataset import preprocess_viseme, VisemeDataset\n",
    "from audio import load_and_pad_audio\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import scipy\n",
    "from scipy import fft\n",
    "import functools\n",
    "from torch import nn\n",
    "import librosa\n",
    "import math\n",
    "import torchaudio \n",
    "import math\n",
    "import yaml\n",
    "from tensorflow_tts.inference import AutoConfig\n",
    "import json\n",
    "\n",
    "\n",
    "from dataset import preprocess_viseme, VisemeDataset\n",
    "\n",
    "torch.__version__\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print('Using {} device'.format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79c5e19",
   "metadata": {},
   "source": [
    "\\---S1---/\\---S2---/\\---S3---/\n",
    "WWWWWWWWWWWWWWWWWWWWWWWWWWWWWW\n",
    "____________\\---V1---/\n",
    "_____________AAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\n",
    "\n",
    "             \\---S1---/\\---S2---/\\---S3---/\n",
    "             WWWWWWWWWWWWWWWWWWWWWWWWWWWWWW\n",
    "______________________\\---V2---/\n",
    "_____________AAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\n",
    "\n",
    "\n",
    "A = audio sample\n",
    "V1, V2, etc = viseme 1, viseme 2, etc (aka \"frame\", but not in the MFCC sense)\n",
    "W = a window of audio samples that will be used as input (aka \"frame\" in the MFCC sense. to avoid confusion, we refer to this as the \"window\" and the viseme as the \"frame\")\n",
    "S1, S2, S3 = STFT of a window \n",
    "num_frames == num_windows\n",
    "\n",
    "audio_bins_per_window = the number of S per W\n",
    "samples_per_bin = the length of each S (i.e. number of As)\n",
    "bin_hop_length = the number of A between S1 and S2 (in the picture above, hop_length == len(S1) == len(S2) == samples_per_bin\n",
    "\n",
    "in practice, we calculate as such:\n",
    "          /-----------V2--------------\\\n",
    "/--------------V1------------\\\n",
    "\\---S1---/\\---S2---/\\---S3---/\\---S4---/\\---S5---/\\---S6---/\n",
    "_______________AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\n",
    "\n",
    "then at V1, we take {S1,S2,S3}, at V2 we take {S4,S5,S6}, etc\n",
    "The STFT \n",
    "STFT hop length should therefore equal the length of a single viseme frame, in samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0de78e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let num_stft_frames be the number of STFT frames per audio sample \n",
    "# num_stft_frames == (audio_len / n_fft)\n",
    "# (assuming FFT window length == hop length == n_ffts)\n",
    "# coeffs will be B x num_stft_frames x num_mels\n",
    "# every \n",
    "# if we've calculated the FFT size correctly, the hop length will be half \n",
    "# so stft_frames_per_window should be odd\n",
    "\n",
    "def coeffs_to_windows(coeffs, num_frames, stft_frames_per_window, n_mels):\n",
    "    \n",
    "    output = np.zeros((num_frames, stft_frames_per_window*n_mels))\n",
    "    \n",
    "    # the number of frames to hop is just half the number of STFT frames per window\n",
    "    hop_len = int(stft_frames_per_window / 2)\n",
    "    \n",
    "    for i in range(num_frames):\n",
    "        c = coeffs[:, (i*hop_len):(i*hop_len)+stft_frames_per_window]\n",
    "        # padding\n",
    "        if(c.shape[1] < stft_frames_per_window):\n",
    "            c = np.pad(c, [(0,0), (0, stft_frames_per_window - c.shape[1])], constant_values=0)\n",
    "        output[i, :] = np.reshape(c, stft_frames_per_window*n_mels)\n",
    "    return output\n",
    "\n",
    "\n",
    "# featurize audio in exactly the same way as TensorflowTTS\n",
    "# this enables mels from the synthesis step to be reused for viseme prediction\n",
    "def kaldi_mels(filepath, \n",
    "               config=None):\n",
    "    audio = load_and_pad_audio(filepath, config[\"sampleRate\"], config[\"paddedAudioLength\"])\n",
    "    audio = torch.unsqueeze(torch.tensor(audio, dtype=torch.float32), dim=0)\n",
    "    \n",
    "    fbank_framelength = 1000 * config[\"fftSize\"] / config[\"sampleRate\"];\n",
    "    fbank_frameshift =fbank_framelength #1000 *config[\"hopSize\"] / config[\"sampleRate\"]\n",
    "    \n",
    "    output = torchaudio.compliance.kaldi.fbank(audio,\n",
    "                                  frame_length=fbank_framelength,\n",
    "                                  frame_shift =fbank_frameshift,\n",
    "                                  num_mel_bins=config[\"numMels\"],\n",
    "                                  high_freq=config[\"fmax\"], \n",
    "                                  low_freq=config[\"fmin\"], \n",
    "                                  sample_frequency=config[\"sampleRate\"],\n",
    "                                  use_energy=False,\n",
    "                                  htk_compat=False,\n",
    "                                  use_log_fbank=True,\n",
    "                                  use_power=False,\n",
    "                                  energy_floor=0.0,\n",
    "                                  window_type='povey'\n",
    "                                 )\n",
    "    num_frames = (config[\"sampleRate\"] * config[\"paddedAudioLength\"]) // config[\"frameLength\"]\n",
    "    output = output.transpose(0,1)\n",
    "    #print(f\"output shape {output.size()} and num_frames {num_frames}, stft frames perindow {config['stftFramesPerWindow']}\")\n",
    "    output = coeffs_to_windows(output, num_frames,  config[\"stftFramesPerWindow\"], config[\"numMels\"])\n",
    "\n",
    "\n",
    "    return torch.tensor(output,dtype=torch.float32).to(device), audio.size()[0], [1] * audio.size()[0]\n",
    "#kaldi_mels(\"tmp/original.wav\", model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07ffb6e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MouthClose', 'MouthFunnel', 'MouthPucker', 'JawOpen', 'EyeBlinkLeft', 'EyeBlinkRight', 'EyeSquintLeft', 'EyeSquintRight', 'BrowDownLeft', 'BrowDownRight', 'MouthUpperUpLeft', 'MouthUpperUpRight', 'MouthLowerDownLeft', 'MouthLowerDownRight', 'MouthRollUpper']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "600"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "import pandas as pd\n",
    "\n",
    "mappings = OrderedDict()\n",
    "mappings[\"MouthClose\"] = \"A37_Mouth_Close\"\n",
    "mappings[\"MouthFunnel\"] = \"A29_Mouth_Funnel\"\n",
    "mappings[\"MouthPucker\"] = \"A30_Mouth_Pucker\"\n",
    "#mappings[\"JawOpen\"] = \"Mouth_Open\"\n",
    "mappings[\"JawOpen\"] = \"Merged_Open_Mouth\"\n",
    "mappings[\"EyeBlinkLeft\"] = \"Eye_Blink_L\"\n",
    "mappings[\"EyeBlinkRight\"] = \"Eye_Blink_R\"\n",
    "mappings[\"EyeSquintLeft\"] = \"Eye_Squint_L\"\n",
    "mappings[\"EyeSquintRight\"] = \"Eye_Squint_R\"\n",
    "mappings[\"BrowDownLeft\"] = \"Brow_Drop_L\"\n",
    "mappings[\"BrowDownRight\"] = \"Brow_Drop_R\"\n",
    "mappings[\"MouthUpperUpLeft\"] = \"Mouth_Snarl_Upper_L\"\n",
    "mappings[\"MouthUpperUpRight\"] = \"Mouth_Snarl_Upper_R\"\n",
    "mappings[\"MouthLowerDownLeft\"] = \"Mouth_Snarl_Lower_L\"\n",
    "mappings[\"MouthLowerDownRight\"] = \"Mouth_Snarl_Lower_R\"\n",
    "mappings[\"MouthRollUpper\"] = \"Mouth_Top_Lip_Under\" # not exact\n",
    "\n",
    "#mappings[\"CheekBlow\"] = \"Cheek_Blow_L\", \"Cheek_Blow_R\"\n",
    "#Nose_Scrunch = Nose_Sneer_L + Nose_Sneer_R\n",
    "print(list(mappings.keys()))\n",
    "# config for the input blendshapes\n",
    "source_config = {\n",
    "    # source framerate for raw viseme label input\n",
    "    \"framerate\":59.97,\n",
    "    # we need to map the blendshapes from the incoming CSV from LiveLinkFace to CC3 blendshapes\n",
    "    # unfortunately not 1-to-1, but seems to work well enough\n",
    "    \"mappings\":mappings\n",
    "}\n",
    " \n",
    "# config for the viseme model\n",
    "model_config = {}\n",
    "# the filename for the trained model that we will export below\n",
    "model_config[\"modelPath\"] = \"bilstm.tflite\"\n",
    "# the blendshapes that will be predicted\n",
    "# we also need to export this so the gltf animator can match the model outputs indices to morph target indices\n",
    "model_config[\"targetNames\"] = list(mappings.values()) #[\"A37_Mouth_Close\", \"A29_Mouth_Funnel\", \"A30_Mouth_Pucker\", \"Mouth_Open\"]\n",
    "\n",
    "# actual framerate to use for viseme labels. \n",
    "# raw labels will be resampled/transformed (either averaged or simply dropped).\n",
    "model_config[\"frameRate\"] = source_config[\"framerate\"] / 3\n",
    "# the sample rate that audio will be resampled to\n",
    "model_config[\"sampleRate\"] = 22050\n",
    "\n",
    "# the duration of a viseme frame is (1 / target_framerate) seconds\n",
    "model_config[\"frameLength\"] = math.ceil(model_config[\"sampleRate\"] * (1 / model_config[\"frameRate\"]))\n",
    "# all audio will be padded to the following size\n",
    "model_config[\"paddedAudioLength\"] = 10\n",
    "# the raw input for each viseme frame will be an audio window of size X \n",
    "# the middle sample of the viseme frame is aligned with the middle sample of the audio window\n",
    "# this means, at the nominal \"anchor sample\" of the viseme frame, there will be \n",
    "# X/2 samples to the left and X/2 samples to the right\n",
    "model_config[\"windowLength\"] = int(1 * model_config[\"sampleRate\"]) \n",
    "\n",
    "# this raw audio input will then be transformed into a number of STFT frames/coefficients\n",
    "# each viseme frame will have this number of STFT frames, which will be the actual input at each timestep\n",
    "# Since audio windows overlap, we won't want to waste cycles repeatedly computing the STFT across the whole audio sequence\n",
    "# So in practice, we pre-calculate the STFT for the whole sequence, then just sub-sample the coefficients at each timestep\n",
    "# when assigning STFT frames, the hop length will then just be half this value\n",
    "model_config[\"stftFramesPerWindow\"] = 20\n",
    "\n",
    "model_config[\"hopSize\"] = int(model_config[\"windowLength\"] // model_config[\"stftFramesPerWindow\"])\n",
    "model_config[\"numMels\"] = 30\n",
    "model_config[\"fmin\"] = 1000\n",
    "model_config[\"fmax\"] = 6000\n",
    "model_config[\"fftSize\"] = 512\n",
    "\n",
    "# load the TTS config so we can match the same parameters \n",
    "# this may override some of the parameters set above\n",
    "USE_TFTTS_CONFIG=False\n",
    "\n",
    "USE_KALDI_FBANK=True\n",
    "\n",
    "\n",
    "if USE_TFTTS_CONFIG:\n",
    "    with open(\"/mnt/hdd_2tb/home/hydroxide/projects/TensorFlowTTS/preprocess/baker_preprocess.yaml\", \"r\") as f:\n",
    "        tftts_config = yaml.safe_load(f)\n",
    "        model_config[\"fftSize\"] = tftts_config[\"fft_size\"]\n",
    "        model_config[\"numMels\"] = tftts_config[\"num_mels\"]       \n",
    "        if \"window_length\" in tftts_config:\n",
    "            assert(tftts_config[\"window_length\"] == model_config[\"windowLength\"])\n",
    "        \n",
    "        model_config[\"window\"] = tftts_config[\"window\"]\n",
    "        model_config[\"fmin\"] = tftts_config[\"fmin\"]\n",
    "        model_config[\"fmax\"] = tftts_config[\"fmax\"]\n",
    "        assert(tftts_config[\"sampling_rate\"] == model_config[\"sampleRate\"])\n",
    "        \n",
    "        #config={\"num_mels\":80, \"sampling_rate\":22050,\"fmin\":80,\"fmax\":6000, \"window\":\"hann\", \"fft_size\":512, \"hop_size\":hop_size})\n",
    "\n",
    "#seq_length = math.ceil((pad_len_in_secs * resample_to) / frame_len)\n",
    "\n",
    "# save the config so we can pass\n",
    "with open(\"output/viseme_model.json\", \"w\",encoding=\"utf-8\") as outfile:\n",
    "    json.dump(model_config, outfile)\n",
    "    \n",
    "process_audio = None\n",
    "\n",
    "if USE_KALDI_FBANK:\n",
    "    process_audio = functools.partial(kaldi_mels, \n",
    "                                          config=model_config)  \n",
    "elif USE_TFTTS_CONFIG:\n",
    "    process_audio = functools.partial(tftts_mels, \n",
    "                                          config=model_config)\n",
    "else:\n",
    "    num_mels=39\n",
    "    process_audio = functools.partial(stft, model_config);\n",
    "        #viseme_frame_len_in_samples=viseme_frame_len_in_samples, # this refers to the size of the viseme/audio window,\n",
    "        #audio_window_in_samples=audio_window_in_samples, # TODO - update these\n",
    "        #stft_frames_per_window=stft_frames_per_window,\n",
    "        #resample_to=resample_to, \n",
    "        #pad_len_in_secs=pad_len_in_secs,\n",
    "        #n_mels=num_mels)\n",
    "        \n",
    "batch_size = 5\n",
    "seq_length = 199\n",
    "text_pad_len=seq_length\n",
    "\n",
    "process_viseme = functools.partial(preprocess_viseme, \n",
    "                                   pad_len_in_secs=model_config[\"paddedAudioLength\"], \n",
    "                                   blendshapes=source_config[\"mappings\"], \n",
    "                                   target_framerate=model_config[\"frameRate\"])\n",
    "\n",
    "training_data = VisemeDataset(\"./data/training/speaker_3/\", \n",
    "                              process_audio, \\\n",
    "                              process_viseme, \n",
    "                              num_ipa_symbols=157,\n",
    "                             text_pad_len=text_pad_len)\n",
    "test_data = VisemeDataset(\"./data/test/speaker_3/\", \n",
    "                              process_audio, \\\n",
    "                              process_viseme,\n",
    "                              num_ipa_symbols=157,\n",
    "                              text_pad_len=text_pad_len\n",
    "                             )\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "input_dim = int(model_config[\"windowLength\"] / model_config[\"hopSize\"] * model_config[\"numMels\"])\n",
    "\n",
    "model_config\n",
    "input_dim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e493a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post-padded shape\n",
      "(220500,)\n",
      "pad_len is 199\n",
      "(199, 63)\n",
      "Post-padded shape\n",
      "(220500,)\n",
      "pad_len is 199\n",
      "(199, 63)\n",
      "Post-padded shape\n",
      "(220500,)\n",
      "pad_len is 199\n",
      "(199, 63)\n",
      "Post-padded shape\n",
      "(220500,)\n",
      "pad_len is 199\n",
      "(199, 63)\n",
      "Post-padded shape\n",
      "(220500,)\n",
      "pad_len is 199\n",
      "(199, 63)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.1718, 0.1318, 0.1247,  ..., 0.1289, 0.1453, 0.0825],\n",
       "        [0.2337, 0.1899, 0.1399,  ..., 0.1447, 0.1613, 0.0995],\n",
       "        [0.1244, 0.2427, 0.1072,  ..., 0.1623, 0.1755, 0.0690],\n",
       "        ...,\n",
       "        [0.1718, 0.1318, 0.1247,  ..., 0.1289, 0.1453, 0.0825],\n",
       "        [0.1718, 0.1318, 0.1247,  ..., 0.1289, 0.1453, 0.0825],\n",
       "        [0.1718, 0.1318, 0.1247,  ..., 0.1289, 0.1453, 0.0825]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SeparableConv1d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, bias=False):\n",
    "        super(SeparableConv1d, self).__init__()\n",
    "        self.depthwise = nn.Conv1d(in_channels, in_channels, kernel_size=kernel_size,\n",
    "                               groups=in_channels, bias=bias, padding=1)\n",
    "        self.pointwise = nn.Conv1d(in_channels, out_channels,\n",
    "                               kernel_size=1, bias=bias)\n",
    "    def forward(self, x):\n",
    "        out = self.depthwise(x)\n",
    "        out = self.pointwise(out)\n",
    "        return out\n",
    "    \n",
    "class Conv1dModel(nn.Module):\n",
    "    def __init__(self, \n",
    "                 seq_length=None, \n",
    "                 input_dim=None, \n",
    "                 text_dim=None, \n",
    "                 text_embedding_dim=512, \n",
    "                 text_padding_idx=None,\n",
    "                 num_visemes=4, \n",
    "                 ks=64):\n",
    "        super(Conv1dModel, self).__init__()\n",
    "        #self.text_padding_idx = text_padding_idx;\n",
    "        #self.text_embedding = nn.Sequential(\n",
    "        #        nn.Embedding(text_dim,text_embedding_dim, padding_idx=self.text_padding_idx),\n",
    "        #        nn.Linear(text_embedding_dim, input_dim),\n",
    "        #        nn.ReLU()\n",
    "        #)\n",
    "                \n",
    "        self.conv1 = nn.Sequential(\n",
    "            SeparableConv1d(seq_length,seq_length,ks),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.conv2 =  nn.Sequential(\n",
    "            SeparableConv1d(seq_length,seq_length,int(ks/2)),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        #self.text_attention = nn.MultiheadAttention(input_dim, 1, batch_first=True)\n",
    "        #self.dropout = torch.nn.Dropout()\n",
    "        #self.attention = nn.MultiheadAttention(1951, 1, batch_first=True)\n",
    "        self.linear_out = nn.Linear(510, num_visemes)\n",
    "        #nn.ModuleList([\n",
    "            \n",
    "        #])\n",
    "        \n",
    "    def forward(self, audio_feats, text_feats):\n",
    "        #audio_feats = torch.transpose(audio_feats,1,2)\n",
    "        #text_out = self.text_embedding(text_feats)        \n",
    "        \n",
    "        #text_out_attn, text_out_attn_weights = self.text_attention(audio_feats,audio_feats,text_out)\n",
    "        audio_out = self.conv1(audio_feats)\n",
    "        \n",
    "        audio_out = self.conv2(audio_out)\n",
    "        \n",
    "        #out = text_out_attn + audio_out\n",
    "        out = audio_out\n",
    "        \n",
    "        #print(out.size())\n",
    "        #out = self.dropout(out)\n",
    "        #return torch.squeeze(torch.stack([v(out) for v in self.linear_out],dim=3))\n",
    "        return self.linear_out(out)\n",
    "\n",
    "#20x199x5040\n",
    "model = Conv1dModel(\n",
    "    seq_length=seq_length,\n",
    "    input_dim=input_dim, \n",
    "    num_visemes=len(model_config[\"targetNames\"]),\n",
    "    text_dim=training_data.num_ipa_symbols,\n",
    "    text_padding_idx=training_data.pad_token).to(device)\n",
    "batch = iter(train_dataloader)\n",
    "audio_features, text_features, train_mask, train_labels, train_files = next(batch, (None,None,None,None,None))\n",
    "model.forward(audio_features.to(device), text_features.to(device))\n",
    "train_labels.size()\n",
    "train_labels[0,:,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab578c3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('data/training/speaker_3/2.csv',\n",
       " 'data/training/speaker_3/33.csv',\n",
       " 'data/training/speaker_3/438.csv',\n",
       " 'data/training/speaker_3/123.csv',\n",
       " 'data/training/speaker_3/315.csv')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c102a5f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post-padded shape\n",
      "(220500,)\n",
      "pad_len is 199\n",
      "(199, 63)\n",
      "Post-padded shape\n",
      "(220500,)\n",
      "pad_len is 199\n",
      "(199, 63)\n",
      "Post-padded shape\n",
      "(220500,)\n",
      "pad_len is 199\n",
      "(199, 63)\n",
      "Post-padded shape\n",
      "(220500,)\n",
      "pad_len is 199\n",
      "(199, 63)\n",
      "Post-padded shape\n",
      "(220500,)\n",
      "pad_len is 199\n",
      "(199, 63)\n",
      "Step 0 Avg loss: 0.00036869656294584273\n",
      "Step 100 Avg loss: 0.0064223116566427055\n",
      "Step 200 Avg loss: 0.00251761358929798\n",
      "Step 300 Avg loss: 0.0015828497451730072\n",
      "Step 400 Avg loss: 0.0012265170610044152\n",
      "Step 500 Avg loss: 0.0010307991382433102\n",
      "Post-padded shape\n",
      "(220500,)\n",
      "pad_len is 199\n",
      "(199, 63)\n",
      "Post-padded shape\n",
      "(220500,)\n",
      "pad_len is 199\n",
      "(199, 63)\n",
      "Post-padded shape\n",
      "(220500,)\n",
      "pad_len is 199\n",
      "(199, 63)\n",
      "Post-padded shape\n",
      "(220500,)\n",
      "pad_len is 199\n",
      "(199, 63)\n",
      "Post-padded shape\n",
      "(220500,)\n",
      "pad_len is 199\n",
      "(199, 63)\n",
      "Post-padded shape\n",
      "(220500,)\n",
      "pad_len is 199\n",
      "(199, 63)\n",
      "Post-padded shape\n",
      "(220500,)\n",
      "pad_len is 199\n",
      "(199, 63)\n",
      "Post-padded shape\n",
      "(220500,)\n",
      "pad_len is 199\n",
      "(199, 63)\n",
      "Post-padded shape\n",
      "(220500,)\n",
      "pad_len is 199\n",
      "(199, 63)\n",
      "Post-padded shape\n",
      "(220500,)\n",
      "pad_len is 199\n",
      "(199, 63)\n",
      "Post-padded shape\n",
      "(220500,)\n",
      "pad_len is 199\n",
      "(199, 63)\n",
      "Test loss 0.014434237033128738\n",
      "Step 600 Avg loss: 0.0006086702004540712\n",
      "Step 700 Avg loss: 0.00034976699185790494\n",
      "Step 800 Avg loss: 0.00023713521906756796\n",
      "Step 900 Avg loss: 0.00013692994383745827\n",
      "Step 1000 Avg loss: 6.59926115986309e-05\n",
      "Test loss 0.015002158936113119\n",
      "Step 1100 Avg loss: 5.461948239826597e-05\n",
      "Step 1200 Avg loss: 4.446430986718042e-05\n",
      "Step 1300 Avg loss: 4.4593978691409576e-05\n",
      "Step 1400 Avg loss: 2.856651940419397e-05\n",
      "Step 1500 Avg loss: 3.215740365703823e-05\n",
      "Test loss 0.01540394127368927\n",
      "Step 1600 Avg loss: 3.556579824817163e-05\n",
      "Step 1700 Avg loss: 2.341296574741136e-05\n",
      "Step 1800 Avg loss: 2.406424368473381e-05\n",
      "Step 1900 Avg loss: 2.5320343956991566e-05\n",
      "Step 2000 Avg loss: 2.3637266858713703e-05\n",
      "Test loss 0.016058190260082483\n",
      "Step 2100 Avg loss: 4.773256320277142e-05\n",
      "Step 2200 Avg loss: 8.655505812384945e-06\n",
      "Step 2300 Avg loss: 1.3842915564055147e-05\n",
      "Step 2400 Avg loss: 2.5158307541914836e-05\n",
      "Step 2500 Avg loss: 1.4576402977581892e-05\n",
      "Test loss 0.014074440114200115\n",
      "Step 2600 Avg loss: 2.2284740148279524e-05\n",
      "Step 2700 Avg loss: 1.069379019554617e-05\n",
      "Step 2800 Avg loss: 3.0348331727054756e-05\n",
      "Step 2900 Avg loss: 1.1379951106391672e-05\n",
      "Step 3000 Avg loss: 1.5119220779524768e-05\n",
      "Test loss 0.015037242323160172\n",
      "Step 3100 Avg loss: 3.556567517989606e-05\n",
      "Step 3200 Avg loss: 8.05057989055058e-06\n",
      "Step 3300 Avg loss: 1.1986830168098095e-05\n",
      "Step 3400 Avg loss: 2.453372105037488e-05\n",
      "Step 3500 Avg loss: 7.038092326183687e-06\n",
      "Test loss 0.013317402685061097\n",
      "Step 3600 Avg loss: 2.36871561571661e-05\n",
      "Step 3700 Avg loss: 8.798760563877295e-06\n",
      "Step 3800 Avg loss: 1.5799795046405053e-05\n",
      "Step 3900 Avg loss: 7.75543126565026e-06\n",
      "Step 4000 Avg loss: 2.6475039034039584e-05\n",
      "Test loss 0.014985479414463043\n",
      "Step 4100 Avg loss: 7.017681127763353e-06\n",
      "Step 4200 Avg loss: 9.915698474287637e-06\n",
      "Step 4300 Avg loss: 1.1654481434106855e-05\n",
      "Step 4400 Avg loss: 1.3848948126451433e-05\n",
      "Step 4500 Avg loss: 8.086257716968248e-06\n",
      "Test loss 0.014561351854354143\n",
      "Step 4600 Avg loss: 1.6566108656661526e-05\n",
      "Step 4700 Avg loss: 7.579460861961706e-06\n",
      "Step 4800 Avg loss: 1.2968408848337276e-05\n",
      "Step 4900 Avg loss: 9.664460660587793e-06\n",
      "Step 5000 Avg loss: 2.3495940026236896e-05\n",
      "Test loss 0.013491383753716946\n",
      "Step 5100 Avg loss: 5.130736212777265e-06\n",
      "Step 5200 Avg loss: 8.103039390334742e-06\n",
      "Step 5300 Avg loss: 7.3688751263034644e-06\n",
      "Step 5400 Avg loss: 2.3707379577899702e-05\n",
      "Step 5500 Avg loss: 5.365044904692695e-06\n",
      "Test loss 0.012577560497447848\n",
      "Step 5600 Avg loss: 7.565372063709219e-06\n",
      "Step 5700 Avg loss: 8.660268319999886e-06\n",
      "Step 5800 Avg loss: 2.6476097757495155e-05\n",
      "Step 5900 Avg loss: 2.7323525875999623e-06\n",
      "Step 6000 Avg loss: 6.552149100116366e-06\n",
      "Test loss 0.014881255105137825\n",
      "Step 6100 Avg loss: 6.333469414130377e-06\n",
      "Step 6200 Avg loss: 7.5382182672001365e-06\n",
      "Step 6300 Avg loss: 8.442802859462971e-06\n",
      "Step 6400 Avg loss: 2.5763783743855128e-05\n",
      "Step 6500 Avg loss: 5.1209610393243564e-06\n",
      "Test loss 0.013710611965507269\n",
      "Step 6600 Avg loss: 7.088189510113807e-06\n",
      "Step 6700 Avg loss: 5.901153583636187e-06\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#model = Conv2dModel(hidden_size=256, input_dim=input_dim, num_visemes=len(model_config[\"targetNames\"])).to(device)\n",
    "\n",
    "\n",
    "learning_rate = 0.0005\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "num_steps = 100000\n",
    "print_loss_every = 100\n",
    "eval_every = 500\n",
    "\n",
    "batch = iter(train_dataloader)\n",
    "accum_loss = 0\n",
    "audio_features, text_features, train_mask, train_labels, train_files = next(batch, (None,None,None,None,None))\n",
    "for t in range(num_steps):\n",
    "    optimizer.zero_grad()\n",
    "#    audio_features, text_features, train_mask, train_labels, train_files = next(batch, (None,None,None,None,None))\n",
    "    \n",
    "#    if audio_features is None:\n",
    "#        batch = iter(train_dataloader)\n",
    "#        audio_features, text_features, train_mask, train_labels, _ = next(batch)\n",
    "            \n",
    "    preds = model(audio_features.to(device),text_features.to(device)) \n",
    "    \n",
    "    #loss = torch.nn.functional.huber_loss(preds, train_labels.to(device) )\n",
    "    loss = torch.nn.functional.mse_loss(preds, train_labels.to(device))\n",
    "    if len(preds.shape) < 3:\n",
    "        preds = torch.unsqueeze(preds, 0)\n",
    "    \n",
    "    #for i in range(preds.shape[1] - 1):\n",
    "    #    cosine_loss = torch.nn.functional.cosine_embedding_loss(\n",
    "    #        preds[:,i,:], \n",
    "    #        preds[:,i+1,:], \n",
    "    #        (torch.ones(preds.shape[0])\n",
    "    #    ).to(device))\n",
    "    #    loss += cosine_loss\n",
    "    accum_loss += loss.item()\n",
    "    if t % print_loss_every == 0:\n",
    "        print(f\"Step {t} Avg loss: {accum_loss / print_loss_every}\")\n",
    "        accum_loss = 0\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "        \n",
    "    if t > 0 and t % eval_every == 0:\n",
    "        accum_loss = 0\n",
    "        for audio_feats, text_feats, test_mask, test_labels, _ in iter(test_dataloader):\n",
    "            y = test_labels.to(device)\n",
    "            preds = model(audio_feats.to(device), text_feats.to(device))\n",
    "            accum_loss += torch.nn.functional.mse_loss(preds, y).item()\n",
    "\n",
    "        print(f\"Test loss {accum_loss}\")\n",
    "        accum_loss = 0\n",
    "    \n",
    "#pred_probab = nn.Softmax(dim=1)(logits)\n",
    "#y_pred = pred_probab.argmax(1)\n",
    "#print(f\"Predicted class: {y_pred}\")199"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17c8fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41510411",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_feats, text_feats, _, _, files = next(iter(test_dataloader))\n",
    "export_y_batch = model(audio_feats.to(device), text_feats.to(device))\n",
    "export_y = export_y_batch[0,:,:]\n",
    "print(files[0])\n",
    "print(export_y)\n",
    "\n",
    "header = \"Timecode,BlendShapeCount,eyeBlinkRight,eyeLookDownRight,eyeLookInRight,eyeLookOutRight,eyeLookUpRight,eyeSquintRight,eyeWideRight,eyeBlinkLeft,eyeLookDownLeft,eyeLookInLeft,eyeLookOutLeft,eyeLookUpLeft,eyeSquintLeft,eyeWideLeft,jawForward,jawRight,jawLeft,jawOpen,mouthClose,mouthFunnel,mouthPucker,mouthRight,mouthLeft,mouthSmileRight,mouthSmileLeft,mouthFrownRight,mouthFrownLeft,mouthDimpleRight,mouthDimpleLeft,mouthStretchRight,mouthStretchLeft,mouthRollLower,mouthRollUpper,mouthShrugLower,mouthShrugUpper,mouthPressRight,mouthPressLeft,mouthLowerDownRight,mouthLowerDownLeft,mouthUpperUpRight,mouthUpperUpLeft,browDownRight,browDownLeft,browInnerUp,browOuterUpRight,browOuterUpLeft,cheekPuff,cheekSquintRight,cheekSquintLeft,noseSneerRight,noseSneerLeft,tongueOut,HeadYaw,HeadPitch,HeadRoll,LeftEyeYaw,LeftEyePitch,LeftEyeRoll,RightEyeYaw,RightEyePitch,RightEyeRoll\".split(',')\n",
    "selected_output_indices = [header.index(x[0].lower() + x[1:]) for x in source_config[\"mappings\"].keys()]\n",
    "num_visemes = len(source_config[\"mappings\"])\n",
    "with open(\"output/prediction.csv\", \"w\") as outfile:\n",
    "    outfile.write(\",\".join(header) + \"\\n\")\n",
    "    timer_ms = 0\n",
    "    for t in range(export_y.shape[1]):\n",
    "        output = [str(0)] * len(header)\n",
    "        second = str(int(timer_ms // 1000)).zfill(2)\n",
    "        frame = (timer_ms % 1000) * model_config[\"frameRate\"] / 1000\n",
    "        output[0] = f\"00:00:{second}:{frame}\"\n",
    "        for viseme in range(num_visemes): \n",
    "            output[selected_output_indices[viseme]] = str(export_y[viseme,t].item())\n",
    "        timer_ms += (1 / model_config[\"frameRate\"]) * 1000\n",
    "        outfile.write(\",\".join(output) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16652a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_header = \"Timecode,BlendShapeCount,EyeBlinkLeft,EyeLookDownLeft,EyeLookInLeft,EyeLookOutLeft,EyeLookUpLeft,EyeSquintLeft,EyeWideLeft,EyeBlinkRight,EyeLookDownRight,EyeLookInRight,EyeLookOutRight,EyeLookUpRight,EyeSquintRight,EyeWideRight,JawForward,JawRight,JawLeft,JawOpen,MouthClose,MouthFunnel,MouthPucker,MouthRight,MouthLeft,MouthSmileLeft,MouthSmileRight,MouthFrownLeft,MouthFrownRight,MouthDimpleLeft,MouthDimpleRight,MouthStretchLeft,MouthStretchRight,MouthRollLower,MouthRollUpper,MouthShrugLower,MouthShrugUpper,MouthPressLeft,MouthPressRight,MouthLowerDownLeft,MouthLowerDownRight,MouthUpperUpLeft,MouthUpperUpRight,BrowDownLeft,BrowDownRight,BrowInnerUp,BrowOuterUpLeft,BrowOuterUpRight,CheekPuff,CheekSquintLeft,CheekSquintRight,NoseSneerLeft,NoseSneerRight,TongueOut,HeadYaw,HeadPitch,HeadRoll,LeftEyeYaw,LeftEyePitch,LeftEyeRoll,RightEyeYaw,RightEyePitch,RightEyeRoll\"\n",
    "remap = {h:(h[0].lower() + h[1:]) if h not in [\"Timecode\",\"BlendShapeCount\",\"HeadYaw\",\"HeadPitch\",\"HeadRoll\",\"LeftEyeYaw\",\"LeftEyePitch\",\"LeftEyeRoll\",\"RightEyeYaw\",\"RightEyePitch\",\"RightEyeRoll\"]  else h for h in new_header.split(\",\") }\n",
    "for oh in remap.values():\n",
    "    if oh not in header:\n",
    "        print(oh)\n",
    "\n",
    "def new_to_old(csv_df):\n",
    "    return csv_df.rename(columns=remap)\n",
    "df = preprocess_viseme(\"data/training/speaker_1/20210824_1/61.csv\", pad_len_in_secs=model_config[\"paddedAudioLength\"], \n",
    "                                   target_framerate=model_config[\"frameRate\"])\n",
    "df = new_to_old(df)\n",
    "#cols = [c for c in df.columns if c not in [\"jawOpen\", \"mouthClose\", \"Timecode\"]]\n",
    "#df[cols] = 0\n",
    "df = df[df[\"Timecode\"] != 0]\n",
    "df.to_csv(\"original.csv\", index=False)\n",
    "#new_to_old(df)[header].to_csv(\"original.csv\", index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666b8680",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = preprocess_viseme(\"data/training/speaker_1/20210824_1/61.csv\", pad_len_in_secs=model_config[\"paddedAudioLength\"], \n",
    "                                   target_framerate=\n",
    "                       model_config[\"frameRate\"])\n",
    "print(list(source_config[\"mappings\"].keys()))\n",
    "print(list(source_config[\"mappings\"].values()))\n",
    "df = df[list(source_config[\"mappings\"].keys())]\n",
    "df.columns = list(source_config[\"mappings\"].values())\n",
    "df = df.iloc[::int(59.97 / 29.98)]\n",
    "\n",
    "\n",
    "df.to_csv(\"original.csv\", index=False)\n",
    "#model_config\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05231fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[::int(59.97 / 29.98)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00801822",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"output/conv2d.torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70887b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(\"output/conv2d.torch\",map_location=torch.device('cpu'))\n",
    "import torch.onnx \n",
    "\n",
    "# set the model to inference mode \n",
    "model.eval() \n",
    "\n",
    "# Let's create a dummy input tensor  \n",
    "dummy_input = torch.randn(1, input_dim, 119,  requires_grad=False)  \n",
    "\n",
    "# Export the model   \n",
    "torch.onnx.export(model,         # model being run \n",
    "     dummy_input,       # model input (or a tuple for multiple inputs) \n",
    "     \"output/conv2d.onnx\",       # where to save the model  \n",
    "     export_params=True,  # store the trained parameter weights inside the model file \n",
    "     opset_version=10,    # the ONNX version to export the model to \n",
    "     do_constant_folding=True,  # whether to execute constant folding for optimization \n",
    "     #input_names = ['modelInput'],   # the model's input names \n",
    "     #output_names = ['modelOutput'], # the model's output names \n",
    "     dynamic_axes={'modelInput' : {0 : 'batch_size'},    # variable length axes \n",
    "    'modelOutput' : {0 : 'batch_size'}}\n",
    "                 ) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47648b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "\n",
    "import onnxruntime\n",
    "model_onnx = onnx.load('output/conv2d.onnx')\n",
    "\n",
    "\n",
    "from onnx_tf.backend import prepare\n",
    "tf_rep = prepare(model_onnx)\n",
    "tf_rep.export_graph('./output/tf_model')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df4666c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_model_optimization as tfmot\n",
    "\n",
    "quant_aware_model = tfmot.quantization.keras.quantize_model(tf_rep)\n",
    "quant_aware_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953bfeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(\"./output/tf_model\")\n",
    "print(\"Built converter\")\n",
    "\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,\n",
    "                                       tf.lite.OpsSet.SELECT_TF_OPS]\n",
    "converter.allow_custom_ops=False\n",
    "#converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "#converter.experimental_enable_resource_variables = True\n",
    "\n",
    "converter.experimental_new_converter =True\n",
    "tflite_model = converter.convert()\n",
    "print(\"Converted\")\n",
    "\n",
    "# Save the model\n",
    "with open(\"./output/conv2d.tflite\", 'wb') as f:\n",
    "    f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3efc8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter = tf.lite.Interpreter(model_path=\"bilstm.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "    \n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "input_shape = input_details[0]['shape']\n",
    "output_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f1f546",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv = pd.read_csv(\"data/training/speaker_1/20210824_1/61.csv\")\n",
    "columns = [x for x in list(csv.columns) if \"Eye\" not in x]\n",
    "columns.remove(\"Timecode\")\n",
    "columns.remove(\"BlendShapeCount\")\n",
    "csv[columns].var().sort_values()\n",
    "#df = preprocess_viseme(\"data/training/speaker_1/20210824_1/61.csv\", pad_len_in_secs=pad_len_in_secs, \n",
    "#                                   resample_to=target_framerate, blendshapes=[\"MouthClose\",\"MouthFunnel\"])\n",
    "#df.shape\n",
    "#[df.iloc[0][\"EyeLookInLeft\"]]\n",
    "    #csv[columns] = pd.np.digitize(csv[columns], np.linspace(0,1,11))\n",
    "    \n",
    "    #split = csv[\"Timecode\"].str.split(':')\n",
    "    #minute = split.str[1].astype(int)\n",
    "    #second = split.str[2].astype(int)\n",
    "    #frame = split.str[3].astype(float)\n",
    "    #minute -= minute[0]\n",
    "    #ms\n",
    "    #step = minute * 60 + second\n",
    "    #csv[\"step\"] = step\n",
    "    #return csv.drop_duplicates([\"step\"])[[\"step\", \"MouthClose\",\"MouthFunnel\",\"MouthPucker\",\"JawOpen\"]]\n",
    "    \n",
    "# if we want to use softmax across each blendshape as a one-hot\n",
    "    #return np.reshape(vals, (vals.shape[0], vals.shape[1], 1))\n",
    "    #one_hot = np.zeros((vals.shape[0], vals.shape[1], 11, 1))\n",
    "    #oh = np.eye(11)\n",
    "    #for row in range(vals.shape[0]):\n",
    "    #    for t in range(vals.shape[1]):\n",
    "    #        one_hot[row, :, :, 0] = np.eye(11)[int(vals[row,t])-1]\n",
    "    #return one_hot"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
