{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fae648b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: soundfile in /home/hydroxide/.local/lib/python3.7/site-packages (0.10.3.post1)\n",
      "Requirement already satisfied: cffi>=1.0 in /home/hydroxide/.local/lib/python3.7/site-packages (from soundfile) (1.14.6)\n",
      "Requirement already satisfied: pycparser in /home/hydroxide/.local/lib/python3.7/site-packages (from cffi>=1.0->soundfile) (2.20)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in /home/hydroxide/.local/lib/python3.7/site-packages (1.3.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/hydroxide/.local/lib/python3.7/site-packages (from pandas) (2021.1)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /home/hydroxide/.local/lib/python3.7/site-packages (from pandas) (1.19.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/hydroxide/.local/lib/python3.7/site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/hydroxide/.local/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: scipy in /home/hydroxide/.local/lib/python3.7/site-packages (1.7.1)\n",
      "Requirement already satisfied: numpy<1.23.0,>=1.16.5 in /home/hydroxide/.local/lib/python3.7/site-packages (from scipy) (1.19.5)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: librosa in /home/hydroxide/.local/lib/python3.7/site-packages (0.8.1)\n",
      "Requirement already satisfied: decorator>=3.0.0 in /home/hydroxide/.local/lib/python3.7/site-packages (from librosa) (5.0.9)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/hydroxide/.local/lib/python3.7/site-packages (from librosa) (1.19.5)\n",
      "Requirement already satisfied: soundfile>=0.10.2 in /home/hydroxide/.local/lib/python3.7/site-packages (from librosa) (0.10.3.post1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/hydroxide/.local/lib/python3.7/site-packages (from librosa) (21.0)\n",
      "Requirement already satisfied: pooch>=1.0 in /home/hydroxide/.local/lib/python3.7/site-packages (from librosa) (1.4.0)\n",
      "Requirement already satisfied: scipy>=1.0.0 in /home/hydroxide/.local/lib/python3.7/site-packages (from librosa) (1.7.1)\n",
      "Requirement already satisfied: audioread>=2.0.0 in /home/hydroxide/.local/lib/python3.7/site-packages (from librosa) (2.1.9)\n",
      "Requirement already satisfied: resampy>=0.2.2 in /home/hydroxide/.local/lib/python3.7/site-packages (from librosa) (0.2.2)\n",
      "Requirement already satisfied: joblib>=0.14 in /home/hydroxide/.local/lib/python3.7/site-packages (from librosa) (1.0.1)\n",
      "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /home/hydroxide/.local/lib/python3.7/site-packages (from librosa) (0.24.2)\n",
      "Requirement already satisfied: numba>=0.43.0 in /home/hydroxide/.local/lib/python3.7/site-packages (from librosa) (0.48.0)\n",
      "Requirement already satisfied: setuptools in /home/hydroxide/.local/lib/python3.7/site-packages (from numba>=0.43.0->librosa) (58.0.4)\n",
      "Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /home/hydroxide/.local/lib/python3.7/site-packages (from numba>=0.43.0->librosa) (0.31.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/hydroxide/.local/lib/python3.7/site-packages (from packaging>=20.0->librosa) (2.4.7)\n",
      "Requirement already satisfied: appdirs in /home/hydroxide/.local/lib/python3.7/site-packages (from pooch>=1.0->librosa) (1.4.4)\n",
      "Requirement already satisfied: requests in /home/hydroxide/.local/lib/python3.7/site-packages (from pooch>=1.0->librosa) (2.25.1)\n",
      "Requirement already satisfied: six>=1.3 in /home/hydroxide/.local/lib/python3.7/site-packages (from resampy>=0.2.2->librosa) (1.15.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/hydroxide/.local/lib/python3.7/site-packages (from scikit-learn!=0.19.0,>=0.14.0->librosa) (2.2.0)\n",
      "Requirement already satisfied: cffi>=1.0 in /home/hydroxide/.local/lib/python3.7/site-packages (from soundfile>=0.10.2->librosa) (1.14.6)\n",
      "Requirement already satisfied: pycparser in /home/hydroxide/.local/lib/python3.7/site-packages (from cffi>=1.0->soundfile>=0.10.2->librosa) (2.20)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/hydroxide/.local/lib/python3.7/site-packages (from requests->pooch>=1.0->librosa) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/hydroxide/.local/lib/python3.7/site-packages (from requests->pooch>=1.0->librosa) (1.26.6)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/hydroxide/.local/lib/python3.7/site-packages (from requests->pooch>=1.0->librosa) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/hydroxide/.local/lib/python3.7/site-packages (from requests->pooch>=1.0->librosa) (2021.5.30)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1.9.0+cu102'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install soundfile\n",
    "!pip install pandas\n",
    "!pip install scipy\n",
    "!pip install librosa\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import scipy\n",
    "from scipy import fft\n",
    "import functools\n",
    "from torch import nn\n",
    "import librosa\n",
    "import math\n",
    "import torchaudio \n",
    "import math\n",
    "import yaml\n",
    "torch.__version__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3bc2295b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dir should be structured as follows:\n",
    "# - speaker_id_1/\n",
    "# - speaker_id_1/sample_id1.wav\n",
    "# - speaker_id_1/sample_id1.csv\n",
    "# - speaker_id_1/sample_id2.wav\n",
    "# - speaker_id_1/sample_id2.wav\n",
    "# - speaker_id_2/sample_id1.wav\n",
    "# ..\n",
    "class VisemeDataset(Dataset):\n",
    "    def __init__(self, data_dir, audio_transform, viseme_transform):\n",
    "        self.viseme_transform= viseme_transform\n",
    "        self.audio_transform = audio_transform\n",
    "        self.audio_files = []\n",
    "        self.visemes = []\n",
    "        self.processed = {}\n",
    "        for file in list(Path(data_dir).rglob(\"*.wav\")):\n",
    "            self.audio_files.append(file)\n",
    "            self.visemes.append(str(file).replace(\"wav\", \"csv\"))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx not in self.processed:\n",
    "            fft, num_samples, mask = self.audio_transform(self.audio_files[idx])\n",
    "            fft = torch.tensor(fft, dtype=torch.float)\n",
    "            viseme_filename = self.visemes[idx]\n",
    "            visemes = torch.tensor(self.viseme_transform(viseme_filename).values.astype(np.float32))       \n",
    "            #assert(visemes.shape[0] == fft.shape[0])\n",
    "            self.processed[idx] =fft, torch.tensor(mask), visemes, viseme_filename\n",
    "        return self.processed[idx]\n",
    "\n",
    "class SeparableConv1d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, bias=False):\n",
    "        super(SeparableConv1d, self).__init__()\n",
    "        self.depthwise = nn.Conv1d(in_channels, in_channels, kernel_size=kernel_size, \n",
    "                               groups=in_channels, bias=bias, padding=1)\n",
    "        self.pointwise = nn.Conv1d(in_channels, out_channels, \n",
    "                               kernel_size=1, bias=bias)\n",
    "    def forward(self, x):\n",
    "        out = self.depthwise(x)\n",
    "        out = self.pointwise(out)\n",
    "        return out\n",
    "    \n",
    "class Conv1dModel(nn.Module):\n",
    "    def __init__(self, seq_length=633, n_ffts=2457, num_viseme=4, ks=256):\n",
    "        super(Conv1dModel, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            SeparableConv1d(seq_length,seq_length,ks),\n",
    "            nn.ReLU(),\n",
    "            SeparableConv1d(seq_length,seq_length,ks),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.attention = nn.MultiheadAttention(1951, 1, batch_first=True)\n",
    "        #self.linear_relu_stack2 = nn.Sequential(\n",
    "        #    SeparableConv1d(seq_length,seq_length,ks),\n",
    "        #    #nn.Linear(seq_length, seq_length),\n",
    "        #    nn.ReLU(),7\n",
    "        #)\n",
    "        self.linear_out = nn.Linear(1951, 11)\n",
    "\n",
    "    def forward(self, x):\n",
    "        o1 = self.conv(x)\n",
    "        attn_output, attn_output_weights = self.attention(o1, o1, o1)\n",
    "        attn_output = attn_output.tile((4,1,1,1)).transpose(0,1)\n",
    "        o1 = self.linear_out(attn_output)        \n",
    "        return o1\n",
    "\n",
    "class BiLSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim=None, hidden_size=512, num_viseme=4):\n",
    "        super(BiLSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = torch.nn.LSTM(input_dim, hidden_size, 1, bidirectional=True)\n",
    "        self.attention = nn.MultiheadAttention(hidden_size*2, 1, batch_first=True)       \n",
    "        self.l1 = torch.nn.Linear(hidden_size*2, 1)\n",
    "        self.l2 = torch.nn.Linear(hidden_size*2, 1)\n",
    "        self.l3 = torch.nn.Linear(hidden_size*2, 1)\n",
    "        self.l4 = torch.nn.Linear(hidden_size*2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out_f, _ = self.lstm(x)\n",
    "        #out_f = out_f[:,:,:self.embed_dim] + out_f[:,:,self.embed_dim:]\n",
    "        attn_output, attn_output_weights = self.attention(out_f, out_f, out_f)\n",
    "        return torch.stack([\n",
    "            self.l1(attn_output),\n",
    "            self.l2(attn_output),\n",
    "            self.l3(attn_output),\n",
    "            self.l4(attn_output)\n",
    "        ],dim=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f28fb2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_viseme(csv, pad_len_in_secs=None, target_framerate=None, blendshapes=None):\n",
    "    csv = pd.read_csv(csv)\n",
    "    \n",
    "    # first, drop every nth row to reduce effective framerate    \n",
    "    csv = csv.iloc[::int(59.97 / target_framerate)]\n",
    "    \n",
    "    pad_len = int(pad_len_in_secs * target_framerate)\n",
    "        \n",
    "    if(csv.shape[0] < pad_len):\n",
    "        pad = pd.DataFrame(0, index=[i for i in range(pad_len - csv.shape[0])], columns=csv.columns)\n",
    "        pad.pad(inplace=True)\n",
    "        csv = pd.concat([csv, pad])\n",
    "    else:\n",
    "        csv = csv.iloc[:pad_len]\n",
    "        #print(\"Visemes exceeded max length, truncate?\")\n",
    "    columns = list(csv.columns)\n",
    "    columns.remove(\"Timecode\")\n",
    "    \n",
    "    return csv[blendshapes] if blendshapes is not None else csv\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f0403d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_audio(audio, sample_rate, pad_len_in_secs):\n",
    "    # left-pad the audio so we have the left context when starting at the initial viseme\n",
    "    pad_len_in_samples = pad_len_in_secs * resample_to \n",
    "    if len(audio.shape) > 1:\n",
    "        audio = audio[0]\n",
    "    if audio.shape[0] < pad_len_in_samples:\n",
    "        audio = np.pad(audio, (0, (pad_len_in_secs * resample_to) - audio.shape[0]), constant_values=0.001)\n",
    "    elif audio.shape[0] > pad_len_in_samples:\n",
    "        audio = audio[:pad_len_in_samples]\n",
    "    #audio = np.hstack([np.zeros((win_length*2,)), audio])\n",
    "    return audio\n",
    "\n",
    "def load_and_pad_audio(filepath, resample_to, pad_len_in_secs):\n",
    "    audio, rate = sf.read(filepath)\n",
    "    audio = librosa.resample(audio, rate, resample_to)\n",
    "    audio = pad_audio(audio, resample_to, pad_len_in_secs)\n",
    "    return audio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30503fdb",
   "metadata": {},
   "source": [
    "\\---S1---/\\---S2---/\\---S3---/\n",
    "WWWWWWWWWWWWWWWWWWWWWWWWWWWWWW\n",
    "____________\\---V1---/\n",
    "_____________AAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\n",
    "\n",
    "             \\---S1---/\\---S2---/\\---S3---/\n",
    "             WWWWWWWWWWWWWWWWWWWWWWWWWWWWWW\n",
    "______________________\\---V2---/\n",
    "_____________AAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\n",
    "\n",
    "\n",
    "A = audio sample\n",
    "V1, V2, etc = viseme 1, viseme 2, etc (aka \"frame\", but not in the MFCC sense)\n",
    "W = a window of audio samples that will be used as input (aka \"frame\" in the MFCC sense. to avoid confusion, we refer to this as the \"window\" and the viseme as the \"frame\")\n",
    "S1, S2, S3 = STFT of a window \n",
    "num_frames == num_windows\n",
    "\n",
    "audio_bins_per_window = the number of S per W\n",
    "samples_per_bin = the length of each S (i.e. number of As)\n",
    "bin_hop_length = the number of A between S1 and S2 (in the picture above, hop_length == len(S1) == len(S2) == samples_per_bin\n",
    "\n",
    "in practice, we calculate as such:\n",
    "          /-----------V2--------------\\\n",
    "/--------------V1------------\\\n",
    "\\---S1---/\\---S2---/\\---S3---/\\---S4---/\\---S5---/\\---S6---/\n",
    "_______________AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\n",
    "\n",
    "then at V1, we take {S1,S2,S3}, at V2 we take {S4,S5,S6}, etc\n",
    "\n",
    "# STFT hop length should equal to length of a single viseme frame, in samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ee6af27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stft(filepath, \n",
    "         frame_len, \n",
    "         window_len, \n",
    "         stft_frames_per_window,\n",
    "         resample_to, \n",
    "         pad_len_in_secs, \n",
    "         n_mels):\n",
    "    \n",
    "    audio = load_and_pad_audio(filepath, pad_len_in_secs, resample_to)\n",
    "    #wdw = np.hanning(audio_window_in_samples)\n",
    "    \n",
    "    num_frames = audio.shape[0] // frame_len\n",
    "    # TODO - mask for padding \n",
    "    # [1] * actual_seq_length + [0] * (padded_seq_length - actual_seq_length)\n",
    "    \n",
    "    # take the STFT of the entire audio file \n",
    "    # with a window size equivalent to the audio_window_in_samples / audio_bins_per_window\n",
    "    n_fft = int(window_len / stft_frames_per_window)\n",
    "\n",
    "    transformed = librosa.stft(audio, \n",
    "                               n_fft=n_fft,\n",
    "                               win_length=n_fft,\n",
    "                               hop_length=n_fft)\n",
    "    \n",
    "    melfb = librosa.filters.mel(resample_to, n_fft, n_mels=n_mels)    \n",
    "    mels = np.dot(melfb, np.abs(transformed))\n",
    "    \n",
    "    log_mels = np.log(mels)\n",
    "    \n",
    "    output = mels_to_mfccs(log_mels, padded_seq_length, stft_frames_per_window, n_mels)\n",
    "\n",
    "    return output, audio.shape[0], \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c0de78e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let num_stft_frames be the number of STFT frames per audio sample \n",
    "# num_stft_frames == (audio_len / n_fft)\n",
    "# (assuming FFT window length == hop length == n_ffts)\n",
    "# coeffs will be B x num_stft_frames x num_mels\n",
    "# every \n",
    "# if we've calculated the FFT size correctly, the hop length will be half \n",
    "# so stft_frames_per_window should be odd\n",
    "\n",
    "def coeffs_to_windows(coeffs, num_frames, stft_frames_per_window, n_mels):\n",
    "    \n",
    "    output = np.zeros((num_frames, stft_frames_per_window*n_mels))\n",
    "    \n",
    "    # the number of frames to hop is just half the number of STFT frames per window\n",
    "    hop_len = int(stft_frames_per_window / 2)\n",
    "    \n",
    "    for i in range(num_frames):\n",
    "        c = coeffs[:, (i*hop_len):(i*hop_len)+stft_frames_per_window]\n",
    "        # padding\n",
    "        if(c.shape[1] < stft_frames_per_window):\n",
    "            c = np.pad(c, [(0,0), (0, stft_frames_per_window - c.shape[1])], constant_values=0)\n",
    "        \n",
    "        output[i, :] = np.reshape(c, stft_frames_per_window*n_mels)\n",
    "    return output\n",
    "\n",
    "\n",
    "# featurize audio in exactly the same way as TensorflowTTS\n",
    "# this enables mels from the synthesis step to be reused for viseme prediction\n",
    "\n",
    "def tftts_mels(filepath, \n",
    "               frame_len=None,\n",
    "               window_len=None,\n",
    "               stft_frames_per_window=None,\n",
    "               resample_to=None, \n",
    "               config=None):\n",
    "    assert(resample_to==config[\"sampling_rate\"])\n",
    "    \n",
    "    audio = load_and_pad_audio(filepath, resample_to, pad_len_in_secs)\n",
    "     \n",
    "    # this is (mostly) copied verbatim from https://github.com/TensorSpeech/TensorFlowTTS/blob/master/tensorflow_tts/bin/preprocess.py @ 4a7d584 \n",
    "    # except the hop length must be the size of the window (in samples) divided by the number of STFT frames per window\n",
    "    if config[\"hop_size\"] != window_len // stft_frames_per_window:\n",
    "        print(config[\"hop_size\"])\n",
    "        assert(config[\"hop_size\"] == window_len // stft_frames_per_window)\n",
    "    D = librosa.stft(\n",
    "        audio,\n",
    "        n_fft=tftts_config[\"fft_size\"],\n",
    "        hop_length=config[\"hop_size\"],\n",
    "        win_length=config[\"win_length\"] if \"win_length\" in config else config[\"fft_size\"],\n",
    "        window=config[\"window\"],\n",
    "        pad_mode=\"reflect\",\n",
    "    )\n",
    "    \n",
    "    S, _ = librosa.magphase(D)  # (#bins, #frames)\n",
    "    fmin = 0 if config[\"fmin\"] is None else config[\"fmin\"]\n",
    "    fmax = sampling_rate // 2 if config[\"fmax\"] is None else config[\"fmax\"]\n",
    "    mel_basis = librosa.filters.mel(\n",
    "        sr=config[\"sampling_rate\"],\n",
    "        n_fft=config[\"fft_size\"],\n",
    "        n_mels=config[\"num_mels\"],\n",
    "        fmin=fmin,\n",
    "        fmax=fmax,\n",
    "    )\n",
    "    log_mels = np.log10(np.maximum(np.dot(mel_basis, S), 1e-10)).T  # (#frames, #bins)\n",
    "    #print(log_mels.shape)\n",
    "    # coeffs = fft.dct(log_mels.T) <-- where did this come from?\n",
    "    coeffs = log_mels.T\n",
    "    #print(coeffs.shape)\n",
    "    num_frames = (resample_to * pad_len_in_secs) // frame_len\n",
    "\n",
    "    output = coeffs_to_windows(coeffs, num_frames,  stft_frames_per_window, config[\"num_mels\"])\n",
    "\n",
    "    return output, audio.shape[0], [1] * log_mels.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e8677925",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "334"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the blendshapes that will used as input labels (and predictions)\n",
    "blendshapes = [\"MouthClose\", \"MouthFunnel\", \"MouthPucker\", \"JawOpen\"]\n",
    "\n",
    "# source framerate for raw viseme label input\n",
    "framerate=59.97 \n",
    "\n",
    "# actual framerate to use for viseme labels. \n",
    "# raw labels will be resampled/transformed (either averaged or simply dropped).\n",
    "target_framerate = framerate / 2 \n",
    "\n",
    "# the sample rate that audio will be resampled to\n",
    "resample_to = 22050\n",
    "\n",
    "# the duration of a viseme frame is (1 / target_framerate) seconds\n",
    "frame_len = math.ceil(resample_to * (1 / target_framerate))\n",
    "\n",
    "# all audio will be padded to the following size\n",
    "pad_len_in_secs = 10\n",
    "\n",
    "# the raw input for each viseme frame will be an audio window of size X \n",
    "# the middle sample of the viseme frame is aligned with the middle sample of the audio window\n",
    "# this means, at the nominal \"anchor sample\" of the viseme frame, there will be \n",
    "# X/2 samples to the left and X/2 samples to the right\n",
    "# let's use 0.5 seconds\n",
    "window_len = (0.5 * resample_to) \n",
    "\n",
    "# this raw audio input will then be transformed into a number of STFT frames/coefficients\n",
    "# each viseme frame will have this number of STFT frames, which will be the actual input at each timestep\n",
    "# Since audio windows overlap, we won't want to waste cycles repeatedly computing the STFT across the whole audio sequence\n",
    "# So in practice, we pre-calculate the STFT for the whole sequence, then just sub-sample the coefficients at each timestep\n",
    "# when assigning STFT frames, the hop length will then just be half this value\n",
    "stft_frames_per_window = 33\n",
    "\n",
    "seq_length = math.ceil((pad_len_in_secs * resample_to) / frame_len)\n",
    "\n",
    "hop_size = int(window_len // stft_frames_per_window)\n",
    "hop_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d5316a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sampling_rate': 22050,\n",
       " 'fft_size': 512,\n",
       " 'hop_size': 334,\n",
       " 'window': 'hann',\n",
       " 'num_mels': 80,\n",
       " 'fmin': 80,\n",
       " 'fmax': 7600,\n",
       " 'global_gain_scale': 1.0,\n",
       " 'trim_silence': True,\n",
       " 'trim_threshold_in_db': 60,\n",
       " 'trim_frame_size': 2048,\n",
       " 'trim_hop_size': 512,\n",
       " 'format': 'npy'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow_tts.inference import AutoConfig\n",
    "batch_size = 20\n",
    "\n",
    "process_audio = None\n",
    "\n",
    "USE_TFTTS_CONFIG=True\n",
    "\n",
    "#config={\"num_mels\":80, \"sampling_rate\":22050,\"fmin\":80,\"fmax\":6000, \"window\":\"hann\", \"fft_size\":512, \"hop_size\":hop_size})\n",
    "    \n",
    "if USE_TFTTS_CONFIG:\n",
    "    with open(\"/mnt/hdd_2tb/home/hydroxide/projects/TensorFlowTTS/preprocess/baker_preprocess.yaml\", \"r\") as f:\n",
    "        tftts_config = yaml.safe_load(f)\n",
    "        tftts_config[\"hop_size\"] = hop_size\n",
    "        process_audio = functools.partial(tftts_mels, \n",
    "                                          frame_len=frame_len, \n",
    "                                          window_len=window_len,\n",
    "                                          stft_frames_per_window=stft_frames_per_window, \n",
    "                                          resample_to=resample_to, \n",
    "                                          config=tftts_config)\n",
    "else:\n",
    "    num_mels=39\n",
    "    process_audio = functools.partial(stft, \n",
    "                                        viseme_frame_len_in_samples=viseme_frame_len_in_samples, # this refers to the size of the viseme/audio window,\n",
    "        audio_window_in_samples=audio_window_in_samples, # TODO - update these\n",
    "        stft_frames_per_window=stft_frames_per_window,\n",
    "        resample_to=resample_to, \n",
    "        pad_len_in_secs=pad_len_in_secs,\n",
    "        n_mels=num_mels)\n",
    "    \n",
    "process_viseme = functools.partial(preprocess_viseme, \n",
    "                                   pad_len_in_secs=pad_len_in_secs, \n",
    "                                   blendshapes=blendshapes, \n",
    "                                   target_framerate=target_framerate)\n",
    "\n",
    "training_data = VisemeDataset(\"./data/training/speaker_1/\", \n",
    "                              process_audio, \\\n",
    "                              process_viseme)\n",
    "test_data = VisemeDataset(\"./data/test/speaker_1/\", \n",
    "                              process_audio, \\\n",
    "                              process_viseme)\n",
    "def collate_samples(feat_tuples):\n",
    "    return feat_tuples\n",
    "    padded = torch.nn.utils.rnn.pad_sequence([f[0] for f in feat_tuples], batch_first=True, padding_value=0.0)\n",
    "    #mask = torch.stack([feat_tuples[i][1] for i in range(len(feat_tuples))])\n",
    "    labels = torch.nn.utils.rnn.pad_sequence([f[2] for f in feat_tuples], batch_first=True, padding_value=0.0)\n",
    "    viseme_filenames = [feat_tuples[i][3] for i in range(len(feat_tuples))]\n",
    "    \n",
    "    return padded, labels,viseme_filenames\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Using {} device'.format(device))\n",
    "\n",
    "tftts_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c102a5f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2640\n",
      "Step 0 Avg loss: 0.00014744590036571025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hydroxide/.local/lib/python3.7/site-packages/ipykernel_launcher.py:39: UserWarning: Using a target size (torch.Size([1, 299, 4])) that is different to the input size (torch.Size([299, 4])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 50 Avg loss: 0.01248262720182538\n",
      "Step 100 Avg loss: 0.006710680797696113\n",
      "Step 150 Avg loss: 0.0066580833867192265\n",
      "Step 200 Avg loss: 0.00656532071530819\n",
      "Step 250 Avg loss: 0.0064372603921219705\n",
      "Step 300 Avg loss: 0.00647843457525596\n",
      "Step 350 Avg loss: 0.006739921793341637\n",
      "Step 400 Avg loss: 0.006546479612588882\n",
      "Step 450 Avg loss: 0.0062484141043387354\n",
      "Step 500 Avg loss: 0.006679508625529707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hydroxide/.local/lib/python3.7/site-packages/ipykernel_launcher.py:60: UserWarning: Using a target size (torch.Size([20, 299, 4])) that is different to the input size (torch.Size([20, 1, 299, 4])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "/home/hydroxide/.local/lib/python3.7/site-packages/ipykernel_launcher.py:60: UserWarning: Using a target size (torch.Size([2, 299, 4])) that is different to the input size (torch.Size([2, 1, 299, 4])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss 0.025016657076776028\n",
      "Step 550 Avg loss: 0.00644154991954565\n",
      "Step 600 Avg loss: 0.006579419514164329\n",
      "Step 650 Avg loss: 0.006532560531049967\n",
      "Step 700 Avg loss: 0.006511105103418231\n",
      "Step 750 Avg loss: 0.0065396463684737685\n",
      "Step 800 Avg loss: 0.006689157695509493\n",
      "Step 850 Avg loss: 0.006584917176514864\n",
      "Step 900 Avg loss: 0.006538921017199755\n",
      "Step 950 Avg loss: 0.006255963309668005\n",
      "Step 1000 Avg loss: 0.006411707543302328\n",
      "Test loss 0.02570384554564953\n",
      "Step 1050 Avg loss: 0.00629792955936864\n",
      "Step 1100 Avg loss: 0.0063398181553930046\n",
      "Step 1150 Avg loss: 0.006382776042446494\n",
      "Step 1200 Avg loss: 0.006392291933298111\n",
      "Step 1250 Avg loss: 0.006244062534533441\n",
      "Step 1300 Avg loss: 0.006384277283214033\n",
      "Step 1350 Avg loss: 0.006148095363751054\n",
      "Step 1400 Avg loss: 0.00653546198271215\n",
      "Step 1450 Avg loss: 0.006418077489361167\n",
      "Step 1500 Avg loss: 0.006615049056708813\n",
      "Test loss 0.02088141767308116\n",
      "Step 1550 Avg loss: 0.006351466856431216\n",
      "Step 1600 Avg loss: 0.006736885532736778\n",
      "Step 1650 Avg loss: 0.006454673204571009\n",
      "Step 1700 Avg loss: 0.006475606565363706\n",
      "Step 1750 Avg loss: 0.006567239454016089\n",
      "Step 1800 Avg loss: 0.00639528919942677\n",
      "Step 1850 Avg loss: 0.006571063473820686\n",
      "Step 1900 Avg loss: 0.006475424049422145\n",
      "Step 1950 Avg loss: 0.006479498688131571\n",
      "Step 2000 Avg loss: 0.006333185127004981\n",
      "Test loss 0.02425391972064972\n",
      "Step 2050 Avg loss: 0.006424771090969443\n",
      "Step 2100 Avg loss: 0.006217051807325334\n",
      "Step 2150 Avg loss: 0.006404145730193704\n",
      "Step 2200 Avg loss: 0.006322169611230492\n",
      "Step 2250 Avg loss: 0.006385554475709796\n",
      "Step 2300 Avg loss: 0.006101926607079804\n",
      "Step 2350 Avg loss: 0.006542795868590474\n",
      "Step 2400 Avg loss: 0.006448888801969588\n",
      "Step 2450 Avg loss: 0.006324229771271348\n",
      "Step 2500 Avg loss: 0.006184610244818031\n",
      "Test loss 0.016975648468360305\n",
      "Step 2550 Avg loss: 0.0063056213408708576\n",
      "Step 2600 Avg loss: 0.006265837652608752\n",
      "Step 2650 Avg loss: 0.006428661197423935\n",
      "Step 2700 Avg loss: 0.006342948926612735\n",
      "Step 2750 Avg loss: 0.00620441272854805\n",
      "Step 2800 Avg loss: 0.006292165610939264\n",
      "Step 2850 Avg loss: 0.006404022006317973\n",
      "Step 2900 Avg loss: 0.006411387845873833\n",
      "Step 2950 Avg loss: 0.006295937849208713\n",
      "Step 3000 Avg loss: 0.0060550114093348385\n",
      "Test loss 0.02675758022814989\n",
      "Step 3050 Avg loss: 0.006311253579333424\n",
      "Step 3100 Avg loss: 0.006255541769787669\n",
      "Step 3150 Avg loss: 0.006185004878789186\n",
      "Step 3200 Avg loss: 0.006359334625303745\n",
      "Step 3250 Avg loss: 0.006161203212104738\n",
      "Step 3300 Avg loss: 0.006110610358882695\n",
      "Step 3350 Avg loss: 0.006101271398365498\n",
      "Step 3400 Avg loss: 0.006628048466518521\n",
      "Step 3450 Avg loss: 0.006242966316640377\n",
      "Step 3500 Avg loss: 0.006246909005567431\n",
      "Test loss 0.022098890505731106\n",
      "Step 3550 Avg loss: 0.0061833646707236765\n",
      "Step 3600 Avg loss: 0.006108418013900519\n",
      "Step 3650 Avg loss: 0.006626513581722975\n",
      "Step 3700 Avg loss: 0.006221100362017751\n",
      "Step 3750 Avg loss: 0.006235527005046606\n",
      "Step 3800 Avg loss: 0.006081762141548097\n",
      "Step 3850 Avg loss: 0.006258652156684547\n",
      "Step 3900 Avg loss: 0.0061722509982064366\n",
      "Step 3950 Avg loss: 0.006160708819516003\n",
      "Step 4000 Avg loss: 0.0064997169189155105\n",
      "Test loss 0.017346326494589448\n",
      "Step 4050 Avg loss: 0.006180412732064724\n",
      "Step 4100 Avg loss: 0.006134993932209909\n",
      "Step 4150 Avg loss: 0.006268600858747959\n",
      "Step 4200 Avg loss: 0.006171110835857689\n",
      "Step 4250 Avg loss: 0.006143088778480887\n",
      "Step 4300 Avg loss: 0.006103326850570738\n",
      "Step 4350 Avg loss: 0.00623490285128355\n",
      "Step 4400 Avg loss: 0.006072380444966257\n",
      "Step 4450 Avg loss: 0.006572346296161413\n",
      "Step 4500 Avg loss: 0.006349923750385642\n",
      "Test loss 0.02241693250834942\n",
      "Step 4550 Avg loss: 0.006041375701315701\n",
      "Step 4600 Avg loss: 0.006113649900071323\n",
      "Step 4650 Avg loss: 0.006091817528940737\n",
      "Step 4700 Avg loss: 0.006249468652531505\n",
      "Step 4750 Avg loss: 0.0063662969041615725\n",
      "Step 4800 Avg loss: 0.00596040470758453\n",
      "Step 4850 Avg loss: 0.006320603201165795\n",
      "Step 4900 Avg loss: 0.006655219625681639\n",
      "Step 4950 Avg loss: 0.006090802759863436\n",
      "Step 5000 Avg loss: 0.006169278537854552\n",
      "Test loss 0.019726498518139124\n",
      "Step 5050 Avg loss: 0.006352963484823703\n",
      "Step 5100 Avg loss: 0.006150945243425667\n",
      "Step 5150 Avg loss: 0.005977216851897537\n",
      "Step 5200 Avg loss: 0.0060265237325802445\n",
      "Step 5250 Avg loss: 0.00625250562094152\n",
      "Step 5300 Avg loss: 0.00620088504627347\n",
      "Step 5350 Avg loss: 0.006040919681545347\n",
      "Step 5400 Avg loss: 0.006108946274034679\n",
      "Step 5450 Avg loss: 0.006119234075304121\n",
      "Step 5500 Avg loss: 0.006068623228929937\n",
      "Test loss 0.024575102142989635\n",
      "Step 5550 Avg loss: 0.006194894881919027\n",
      "Step 5600 Avg loss: 0.006502050217241049\n",
      "Step 5650 Avg loss: 0.006241377172991633\n",
      "Step 5700 Avg loss: 0.006064780885353684\n",
      "Step 5750 Avg loss: 0.006594946309924125\n",
      "Step 5800 Avg loss: 0.006169237401336431\n",
      "Step 5850 Avg loss: 0.006273015448823571\n",
      "Step 5900 Avg loss: 0.006260205153375864\n",
      "Step 5950 Avg loss: 0.005952481799758971\n",
      "Step 6000 Avg loss: 0.00603228215361014\n",
      "Test loss 0.0239394661039114\n",
      "Step 6050 Avg loss: 0.006209867959842086\n",
      "Step 6100 Avg loss: 0.00601757691707462\n",
      "Step 6150 Avg loss: 0.006325718192383647\n",
      "Step 6200 Avg loss: 0.006204368774779141\n",
      "Step 6250 Avg loss: 0.005955824055708945\n",
      "Step 6300 Avg loss: 0.006379053294658661\n",
      "Step 6350 Avg loss: 0.0061968665337190035\n",
      "Step 6400 Avg loss: 0.006171605200506747\n",
      "Step 6450 Avg loss: 0.006130866520106792\n",
      "Step 6500 Avg loss: 0.006174943158403039\n",
      "Test loss 0.023243417963385582\n",
      "Step 6550 Avg loss: 0.006111001670360565\n",
      "Step 6600 Avg loss: 0.006087882542051375\n",
      "Step 6650 Avg loss: 0.006101418472826481\n",
      "Step 6700 Avg loss: 0.0061277816444635395\n",
      "Step 6750 Avg loss: 0.006076127840206027\n",
      "Step 6800 Avg loss: 0.0062249017180874945\n",
      "Step 6850 Avg loss: 0.006046451167203486\n",
      "Step 6900 Avg loss: 0.00608917139004916\n",
      "Step 6950 Avg loss: 0.006158646401017904\n",
      "Step 7000 Avg loss: 0.006150727570056916\n",
      "Test loss 0.025164439342916012\n",
      "Step 7050 Avg loss: 0.006105998624116182\n",
      "Step 7100 Avg loss: 0.006322930445894599\n",
      "Step 7150 Avg loss: 0.006085436027497053\n",
      "Step 7200 Avg loss: 0.006025734264403582\n",
      "Step 7250 Avg loss: 0.0064556077681481834\n",
      "Step 7300 Avg loss: 0.0061756287794560195\n",
      "Step 7350 Avg loss: 0.006218513576313854\n",
      "Step 7400 Avg loss: 0.006049883617088199\n",
      "Step 7450 Avg loss: 0.006131799947470426\n",
      "Step 7500 Avg loss: 0.006098865789826959\n",
      "Test loss 0.0233761016279459\n",
      "Step 7550 Avg loss: 0.006143979616463184\n",
      "Step 7600 Avg loss: 0.006300593432970345\n",
      "Step 7650 Avg loss: 0.006129234596155584\n",
      "Step 7700 Avg loss: 0.006130453087389469\n",
      "Step 7750 Avg loss: 0.0060021468764171\n",
      "Step 7800 Avg loss: 0.006104712774977088\n",
      "Step 7850 Avg loss: 0.00620966350659728\n",
      "Step 7900 Avg loss: 0.006136170104146004\n",
      "Step 7950 Avg loss: 0.006107179154641926\n",
      "Step 8000 Avg loss: 0.006020528143271804\n",
      "Test loss 0.019079549703747034\n",
      "Step 8050 Avg loss: 0.006049417036119848\n",
      "Step 8100 Avg loss: 0.0060401887260377405\n",
      "Step 8150 Avg loss: 0.006056879386305809\n",
      "Step 8200 Avg loss: 0.006347969565540552\n",
      "Step 8250 Avg loss: 0.006176131442189216\n",
      "Step 8300 Avg loss: 0.006078952830284834\n",
      "Step 8350 Avg loss: 0.006092774532735348\n",
      "Step 8400 Avg loss: 0.006138246720656753\n",
      "Step 8450 Avg loss: 0.005946362386457622\n",
      "Step 8500 Avg loss: 0.00615841536782682\n",
      "Test loss 0.026433415710926056\n",
      "Step 8550 Avg loss: 0.006236998718231916\n",
      "Step 8600 Avg loss: 0.006095136953517794\n",
      "Step 8650 Avg loss: 0.00606748977676034\n",
      "Step 8700 Avg loss: 0.0061728516826406125\n",
      "Step 8750 Avg loss: 0.006066173976287246\n",
      "Step 8800 Avg loss: 0.006003295180853456\n",
      "Step 8850 Avg loss: 0.005975078432820738\n",
      "Step 8900 Avg loss: 0.006209039315581322\n",
      "Step 8950 Avg loss: 0.006014829548075795\n",
      "Step 9000 Avg loss: 0.006134764803573489\n",
      "Test loss 0.018114774022251368\n",
      "Step 9050 Avg loss: 0.005968645578250289\n",
      "Step 9100 Avg loss: 0.006214011404663325\n",
      "Step 9150 Avg loss: 0.006180034829303622\n",
      "Step 9200 Avg loss: 0.00607841516379267\n",
      "Step 9250 Avg loss: 0.005893763850908726\n",
      "Step 9300 Avg loss: 0.005954147051088512\n",
      "Step 9350 Avg loss: 0.006021863052155823\n",
      "Step 9400 Avg loss: 0.006326348232105375\n",
      "Step 9450 Avg loss: 0.0060992947034537795\n",
      "Step 9500 Avg loss: 0.006097103273496032\n",
      "Test loss 0.018850050400942564\n",
      "Step 9550 Avg loss: 0.0061829402204602955\n",
      "Step 9600 Avg loss: 0.006037068599835038\n",
      "Step 9650 Avg loss: 0.006194326737895608\n",
      "Step 9700 Avg loss: 0.005925471531227231\n",
      "Step 9750 Avg loss: 0.006086401371285319\n",
      "Step 9800 Avg loss: 0.006089857695624232\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 9850 Avg loss: 0.0061470425873994825\n",
      "Step 9900 Avg loss: 0.0059511422412469985\n",
      "Step 9950 Avg loss: 0.006058933325111866\n",
      "Step 10000 Avg loss: 0.005997133399359882\n",
      "Test loss 0.03272348362952471\n",
      "Step 10050 Avg loss: 0.006137542147189379\n",
      "Step 10100 Avg loss: 0.005961675113067031\n",
      "Step 10150 Avg loss: 0.005978057575412095\n",
      "Step 10200 Avg loss: 0.0063346672896295786\n",
      "Step 10250 Avg loss: 0.006146139418706298\n",
      "Step 10300 Avg loss: 0.006047660261392593\n",
      "Step 10350 Avg loss: 0.0060468278173357246\n",
      "Step 10400 Avg loss: 0.006061419700272381\n",
      "Step 10450 Avg loss: 0.006066484423354268\n",
      "Step 10500 Avg loss: 0.0063798254728317265\n",
      "Test loss 0.021921115461736917\n",
      "Step 10550 Avg loss: 0.006094900658354163\n",
      "Step 10600 Avg loss: 0.005985372327268123\n",
      "Step 10650 Avg loss: 0.005904308834578842\n",
      "Step 10700 Avg loss: 0.006312094386667013\n",
      "Step 10750 Avg loss: 0.005930282201152295\n",
      "Step 10800 Avg loss: 0.005987819293513894\n",
      "Step 10850 Avg loss: 0.006143526223022491\n",
      "Step 10900 Avg loss: 0.006132646608166397\n",
      "Step 10950 Avg loss: 0.005957037121988833\n",
      "Step 11000 Avg loss: 0.006136623984202743\n",
      "Test loss 0.019459386821836233\n",
      "Step 11050 Avg loss: 0.006069113090634346\n",
      "Step 11100 Avg loss: 0.006019472796469927\n",
      "Step 11150 Avg loss: 0.006074223865289241\n",
      "Step 11200 Avg loss: 0.006198742501437664\n",
      "Step 11250 Avg loss: 0.005966503955423832\n",
      "Step 11300 Avg loss: 0.0062031460693106055\n",
      "Step 11350 Avg loss: 0.006028924426063895\n",
      "Step 11400 Avg loss: 0.006076720268465579\n",
      "Step 11450 Avg loss: 0.0059126246767118576\n",
      "Step 11500 Avg loss: 0.005966607467271388\n",
      "Test loss 0.016273202607408166\n",
      "Step 11550 Avg loss: 0.006196632925421\n",
      "Step 11600 Avg loss: 0.006201172610744834\n",
      "Step 11650 Avg loss: 0.006193703319877386\n",
      "Step 11700 Avg loss: 0.006039530429989099\n",
      "Step 11750 Avg loss: 0.005756352131720632\n",
      "Step 11800 Avg loss: 0.006229972960427404\n",
      "Step 11850 Avg loss: 0.005858393451198936\n",
      "Step 11900 Avg loss: 0.006043972866609693\n",
      "Step 11950 Avg loss: 0.006004833106417209\n",
      "Step 12000 Avg loss: 0.005838438253849745\n",
      "Test loss 0.02234746515750885\n",
      "Step 12050 Avg loss: 0.00600980257615447\n",
      "Step 12100 Avg loss: 0.006048654094338417\n",
      "Step 12150 Avg loss: 0.005967662027105689\n",
      "Step 12200 Avg loss: 0.0060195320216007535\n",
      "Step 12250 Avg loss: 0.006060597877949476\n",
      "Step 12300 Avg loss: 0.005929252253845334\n",
      "Step 12350 Avg loss: 0.005919906129129231\n",
      "Step 12400 Avg loss: 0.005987648600712419\n",
      "Step 12450 Avg loss: 0.005955101284198463\n",
      "Step 12500 Avg loss: 0.00593645628541708\n",
      "Test loss 0.023949972353875637\n",
      "Step 12550 Avg loss: 0.006015157611109316\n",
      "Step 12600 Avg loss: 0.006000224305316806\n",
      "Step 12650 Avg loss: 0.005903889162000269\n",
      "Step 12700 Avg loss: 0.006027976507321\n",
      "Step 12750 Avg loss: 0.006517864475026727\n",
      "Step 12800 Avg loss: 0.006113739516586066\n",
      "Step 12850 Avg loss: 0.005959261953830719\n",
      "Step 12900 Avg loss: 0.005882550301030278\n",
      "Step 12950 Avg loss: 0.005957806743681431\n",
      "Step 13000 Avg loss: 0.005921823150711134\n",
      "Test loss 0.04139446280896664\n",
      "Step 13050 Avg loss: 0.005997541667893529\n",
      "Step 13100 Avg loss: 0.005917729325592518\n",
      "Step 13150 Avg loss: 0.006016039382666349\n",
      "Step 13200 Avg loss: 0.005895836604759097\n",
      "Step 13250 Avg loss: 0.005893396628089249\n",
      "Step 13300 Avg loss: 0.006175005733966827\n",
      "Step 13350 Avg loss: 0.005911616361699999\n",
      "Step 13400 Avg loss: 0.005857231204863637\n",
      "Step 13450 Avg loss: 0.0058598297275602815\n",
      "Step 13500 Avg loss: 0.006260007293894887\n",
      "Test loss 0.024623223580420017\n",
      "Step 13550 Avg loss: 0.006058363523334265\n",
      "Step 13600 Avg loss: 0.0058309856615960595\n",
      "Step 13650 Avg loss: 0.005988182118162513\n",
      "Step 13700 Avg loss: 0.005884876782074571\n",
      "Step 13750 Avg loss: 0.006007225010544062\n",
      "Step 13800 Avg loss: 0.0059240341652184725\n",
      "Step 13850 Avg loss: 0.005982574708759784\n",
      "Step 13900 Avg loss: 0.0060602667927742\n",
      "Step 13950 Avg loss: 0.005895647378638387\n",
      "Step 14000 Avg loss: 0.005936731798574329\n",
      "Test loss 0.027091960422694683\n",
      "Step 14050 Avg loss: 0.006008823486045003\n",
      "Step 14100 Avg loss: 0.006026210505515337\n",
      "Step 14150 Avg loss: 0.00595448196399957\n",
      "Step 14200 Avg loss: 0.006009524967521429\n",
      "Step 14250 Avg loss: 0.005892752637155354\n",
      "Step 14300 Avg loss: 0.005979778673499823\n",
      "Step 14350 Avg loss: 0.005853937310166657\n",
      "Step 14400 Avg loss: 0.006190961059182883\n",
      "Step 14450 Avg loss: 0.005831902651116252\n",
      "Step 14500 Avg loss: 0.005942365154623985\n",
      "Test loss 0.0276408726349473\n",
      "Step 14550 Avg loss: 0.0059312509139999746\n",
      "Step 14600 Avg loss: 0.005926672974601388\n",
      "Step 14650 Avg loss: 0.005852409759536385\n",
      "Step 14700 Avg loss: 0.006268932186067104\n",
      "Step 14750 Avg loss: 0.005876302532851696\n",
      "Step 14800 Avg loss: 0.00607397710904479\n",
      "Step 14850 Avg loss: 0.005858089248649776\n",
      "Step 14900 Avg loss: 0.005877216514199972\n",
      "Step 14950 Avg loss: 0.006357256239280105\n",
      "Step 15000 Avg loss: 0.005909527679905296\n",
      "Test loss 0.02333337627351284\n",
      "Step 15050 Avg loss: 0.005958015164360404\n",
      "Step 15100 Avg loss: 0.0059903719229623675\n",
      "Step 15150 Avg loss: 0.00585004408378154\n",
      "Step 15200 Avg loss: 0.005952840894460678\n",
      "Step 15250 Avg loss: 0.006161763123236597\n",
      "Step 15300 Avg loss: 0.00586214589420706\n",
      "Step 15350 Avg loss: 0.005810126294381917\n",
      "Step 15400 Avg loss: 0.0058051964326296\n",
      "Step 15450 Avg loss: 0.005830474118702114\n",
      "Step 15500 Avg loss: 0.006116522843949496\n",
      "Test loss 0.02872608322650194\n",
      "Step 15550 Avg loss: 0.005855429993243888\n",
      "Step 15600 Avg loss: 0.005844090328318998\n",
      "Step 15650 Avg loss: 0.005823508030734956\n",
      "Step 15700 Avg loss: 0.006228037942200899\n",
      "Step 15750 Avg loss: 0.005799838458187878\n",
      "Step 15800 Avg loss: 0.006323934001848101\n",
      "Step 15850 Avg loss: 0.006112239784561098\n",
      "Step 15900 Avg loss: 0.005766261462122202\n",
      "Step 15950 Avg loss: 0.00595242690294981\n",
      "Step 16000 Avg loss: 0.005847802991047502\n",
      "Test loss 0.035157978534698486\n",
      "Step 16050 Avg loss: 0.0058894231216982005\n",
      "Step 16100 Avg loss: 0.006002152762375772\n",
      "Step 16150 Avg loss: 0.005879500769078731\n",
      "Step 16200 Avg loss: 0.006150246146135032\n",
      "Step 16250 Avg loss: 0.005928228814154863\n",
      "Step 16300 Avg loss: 0.005782610457390547\n",
      "Step 16350 Avg loss: 0.006020059278234839\n",
      "Step 16400 Avg loss: 0.005899842018261552\n",
      "Step 16450 Avg loss: 0.006304229311645031\n",
      "Step 16500 Avg loss: 0.0059043232584372165\n",
      "Test loss 0.029858111403882504\n",
      "Step 16550 Avg loss: 0.005826443918049335\n",
      "Step 16600 Avg loss: 0.005994685012847185\n",
      "Step 16650 Avg loss: 0.0059344481071457265\n",
      "Step 16700 Avg loss: 0.0058607708849012854\n",
      "Step 16750 Avg loss: 0.005920763239264488\n",
      "Step 16800 Avg loss: 0.005849662486580201\n",
      "Step 16850 Avg loss: 0.0058716572541743514\n",
      "Step 16900 Avg loss: 0.006102756154723466\n",
      "Step 16950 Avg loss: 0.0059076045593246815\n",
      "Step 17000 Avg loss: 0.005714449174702168\n",
      "Test loss 0.03799834940582514\n",
      "Step 17050 Avg loss: 0.005830384609289467\n",
      "Step 17100 Avg loss: 0.006087382342666387\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_94302/2167173573.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;31m#train_mask = torch.unsqueeze(train_mask, 2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#* train_mask.to(device)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "input_dim = int(window_len / tftts_config[\"hop_size\"] * tftts_config[\"num_mels\"])\n",
    "#print(stft_frames_per_window)\n",
    "#print(tftts_config[\"hop_size\"])\n",
    "print(input_dim)\n",
    "model = BiLSTMModel(hidden_size=512, input_dim=input_dim).to(device)\n",
    "\n",
    "learning_rate = 0.0001\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "num_steps = 100000\n",
    "print_loss_every = 50\n",
    "eval_every = 500\n",
    "\n",
    "batch = iter(train_dataloader)\n",
    "accum_loss = 0\n",
    "\n",
    "for t in range(num_steps):\n",
    "    optimizer.zero_grad()\n",
    "    train_features, train_mask, train_labels, train_files = next(batch, (None,None,None,None))\n",
    "    #print(train_features.shape)\n",
    "    #print(train_labels.shape)\n",
    "\n",
    "    if train_features is None:\n",
    "        batch = iter(train_dataloader)\n",
    "        train_features, train_mask , train_labels, _ = next(batch)\n",
    "    \n",
    "    #train_mask = torch.unsqueeze(train_mask, 2)\n",
    "    \n",
    "    x = train_features.to(device)     \n",
    "    \n",
    "    y = train_labels.to(device) #* train_mask.to(device)\n",
    "    \n",
    "    preds = model(x) #* train_mask.to(device)\n",
    "    \n",
    "    #print(preds.shape)\n",
    "    \n",
    "    preds = torch.transpose(preds, 1,3).squeeze()\n",
    "    #print(f\"x {x.shape} preds {preds.shape} y {y.shape}\")    \n",
    "    #loss = torch.nn.functional.cross_entropy(preds, y)\n",
    "    loss = torch.nn.functional.huber_loss(preds, y)\n",
    "    #for i in range(preds.shape[1] - 1):\n",
    "    #    loss += torch.nn.functional.cosine_embedding_loss(preds[:,i,:], preds[:,i+1,:], (torch.ones(preds.shape[0])).to(device))\n",
    "    accum_loss += loss.item()\n",
    "    if t % print_loss_every == 0:\n",
    "        print(f\"Step {t} Avg loss: {accum_loss / print_loss_every}\")\n",
    "        accum_loss = 0\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    \n",
    "    \n",
    "    if t > 0 and t % eval_every == 0:\n",
    "        accum_loss = 0\n",
    "        for test_features, test_mask, test_labels, _ in iter(test_dataloader):\n",
    "            x = test_features.to(device)\n",
    "            #x = torch.transpose(x, 1, 2)\n",
    "            #x = torch.unsqueeze(x, dim=3)\n",
    "            y = test_labels.to(device)\n",
    "            preds = model(x)\n",
    "            preds = torch.transpose(preds, 1,3)\n",
    "            accum_loss += torch.nn.functional.mse_loss(preds, y).item()\n",
    "            #accum_loss = torch.nn.functional.cross_entropy(preds, y)\n",
    "\n",
    "        print(f\"Test loss {accum_loss}\")\n",
    "        accum_loss = 0\n",
    "    \n",
    "#pred_probab = nn.Softmax(dim=1)(logits)\n",
    "#y_pred = pred_probab.argmax(1)\n",
    "#print(f\"Predicted class: {y_pred}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41510411",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_features, test_mask, test_labels, test_files = next(iter(test_dataloader))\n",
    "#test_features, test_mask, test_labels, test_files = next(iter(train_dataloader))\n",
    "##test_features = torch.transpose(test_features, 1, 2)\n",
    "test_features = train_features\n",
    "export_y_batch = model(test_features.to(device)) \n",
    "export_y = export_y_batch[0,:,:]\n",
    "print(export_y.shape)\n",
    "header = \"Timecode,BlendShapeCount,eyeBlinkRight,eyeLookDownRight,eyeLookInRight,eyeLookOutRight,eyeLookUpRight,eyeSquintRight,eyeWideRight,eyeBlinkLeft,eyeLookDownLeft,eyeLookInLeft,eyeLookOutLeft,eyeLookUpLeft,eyeSquintLeft,eyeWideLeft,jawForward,jawRight,jawLeft,jawOpen,mouthClose,mouthFunnel,mouthPucker,mouthRight,mouthLeft,mouthSmileRight,mouthSmileLeft,mouthFrownRight,mouthFrownLeft,mouthDimpleRight,mouthDimpleLeft,mouthStretchRight,mouthStretchLeft,mouthRollLower,mouthRollUpper,mouthShrugLower,mouthShrugUpper,mouthPressRight,mouthPressLeft,mouthLowerDownRight,mouthLowerDownLeft,mouthUpperUpRight,mouthUpperUpLeft,browDownRight,browDownLeft,browInnerUp,browOuterUpRight,browOuterUpLeft,cheekPuff,cheekSquintRight,cheekSquintLeft,noseSneerRight,noseSneerLeft,tongueOut,HeadYaw,HeadPitch,HeadRoll,LeftEyeYaw,LeftEyePitch,LeftEyeRoll,RightEyeYaw,RightEyePitch,RightEyeRoll\".split(',')\n",
    "selected_output_indices = [header.index(x[0].lower() + x[1:]) for x in blendshapes]\n",
    "num_visemes = len(blendshapes)\n",
    "with open(\"output.csv\", \"w\") as outfile:\n",
    "    outfile.write(\",\".join(header) + \"\\n\")\n",
    "    timer_ms = 0\n",
    "    print(export_y)\n",
    "    for t in range(export_y.shape[1]):\n",
    "        output = [str(0)] * len(header)\n",
    "        second = str(int(timer_ms // 1000)).zfill(2)\n",
    "        frame = (timer_ms % 1000) * target_framerate / 1000\n",
    "        output[0] = f\"00:00:{second}:{frame}\"\n",
    "        for viseme in range(num_visemes): \n",
    "            output[selected_output_indices[viseme]] = str(export_y[viseme,t,:].item())\n",
    "        timer_ms += (1 / target_framerate) * 1000\n",
    "        outfile.write(\",\".join(output) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923dc158",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16652a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_header = \"Timecode,BlendShapeCount,EyeBlinkLeft,EyeLookDownLeft,EyeLookInLeft,EyeLookOutLeft,EyeLookUpLeft,EyeSquintLeft,EyeWideLeft,EyeBlinkRight,EyeLookDownRight,EyeLookInRight,EyeLookOutRight,EyeLookUpRight,EyeSquintRight,EyeWideRight,JawForward,JawRight,JawLeft,JawOpen,MouthClose,MouthFunnel,MouthPucker,MouthRight,MouthLeft,MouthSmileLeft,MouthSmileRight,MouthFrownLeft,MouthFrownRight,MouthDimpleLeft,MouthDimpleRight,MouthStretchLeft,MouthStretchRight,MouthRollLower,MouthRollUpper,MouthShrugLower,MouthShrugUpper,MouthPressLeft,MouthPressRight,MouthLowerDownLeft,MouthLowerDownRight,MouthUpperUpLeft,MouthUpperUpRight,BrowDownLeft,BrowDownRight,BrowInnerUp,BrowOuterUpLeft,BrowOuterUpRight,CheekPuff,CheekSquintLeft,CheekSquintRight,NoseSneerLeft,NoseSneerRight,TongueOut,HeadYaw,HeadPitch,HeadRoll,LeftEyeYaw,LeftEyePitch,LeftEyeRoll,RightEyeYaw,RightEyePitch,RightEyeRoll\"\n",
    "remap = {h:(h[0].lower() + h[1:]) if h not in [\"Timecode\",\"BlendShapeCount\",\"HeadYaw\",\"HeadPitch\",\"HeadRoll\",\"LeftEyeYaw\",\"LeftEyePitch\",\"LeftEyeRoll\",\"RightEyeYaw\",\"RightEyePitch\",\"RightEyeRoll\"]  else h for h in new_header.split(\",\") }\n",
    "for oh in remap.values():\n",
    "    if oh not in header:\n",
    "        print(oh)\n",
    "\n",
    "def new_to_old(csv_df):\n",
    "    return csv_df.rename(columns=remap)\n",
    "df = preprocess_viseme(\"data/training/speaker_1/20210824_1/61.csv\", pad_len_in_secs=pad_len_in_secs, \n",
    "                                   resample_to=target_framerate)\n",
    "#print(df)\n",
    "df = new_to_old(df)\n",
    "cols = [c for c in df.columns if c not in [\"jawOpen\", \"mouthClose\", \"Timecode\"]]\n",
    "df[cols] = 0\n",
    "df = df[df[\"Timecode\"] != 0]\n",
    "df.to_csv(\"original.csv\", index=False)\n",
    "#new_to_old(df)[header].to_csv(\"original.csv\", index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "00801822",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"bilstm.torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c58f7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in [\"MouthClose\", \"MouthFunnel\", \"MouthPucker\", \"JawOpen\"]:\n",
    "    print(new_header.split(\",\").index(item))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "70887b32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hydroxide/.local/lib/python3.7/site-packages/torch/onnx/symbolic_opset9.py:2099: UserWarning: Exporting a model to ONNX with a batch_size other than 1, with a variable length with LSTM can cause an error when running the ONNX model with a different batch size. Make sure to save the model with a batch size of 1, or define the initial states (h0/c0) as inputs of the model. \n",
      "  \"or define the initial states (h0/c0) as inputs of the model. \")\n"
     ]
    }
   ],
   "source": [
    "model = torch.load(\"bilstm.torch\",map_location=torch.device('cpu'))\n",
    "import torch.onnx \n",
    "\n",
    "# set the model to inference mode \n",
    "model.eval() \n",
    "\n",
    "# Let's create a dummy input tensor  \n",
    "dummy_input = torch.randn(1, 119, 2640, requires_grad=True)  \n",
    "\n",
    "# Export the model   \n",
    "torch.onnx.export(model,         # model being run \n",
    "     dummy_input,       # model input (or a tuple for multiple inputs) \n",
    "     \"bilstm.onnx\",       # where to save the model  \n",
    "     export_params=True,  # store the trained parameter weights inside the model file \n",
    "     opset_version=10,    # the ONNX version to export the model to \n",
    "     do_constant_folding=True,  # whether to execute constant folding for optimization \n",
    "     #input_names = ['modelInput'],   # the model's input names \n",
    "     #output_names = ['modelOutput'], # the model's output names \n",
    "     dynamic_axes={'modelInput' : {0 : 'batch_size'},    # variable length axes \n",
    "    'modelOutput' : {0 : 'batch_size'}}\n",
    "                 ) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b9c98c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-19 00:50:56.130563: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-09-19 00:50:56.490301: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-09-19 00:50:56.491554: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-09-19 00:50:56.494104: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-19 00:50:56.495393: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-09-19 00:50:56.496690: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-09-19 00:50:56.497811: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-09-19 00:50:57.657418: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-09-19 00:50:57.657755: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-09-19 00:50:57.658011: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-09-19 00:50:57.658242: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4560 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From /home/hydroxide/.local/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py:463: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /home/hydroxide/.local/lib/python3.7/site-packages/tensorflow/python/ops/rnn.py:447: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /home/hydroxide/.local/lib/python3.7/site-packages/keras/layers/legacy_rnn/rnn_cell_impl.py:979: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hydroxide/.local/lib/python3.7/site-packages/keras/layers/legacy_rnn/rnn_cell_impl.py:901: UserWarning: `tf.nn.rnn_cell.LSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
      "  warnings.warn(\"`tf.nn.rnn_cell.LSTMCell` is deprecated and will be \"\n",
      "/home/hydroxide/.local/lib/python3.7/site-packages/keras/engine/base_layer.py:2223: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
      "  warnings.warn('`layer.add_variable` is deprecated and '\n",
      "/home/hydroxide/.local/lib/python3.7/site-packages/keras/engine/base_layer.py:1348: UserWarning: `layer.updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`layer.updates` will be removed in a future version. '\n",
      "WARNING:absl:Function `__call__` contains input name(s) input.1 with unsupported characters which will be renamed to input_1 in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-19 00:51:02.395646: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hydroxide/.local/lib/python3.7/site-packages/keras/layers/legacy_rnn/rnn_cell_impl.py:901: UserWarning: `tf.nn.rnn_cell.LSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
      "  warnings.warn(\"`tf.nn.rnn_cell.LSTMCell` is deprecated and will be \"\n",
      "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
      "/home/hydroxide/.local/lib/python3.7/site-packages/keras/engine/base_layer.py:1348: UserWarning: `layer.updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`layer.updates` will be removed in a future version. '\n",
      "/home/hydroxide/.local/lib/python3.7/site-packages/keras/engine/base_layer.py:2223: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
      "  warnings.warn('`layer.add_variable` is deprecated and '\n",
      "WARNING:absl:Found untraced functions such as gen_tensor_dict while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./tf_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./tf_model/assets\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "from onnx_tf.backend import prepare\n",
    "import onnxruntime\n",
    "model_onnx = onnx.load('bilstm.onnx')\n",
    "tf_rep = prepare(model_onnx)\n",
    "tf_rep.export_graph('./tf_model')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5df4666c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "`to_quantize` can only be a `tf.keras.Model` instance. Use the `quantize_annotate_layer` API to handle individual layers.You passed an instance of type: TensorflowRep.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_94302/3583183096.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow_model_optimization\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtfmot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mquant_aware_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfmot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantization\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantize_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf_rep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mquant_aware_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_model_optimization/python/core/quantization/keras/quantize.py\u001b[0m in \u001b[0;36mquantize_model\u001b[0;34m(to_quantize)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0;34m'the `quantize_annotate_layer` API to handle individual layers.'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         'You passed an instance of type: {input}.'.format(\n\u001b[0;32m--> 130\u001b[0;31m             input=to_quantize.__class__.__name__))\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_quantize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequential\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: `to_quantize` can only be a `tf.keras.Model` instance. Use the `quantize_annotate_layer` API to handle individual layers.You passed an instance of type: TensorflowRep."
     ]
    }
   ],
   "source": [
    "import tensorflow_model_optimization as tfmot\n",
    "\n",
    "quant_aware_model = tfmot.quantization.keras.quantize_model(tf_rep)\n",
    "quant_aware_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "953bfeb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-19 00:51:24.084527: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:351] Ignored output_format.\n",
      "2021-09-19 00:51:24.084557: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:354] Ignored drop_control_dependency.\n",
      "2021-09-19 00:51:24.084563: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:360] Ignored change_concat_input_ranges.\n",
      "2021-09-19 00:51:24.085352: I tensorflow/cc/saved_model/reader.cc:38] Reading SavedModel from: ./tf_model\n",
      "2021-09-19 00:51:24.098702: I tensorflow/cc/saved_model/reader.cc:90] Reading meta graph with tags { serve }\n",
      "2021-09-19 00:51:24.098737: I tensorflow/cc/saved_model/reader.cc:132] Reading SavedModel debug info (if present) from: ./tf_model\n",
      "2021-09-19 00:51:24.106997: I tensorflow/cc/saved_model/loader.cc:211] Restoring SavedModel bundle.\n",
      "2021-09-19 00:51:24.148006: I tensorflow/cc/saved_model/loader.cc:195] Running initialization op on SavedModel bundle at path: ./tf_model\n",
      "2021-09-19 00:51:24.162825: I tensorflow/cc/saved_model/loader.cc:283] SavedModel load for tags { serve }; Status: success: OK. Took 77474 microseconds.\n",
      "2021-09-19 00:51:24.225967: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:210] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2021-09-19 00:51:26.039595: W tensorflow/compiler/mlir/lite/flatbuffer_export.cc:1827] Graph contains the following resource op(s), that use(s) resource type. Currently, the resource type is not natively supported in TFLite. Please consider not using the resource type if there are issues with either TFLite converter or TFLite runtime:\n",
      "Resource ops: AssignVariableOp, ReadVariableOp, VarHandleOp\n",
      "Details:\n",
      "\ttf.AssignVariableOp(tensor<!tf.resource<tensor<2048xf32>>>, tensor<2048xf32>) -> ()\n",
      "\ttf.AssignVariableOp(tensor<!tf.resource<tensor<2048xf32>>>, tensor<2048xf32>) -> () : {device = \"\"}\n",
      "\ttf.AssignVariableOp(tensor<!tf.resource<tensor<?x?xf32>>>, tensor<1x1xf32>) -> ()\n",
      "\ttf.AssignVariableOp(tensor<!tf.resource<tensor<?x?xf32>>>, tensor<3152x2048xf32>) -> () : {device = \"\"}\n",
      "\ttf.ReadVariableOp(tensor<!tf.resource<tensor<2048xf32>>>) -> (tensor<2048xf32>) : {device = \"\"}\n",
      "\ttf.ReadVariableOp(tensor<!tf.resource<tensor<?x?xf32>>>) -> (tensor<?x?xf32>) : {device = \"\"}\n",
      "\ttf.VarHandleOp() -> (tensor<!tf.resource<tensor<2048xf32>>>) : {allowed_devices = [], container = \"\", device = \"\", shared_name = \"lstm_bias_lstm_9\"}\n",
      "\ttf.VarHandleOp() -> (tensor<!tf.resource<tensor<?x?xf32>>>) : {allowed_devices = [], container = \"\", device = \"\", shared_name = \"lstm_kernel_lstm_9\"}\n",
      "2021-09-19 00:51:26.039630: W tensorflow/compiler/mlir/lite/flatbuffer_export.cc:1838] TFLite interpreter needs to link Flex delegate in order to run the model since it contains the following flex op(s):\n",
      "Flex ops: FlexAssignVariableOp, FlexReadVariableOp, FlexVarHandleOp\n",
      "Details:\n",
      "\ttf.AssignVariableOp(tensor<!tf.resource<tensor<2048xf32>>>, tensor<2048xf32>) -> ()\n",
      "\ttf.AssignVariableOp(tensor<!tf.resource<tensor<2048xf32>>>, tensor<2048xf32>) -> () : {device = \"\"}\n",
      "\ttf.AssignVariableOp(tensor<!tf.resource<tensor<?x?xf32>>>, tensor<1x1xf32>) -> ()\n",
      "\ttf.AssignVariableOp(tensor<!tf.resource<tensor<?x?xf32>>>, tensor<3152x2048xf32>) -> () : {device = \"\"}\n",
      "\ttf.ReadVariableOp(tensor<!tf.resource<tensor<2048xf32>>>) -> (tensor<2048xf32>) : {device = \"\"}\n",
      "\ttf.ReadVariableOp(tensor<!tf.resource<tensor<?x?xf32>>>) -> (tensor<?x?xf32>) : {device = \"\"}\n",
      "\ttf.VarHandleOp() -> (tensor<!tf.resource<tensor<2048xf32>>>) : {allowed_devices = [], container = \"\", device = \"\", shared_name = \"lstm_bias_lstm_9\"}\n",
      "\ttf.VarHandleOp() -> (tensor<!tf.resource<tensor<?x?xf32>>>) : {allowed_devices = [], container = \"\", device = \"\", shared_name = \"lstm_kernel_lstm_9\"}\n",
      "2021-09-19 00:51:26.097831: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor onnx_tf_prefix_MatMul_642 because it has no allocated buffer.\n",
      "2021-09-19 00:51:26.097860: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor onnx_tf_prefix_MatMul_663 because it has no allocated buffer.\n",
      "2021-09-19 00:51:26.112168: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor arg7 because it has no allocated buffer.\n",
      "2021-09-19 00:51:26.112193: I tensorflow/lite/tools/optimize/quantize_weights.cc:225] Skipping quantization of tensor LSTM_f75153e5/bidirectional_rnn/fw/fw/while/fw/multi_rnn_cell/cell_0/lstm_cell/MatMul because it has fewer than 1024 elements (1).\n",
      "2021-09-19 00:51:26.112204: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor arg7 because it has no allocated buffer.\n",
      "2021-09-19 00:51:26.112209: I tensorflow/lite/tools/optimize/quantize_weights.cc:225] Skipping quantization of tensor LSTM_f75153e5/bidirectional_rnn/bw/bw/while/bw/multi_rnn_cell/cell_0/lstm_cell/MatMul because it has fewer than 1024 elements (1).\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(\"./tf_model\")\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,\n",
    "                                       tf.lite.OpsSet.SELECT_TF_OPS]\n",
    "converter.allow_custom_ops=False\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.experimental_enable_resource_variables = True\n",
    "\n",
    "converter.experimental_new_converter =True\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the model\n",
    "with open(\"bilstm.tflite\", 'wb') as f:\n",
    "    f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "456c396c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite delegate for select TF ops.\n",
      "2021-09-19 00:51:47.989188: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-09-19 00:51:47.989570: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-09-19 00:51:47.989887: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-09-19 00:51:47.990275: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-09-19 00:51:47.990593: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-09-19 00:51:47.990890: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4560 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1\n",
      "INFO: TfLiteFlexDelegate delegate: 2 nodes delegated out of 755 nodes with 1 partitions.\n",
      "\n",
      "INFO: TfLiteFlexDelegate delegate: 4 nodes delegated out of 4 nodes with 1 partitions.\n",
      "\n",
      "INFO: TfLiteFlexDelegate delegate: 0 nodes delegated out of 3 nodes with 0 partitions.\n",
      "\n",
      "INFO: TfLiteFlexDelegate delegate: 4 nodes delegated out of 29 nodes with 1 partitions.\n",
      "\n",
      "INFO: TfLiteFlexDelegate delegate: 0 nodes delegated out of 3 nodes with 0 partitions.\n",
      "\n",
      "INFO: TfLiteFlexDelegate delegate: 4 nodes delegated out of 29 nodes with 1 partitions.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'name': 'StatefulPartitionedCall:0',\n",
       "  'index': 919,\n",
       "  'shape': array([  1,   4, 119,   1], dtype=int32),\n",
       "  'shape_signature': array([  1,   4, 119,   1], dtype=int32),\n",
       "  'dtype': numpy.float32,\n",
       "  'quantization': (0.0, 0),\n",
       "  'quantization_parameters': {'scales': array([], dtype=float32),\n",
       "   'zero_points': array([], dtype=int32),\n",
       "   'quantized_dimension': 0},\n",
       "  'sparsity_parameters': {}}]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interpreter = tf.lite.Interpreter(model_path=\"bilstm.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "    \n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "input_shape = input_details[0]['shape']\n",
    "output_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3e4350",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv = pd.read_csv(\"data/training/speaker_1/20210824_1/61.csv\")\n",
    "columns = [x for x in list(csv.columns) if \"Eye\" not in x]\n",
    "columns.remove(\"Timecode\")\n",
    "columns.remove(\"BlendShapeCount\")\n",
    "csv[columns].var().sort_values()\n",
    "#df = preprocess_viseme(\"data/training/speaker_1/20210824_1/61.csv\", pad_len_in_secs=pad_len_in_secs, \n",
    "#                                   resample_to=target_framerate, blendshapes=[\"MouthClose\",\"MouthFunnel\"])\n",
    "#df.shape\n",
    "#[df.iloc[0][\"EyeLookInLeft\"]]\n",
    "    #csv[columns] = pd.np.digitize(csv[columns], np.linspace(0,1,11))\n",
    "    \n",
    "    #split = csv[\"Timecode\"].str.split(':')\n",
    "    #minute = split.str[1].astype(int)\n",
    "    #second = split.str[2].astype(int)\n",
    "    #frame = split.str[3].astype(float)\n",
    "    #minute -= minute[0]\n",
    "    #ms\n",
    "    #step = minute * 60 + second\n",
    "    #csv[\"step\"] = step\n",
    "    #return csv.drop_duplicates([\"step\"])[[\"step\", \"MouthClose\",\"MouthFunnel\",\"MouthPucker\",\"JawOpen\"]]\n",
    "    \n",
    "# if we want to use softmax across each blendshape as a one-hot\n",
    "    #return np.reshape(vals, (vals.shape[0], vals.shape[1], 1))\n",
    "    #one_hot = np.zeros((vals.shape[0], vals.shape[1], 11, 1))\n",
    "    #oh = np.eye(11)\n",
    "    #for row in range(vals.shape[0]):\n",
    "    #    for t in range(vals.shape[1]):\n",
    "    #        one_hot[row, :, :, 0] = np.eye(11)[int(vals[row,t])-1]\n",
    "    #return one_hot"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
